<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.27">
  <meta charset="utf-8">
  <meta name="generator" content="ReSpec 26.6.6">
  <meta name="viewport" content=
  "width=device-width, initial-scale=1, shrink-to-fit=no">
  <style>
  span.example-title{text-transform:none}
  aside.example,div.example,div.illegal-example{padding:.5em;margin:1em 0;position:relative;clear:both}
  div.illegal-example{color:red}
  div.illegal-example p{color:#000}
  aside.example,div.example{padding:.5em;border-left-width:.5em;border-left-style:solid;border-color:#e0cb52;background:#fcfaee}
  aside.example div.example{border-left-width:.1em;border-color:#999;background:#fff}
  aside.example div.example span.example-title{color:#999}
  .example pre{background-color:rgba(0,0,0,.03)}
  </style>
  <style>
  dfn{cursor:pointer}
  .dfn-panel{position:absolute;z-index:35;min-width:300px;max-width:500px;padding:.5em .75em;margin-top:.6em;font:small Helvetica Neue,sans-serif,Droid Sans Fallback;background:#fff;color:#000;box-shadow:0 1em 3em -.4em rgba(0,0,0,.3),0 0 1px 1px rgba(0,0,0,.05);border-radius:2px}
  .dfn-panel:not(.docked)>.caret{position:absolute;top:-9px}
  .dfn-panel:not(.docked)>.caret::after,.dfn-panel:not(.docked)>.caret::before{content:"";position:absolute;border:10px solid transparent;border-top:0;border-bottom:10px solid #fff;top:0}
  .dfn-panel:not(.docked)>.caret::before{border-bottom:9px solid #a2a9b1}
  .dfn-panel *{margin:0}
  .dfn-panel b{display:block;color:#000;margin-top:.25em}
  .dfn-panel ul a[href]{color:#333}
  .dfn-panel>div{display:flex}
  .dfn-panel a.self-link{font-weight:700;margin-right:auto}
  .dfn-panel .marker{padding:.1em;margin-left:.5em;border-radius:.2em;text-align:center;white-space:nowrap;font-size:90%;color:#040b1c}
  .dfn-panel .marker.dfn-exported{background:#d1edfd;box-shadow:0 0 0 .125em #1ca5f940}
  .dfn-panel .marker.idl-block{background:#8ccbf2;box-shadow:0 0 0 .125em #0670b161}
  .dfn-panel a:not(:hover){text-decoration:none!important;border-bottom:none!important}
  .dfn-panel a[href]:hover{border-bottom-width:1px}
  .dfn-panel ul{padding:0}
  .dfn-panel li{margin-left:1em}
  .dfn-panel.docked{position:fixed;left:.5em;top:unset;bottom:2em;margin:0 auto;max-width:calc(100vw - .75em * 2 - .5em - .2em * 2);max-height:30vh;overflow:auto}
  </style>
  <link rel="stylesheet" href="tablestyle.css">
  <title>Web of Things (WoT): Use Cases and Requirements</title>
  <style id="respec-mainstyle">
  @keyframes pop{
  0%{transform:scale(1,1)}
  25%{transform:scale(1.25,1.25);opacity:.75}
  100%{transform:scale(1,1)}
  }
  .hljs{background:0 0!important}
  a abbr,h1 abbr,h2 abbr,h3 abbr,h4 abbr,h5 abbr,h6 abbr{border:none}
  dfn{font-weight:700}
  a.internalDFN{color:inherit;border-bottom:1px solid #99c;text-decoration:none}
  a.externalDFN{color:inherit;border-bottom:1px dotted #ccc;text-decoration:none}
  a.bibref{text-decoration:none}
  .respec-offending-element:target{animation:pop .25s ease-in-out 0s 1}
  .respec-offending-element,a[href].respec-offending-element{text-decoration:red wavy underline}
  @supports not (text-decoration:red wavy underline){
  .respec-offending-element:not(pre){display:inline-block}
  .respec-offending-element{background:url(data:image/gif;base64,R0lGODdhBAADAPEAANv///8AAP///wAAACwAAAAABAADAEACBZQjmIAFADs=) bottom repeat-x}
  }
  #references :target{background:#eaf3ff;animation:pop .4s ease-in-out 0s 1}
  cite .bibref{font-style:normal}
  code{color:#c63501}
  th code{color:inherit}
  a[href].orcid{padding-left:4px;padding-right:4px}
  a[href].orcid>svg{margin-bottom:-2px}
  .toc a,.tof a{text-decoration:none}
  a .figno,a .secno{color:#000}
  ol.tof,ul.tof{list-style:none outside none}
  .caption{margin-top:.5em;font-style:italic}
  table.simple{border-spacing:0;border-collapse:collapse;border-bottom:3px solid #005a9c}
  .simple th{background:#005a9c;color:#fff;padding:3px 5px;text-align:left}
  .simple th a{color:#fff;padding:3px 5px;text-align:left}
  .simple th[scope=row]{background:inherit;color:inherit;border-top:1px solid #ddd}
  .simple td{padding:3px 10px;border-top:1px solid #ddd}
  .simple tr:nth-child(even){background:#f0f6ff}
  .section dd>p:first-child{margin-top:0}
  .section dd>p:last-child{margin-bottom:0}
  .section dd{margin-bottom:1em}
  .section dl.attrs dd,.section dl.eldef dd{margin-bottom:0}
  #issue-summary>ul{column-count:2}
  #issue-summary li{list-style:none;display:inline-block}
  details.respec-tests-details{margin-left:1em;display:inline-block;vertical-align:top}
  details.respec-tests-details>*{padding-right:2em}
  details.respec-tests-details[open]{z-index:999999;position:absolute;border:thin solid #cad3e2;border-radius:.3em;background-color:#fff;padding-bottom:.5em}
  details.respec-tests-details[open]>summary{border-bottom:thin solid #cad3e2;padding-left:1em;margin-bottom:1em;line-height:2em}
  details.respec-tests-details>ul{width:100%;margin-top:-.3em}
  details.respec-tests-details>li{padding-left:1em}
  a[href].self-link:hover{opacity:1;text-decoration:none;background-color:transparent}
  h2,h3,h4,h5,h6{position:relative}
  aside.example .marker>a.self-link{color:inherit}
  h2>a.self-link,h3>a.self-link,h4>a.self-link,h5>a.self-link,h6>a.self-link{border:none;color:inherit;font-size:83%;height:2em;left:-1.6em;opacity:.5;position:absolute;text-align:center;text-decoration:none;top:0;transition:opacity .2s;width:2em}
  h2>a.self-link::before,h3>a.self-link::before,h4>a.self-link::before,h5>a.self-link::before,h6>a.self-link::before{content:"§";display:block}
  @media (max-width:767px){
  dd{margin-left:0}
  h2>a.self-link,h3>a.self-link,h4>a.self-link,h5>a.self-link,h6>a.self-link{left:auto;top:auto}
  }
  @media print{
  .removeOnSave{display:none}
  }
  </style>
  <meta name="description" content=
  "The Web of Things is applicable to multiple IoT domains, including Smart Home, Industrial, Smart City, Retail, and Health applications, where usage of the W3C WoT standards can simplify the development of IoT systems that combine devices from multiple vendors and ecosystems. During the last charter period of the WoT Working Group several specifications were developed to address requirements for these domains.">
  <link rel="canonical" href="https://www.w3.org/TR/wot-usecases/">
  <style>
  .hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}
  .hljs-comment,.hljs-quote{color:#717277;font-style:italic}
  .hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}
  .hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#ca4706;font-weight:700}
  .hljs-literal{color:#0b76c5}
  .hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#42803c}
  .hljs-built_in,.hljs-class .hljs-title{color:#9a6a01}
  .hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}
  .hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#336ae3}
  .hljs-emphasis{font-style:italic}
  .hljs-strong{font-weight:700}
  .hljs-link{text-decoration:underline}
  </style>
  <style>
  var{position:relative;cursor:pointer}
  var[data-type]::after,var[data-type]::before{position:absolute;left:50%;top:-6px;opacity:0;transition:opacity .4s;pointer-events:none}
  var[data-type]::before{content:"";transform:translateX(-50%);border-width:4px 6px 0 6px;border-style:solid;border-color:transparent;border-top-color:#000}
  var[data-type]::after{content:attr(data-type);transform:translateX(-50%) translateY(-100%);background:#000;text-align:center;font-family:"Dank Mono","Fira Code",monospace;font-style:normal;padding:6px;border-radius:3px;color:#daca88;text-indent:0;font-weight:400}
  var[data-type]:hover::after,var[data-type]:hover::before{opacity:1}
  </style>
  <script id="initialUserConfig" type="application/json">
  {
  "lint": {
    "no-headingless-sections": false
  },
  "specStatus": "ED",
  "noRecTrack": "true",
  "maxTocLevel": 6,
  "processVersion": 2019,
  "shortName": "wot-usecases",
  "copyrightStart": 2020,
  "group": "ig/wot",
  "edDraftURI": "https://w3c.github.io/wot-usecases/",
  "githubAPI": "https://api.github.com/repos/w3c/wot-usecases/",
  "issueBase": "https://www.github.com/w3c/wot-usecases/issues",
  "editors": [
    {
      "name": "Michael Lagally",
      "w3cid": "47166",
      "company": "Oracle Corp.",
      "companyURL": "https://www.oracle.com/"
    },
    {
      "name": "Michael McCool",
      "w3cid": "93137",
      "company": "Intel Corp.",
      "companyURL": "https://www.intel.com/"
    },
    {
      "name": "Ryuichi Matsukura",
      "w3cid": "64284",
      "company": "Fujitsu Ltd.",
      "companyURL": "https://www.fujitsu.com/"
    },
    {
      "name": "Tomoaki Mizushima",
      "w3cid": "98915",
      "company": "Internet Research Institute, Inc.",
      "companyURL": "https://www.iri.co.jp/"
    }
  ],
  "otherLinks": [
    {
      "key": "Contributors",
      "data": [
        {
          "value": "In the GitHub repository",
          "href": "https://github.com/w3c/wot-usecases/graphs/contributors"
        }
      ]
    },
    {
      "key": "Repository",
      "data": [
        {
          "value": "We are on GitHub",
          "href": "https://github.com/w3c/wot-usecases/"
        },
        {
          "value": "File a bug",
          "href": "https://github.com/w3c/wot-usecases/issues"
        },
        {
          "value": "Contribute",
          "href": "https://github.com/w3c/wot-usecases/pulls"
        }
      ]
    }
  ],
  "localBiblio": {
    "JSON-SCHEMA": {
      "title": "JSON Schema Validation: A Vocabulary for Structural Validation of JSON",
      "href": "https://tools.ietf.org/html/draft-handrews-json-schema-validation-01",
      "authors": [
        "Austin Wright",
        "Henry Andrews",
        "Geraint Luff"
      ],
      "status": "Internet-Draft",
      "date": "19 March 2018",
      "publisher": "IETF"
    },
    "ISO-6709": {
      "title": "ISO-6709:2008 : Standard representation of geographic point location by coordinates",
      "href": "https://www.iso.org/standard/39242.html",
      "status": "Published",
      "date": "2008-07",
      "publisher": "ISO"
    },
    "Hybridcast": {
      "title": "Hybridcast",
      "href": "...",
      "authors": [
        "..."
      ],
      "status": "...",
      "date": "..",
      "publisher": "..."
    },
    "NMEA": {
      "title": "National Marine Electronics Association",
      "href": "https://www.nmea.org"
    },
    "WGS84": {
      "title": "WGS84",
      "href": "https://en.wikipedia.org/wiki/World_Geodetic_System"
    },
    "Basic Geo Vocabulary": {
      "title": "W3C Semantic Web Interest Group",
      "href": "https://www.w3.org/2003/01/geo/",
      "publisher": "W3C"
    },
    "W3C Geolocalization API": {
      "title": "Geolocation API Specification 2nd Edition",
      "href": "https://www.w3.org/TR/geolocation-API/",
      "authors": [
        "Andrei Popescu"
      ],
      "status": "Published",
      "date": "8 Nov 2016",
      "publisher": "W3C"
    },
    "Open Geospatial Consortium": {
      "title": "Open Geospatial Consortium",
      "href": "http://docs.opengeospatial.org/as/18-005r4/18-005r4.html"
    },
    "ISO19111": {
      "title": "ISO19111",
      "href": "https://www.iso.org/standard/74039.html",
      "status": "Published",
      "date": "Jan 2019",
      "publisher": "ISO"
    },
    "SSN": {
      "title": "Semantic Sensor Network Ontology",
      "href": "https://www.w3.org/TR/vocab-ssn/",
      "authors": [
        "Armin Haller",
        "Krzysztof Janowicz",
        "Simon Cox",
        "Danh Le Phuoc",
        "Kerry Taylor",
        "Maxime Lefrançois"
      ],
      "status": "Published",
      "date": "19 Oct 2017",
      "publisher": "W3C"
    },
    "Timestamps": {
      "title": "Timestamps",
      "href": "https://w3c.github.io/hr-time/#dom-domhighrestimestamp",
      "authors": [
        "Ilya Grigorik"
      ],
      "status": "Draft",
      "date": "06 Oct 2020",
      "publisher": "W3C"
    },
    "MMI UC3.1": {
      "title": "MMI UC3.1",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    "MMI UC3.2": {
      "title": "MMI UC3.2",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    "ICE F2761-09(2013)": {
      "title": "ICE F2761-09(2013)",
      "publisher": "IEC"
    },
    "OpenICE": {
      "title": "OpenICE",
      "href": "https://www.openice.info"
    },
    "MDIRA": {
      "title": "MDIRA",
      "href": "https://secwww.jhuapl.edu/mdira/documents"
    },
    "MQTT": {
      "title": "MQTT Version 3.1.1 Plus Errata 01",
      "href": "http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html",
      "authors": [
        "Andrew Banks",
        "Rahul Gupta"
      ],
      "status": "Published",
      "date": "December 2015",
      "publisher": "OASIS Standard"
    },
    "OPC UA": {
      "title": "OPC Unified Architecture",
      "href": "https://opcfoundation.org/about/opc-technologies/opc-ua/",
      "publisher": "OPC"
    },
    "BACnet": {
      "title": "BACnet",
      "href": "http://www.bacnet.org",
      "publisher": "ASHRAE"
    },
    "CoAP": {
      "title": "The Constrained Application Protocol (CoAP)",
      "href": "https://tools.ietf.org/html/rfc7252",
      "authors": [
        "Z. Shelby",
        "K. Hartke",
        "C. Bormann"
      ],
      "status": "Published",
      "date": "June 2014",
      "publisher": "IETF"
    },
    "MMI UC5.1": {
      "title": "MMI UC5.1",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    "MMI UC5.2": {
      "title": "MMI UC5.2",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    ".MMI UC1.1": {
      "title": "MMI UC1.1",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    "MMI UC1.2": {
      "title": "MMI UC1.2",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    "MMI UC2.1": {
      "title": "MMI UC2.1",
      "href": "https://www.w3.org/TR/mmi-use-cases/",
      "authors": [
        "Emily Candell, Dave Raggett"
      ],
      "status": "published",
      "date": "2002",
      "publisher": "W3C"
    },
    "IEC 61850": {
      "title": "IEC 61850",
      "publisher": "IEC"
    },
    "IEEE 1574": {
      "title": "IEEE 1574",
      "publisher": "IEEE"
    },
    "KNX": {
      "title": "KNX",
      "href": "https://www.knx.org/knx-en/for-professionals/index.php",
      "publisher": "KNX"
    },
    "Modbus": {
      "title": "Modbus",
      "href": "https://modbus.org",
      "publisher": "Modbus Organization"
    },
    "OGC Sensor Things": {
      "title": "OGC Sensor Things API",
      "href": "https://www.ogc.org/standards/sensorthings",
      "publisher": "Open Geospatial Consortium"
    },
    "OneM2M": {
      "title": "OneM2M",
      "href": "https://www.onem2m.org",
      "publisher": "ETSI"
    },
    "LWM2M": {
      "title": "Lightweight Machine to Machine Technical Specification: Core",
      "href": "http://openmobilealliance.org/release/LightweightM2M/V1_1-20180710-A/OMA-TS-LightweightM2M_Core-V1_1-20180710-A.pdf",
      "date": "Aug 2018",
      "publisher": "OMA SpecWorks."
    },
    "OCF": {
      "title": "OCF Core Specification",
      "href": "https://openconnectivity.org/developer/specifications",
      "date": "April 2019",
      "publisher": "Open Connectivity Foundation"
    }
  },
  "publishISODate": "2021-05-05T00:00:00.000Z",
  "generatedSubtitle": "Editor's Draft 05 May 2021"
  }
  </script>
  <link rel="stylesheet" href=
  "https://www.w3.org/StyleSheets/TR/2016/W3C-IG-NOTE">
</head>
<body class="h-entry toc-inline">
  <div class="head">
    <a class="logo" href="https://www.w3.org/"><img alt="W3C"
    width="72" height="48" src=
    "https://www.w3.org/StyleSheets/TR/2016/logos/W3C"></a>
    <h1 id="title" class="title">Web of Things (WoT): Use Cases and
    Requirements</h1>
    <h2>W3C Interest Group Note <time class="dt-published"
    datetime="2021-05-13">13 May 2021</time></h2>
    <dl>
      <dt>This version:</dt>
      <dd>
          <a class="u-url" href=
          "https://www.w3.org/TR/2021/NOTE-wot-usecases-20210513/">
          https://www.w3.org/TR/2021/NOTE-wot-usecases-20210513/</a>
      </dd>
      <dt>Latest published version:</dt>
      <dd>
        <a href=
        "https://www.w3.org/TR/wot-usecases/">https://www.w3.org/TR/wot-usecases/</a>
      </dd>
      <dt>Latest editor's draft:</dt>
      <dd>
        <a href=
        "https://w3c.github.io/wot-usecases/">https://w3c.github.io/wot-usecases/</a>
      </dd>
      <dt>Editors:</dt>
      <dd class="p-author h-card vcard" data-editor-id="47166">
        <span class="p-name fn">Michael Lagally</span> (<a class=
        "p-org org h-org h-card" href=
        "https://www.oracle.com/">Oracle Corp.</a>)
      </dd>
      <dd class="p-author h-card vcard" data-editor-id="93137">
        <span class="p-name fn">Michael McCool</span> (<a class=
        "p-org org h-org h-card" href=
        "https://www.intel.com/">Intel Corp.</a>)
      </dd>
      <dd class="p-author h-card vcard" data-editor-id="64284">
        <span class="p-name fn">Ryuichi Matsukura</span> (<a class=
        "p-org org h-org h-card" href=
        "https://www.fujitsu.com/">Fujitsu Ltd.</a>)
      </dd>
      <dd class="p-author h-card vcard" data-editor-id="98915">
        <span class="p-name fn">Tomoaki Mizushima</span> (<a class=
        "p-org org h-org h-card" href=
        "https://www.iri.co.jp/">Internet Research Institute,
        Inc.</a>)
      </dd>
      <dt>Contributors</dt>
      <dd>
        <a href=
        "https://github.com/w3c/wot-usecases/graphs/contributors">In
        the GitHub repository</a>
      </dd>
      <dt>Repository</dt>
      <dd>
        <a href="https://github.com/w3c/wot-usecases/">We are on
        GitHub</a>
      </dd>
      <dd>
        <a href="https://github.com/w3c/wot-usecases/issues">File a
        bug</a>
      </dd>
      <dd>
        <a href=
        "https://github.com/w3c/wot-usecases/pulls">Contribute</a>
      </dd>
    </dl>
    <p class="copyright"><a href=
    "https://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a>
    © 2020-2021 <a href="https://www.w3.org/"><abbr title=
    "World Wide Web Consortium">W3C</abbr></a><sup>®</sup>
    (<a href="https://www.csail.mit.edu/"><abbr title=
    "Massachusetts Institute of Technology">MIT</abbr></a>,
    <a href="https://www.ercim.eu/"><abbr title=
    "European Research Consortium for Informatics and Mathematics">ERCIM</abbr></a>,
    <a href="https://www.keio.ac.jp/">Keio</a>, <a href=
    "https://ev.buaa.edu.cn/">Beihang</a>). W3C <a href=
    "https://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">
    liability</a>, <a href=
    "https://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">
    trademark</a> and <a rel="license" href=
    "https://www.w3.org/Consortium/Legal/2015/copyright-software-and-document">
    permissive document license</a> rules apply.</p>
    <hr title="Separator for header">
  </div>
  <section id="abstract" class="introductory">
    <h2>Abstract</h2>
    <p>The Web of Things is applicable to multiple IoT domains,
    including Smart Home, Industrial, Smart City, Retail, and
    Health applications, where usage of the <abbr title=
    "World Wide Web Consortium">W3C</abbr> WoT standards can
    simplify the development of IoT systems that combine devices
    from multiple vendors and ecosystems. During the last charter
    period of the WoT Working Group several specifications were
    developed to address requirements for these domains.</p>
    <p>This Use Case and Requirements Document is created to
    collect new IoT use cases from various domains that have been
    contributed by various stakeholders. These serve as a baseline
    for identifying requirements for the standardization work in
    the <abbr title="World Wide Web Consortium">W3C</abbr> WoT
    groups.</p>
  </section>
  <section id="sotd" class="introductory">
    <h2>Status of This Document</h2>
    <p><em>This section describes the status of this document at
    the time of its publication. Other documents may supersede this
    document. A list of current <abbr title=
    "World Wide Web Consortium">W3C</abbr> publications and the
    latest revision of this technical report can be found in the
    <a href="https://www.w3.org/TR/"><abbr title=
    "World Wide Web Consortium">W3C</abbr> technical reports
    index</a> at https://www.w3.org/TR/.</em></p>
    <p data-deliverer="75874">This document was published by the <a href=
    "https://www.w3.org/WoT/IG/">Web of Things Interest Group</a>
    as an Interest Group Note.</p>
    <p><a href="https://github.com/w3c/me-media-timed-events/issues/">GitHub
    Issues</a> are preferred for discussion of this specification.</p>
    <p>Publication as an Interest Group Note does not imply endorsement
    by the <abbr title="World Wide Web Consortium">W3C</abbr> Membership.</p>
    <p>This is a draft document and may be updated, replaced or
    obsoleted by other documents at any time. It is inappropriate
    to cite this document as other than work in progress.</p>
    <p>The disclosure obligations of the Participants of this group
    are described in the <a href=
    "https://www.w3.org/2019/10/wot-ig-2019.html#patentpolicy">charter</a>.</p>
    <p>This document is governed by the <a id=
    "w3c_process_revision" href=
    "https://www.w3.org/2020/Process-20200915/">15 September 2020
    <abbr title="World Wide Web Consortium">W3C</abbr> Process
    Document</a>.</p>
  </section>
  <nav id="toc">
    <h2 class="introductory" id="table-of-contents">Table of
    Contents</h2>
    <ol class="toc">
      <li class="tocline">
        <a class="tocxref" href="#intro"><bdi class=
        "secno">1.</bdi> Introduction</a>
        <ol class="toc">
          <li class="tocline">
            <a class="tocxref" href="#domains"><bdi class=
            "secno">1.1</bdi> Domains</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#conformance"><bdi class=
            "secno">1.2</bdi> Conformance</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#definitions"><bdi class=
            "secno">1.3</bdi> Terminology, Stakeholders and
            Roles</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href="#terminology"><bdi class=
                "secno">1.3.1</bdi> Terminology</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href="#stakeholders"><bdi class=
                "secno">1.3.2</bdi> Stakeholders and Roles</a>
              </li>
            </ol>
          </li>
        </ol>
      </li>
      <li class="tocline">
        <a class="tocxref" href="#sec-vertical-ucs"><bdi class=
        "secno">2.</bdi> Domain specific Use Cases</a>
        <ol class="toc">
          <li class="tocline">
            <a class="tocxref" href="#agriculture"><bdi class=
            "secno">2.1</bdi> Smart Agriculture</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#smart-agriculture"><bdi class="secno">2.1.1</bdi>
                Greenhouse Horticulture</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#smart-agriculture-openfield"><bdi class=
                "secno">2.1.2</bdi> Open-field agriculture</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#Agricultural-irrigation"><bdi class=
                "secno">2.1.3</bdi> Irrigation in outdoor
                environment</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#smart-city"><bdi class=
            "secno">2.2</bdi> Smart City</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#smartcity-geolocation"><bdi class=
                "secno">2.2.1</bdi> Geolocation</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#smartcity-dashboard"><bdi class=
                "secno">2.2.2</bdi> Dashboard</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-3-1_interactive-public-spaces"><bdi class=
                "secno">2.2.3</bdi> Interactive Public Spaces</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-3-2_meeting-room-event-assistance"><bdi class="secno">
                2.2.4</bdi> Meeting Room Event Assistance</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href="#smart-campus"><bdi class=
                "secno">2.2.5</bdi> Cross-Domain Discovery in a
                Smart Campus</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#smart-buildings"><bdi class=
            "secno">2.3</bdi> Building Technologies</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#smart-building"><bdi class="secno">2.3.1</bdi>
                Smart Building</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#connected-building-energy-efficiency"><bdi class=
                "secno">2.3.2</bdi> Connected Building Energy
                Efficiency</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#smart-building-things"><bdi class=
                "secno">2.3.3</bdi> Automated Smart Building
                Management</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#manufacturing"><bdi class=
            "secno">2.4</bdi> Manufacturing</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href="#big-data"><bdi class=
                "secno">2.4.1</bdi> Manufacturing</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#retail"><bdi class=
            "secno">2.5</bdi> Retail</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#health"><bdi class=
            "secno">2.6</bdi> Health</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#public-health"><bdi class="secno">2.6.1</bdi>
                Public Health</a>
                <ol class="toc">
                  <li class="tocline">
                    <a class="tocxref" href=
                    "#smartcity-health-monitoring"><bdi class=
                    "secno">2.6.1.1</bdi> Public Health
                    Monitoring</a>
                  </li>
                  <li class="tocline">
                    <a class="tocxref" href=
                    "#MedicalDevices"><bdi class=
                    "secno">2.6.1.2</bdi> Interconnected medical
                    devices in a hospital ICU</a>
                  </li>
                </ol>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#private-health"><bdi class="secno">2.6.2</bdi>
                Private Health</a>
                <ol class="toc">
                  <li class="tocline">
                    <a class="tocxref" href=
                    "#mmi-4-1_health-notifiers"><bdi class=
                    "secno">2.6.2.1</bdi> Health Notifiers</a>
                  </li>
                </ol>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#energy"><bdi class=
            "secno">2.7</bdi> Energy</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href="#smart-grid"><bdi class=
                "secno">2.7.1</bdi> Smart Grids</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#transportation"><bdi class=
            "secno">2.8</bdi> Transportation</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#automotive"><bdi class=
            "secno">2.9</bdi> Automotive</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-2-1_smart-car-configuration-management"><bdi class="secno">
                2.9.1</bdi> Smart Car Configuration Management</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#smart-home"><bdi class=
            "secno">2.10</bdi> Smart Home</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#nhk-device-tv-sync"><bdi class=
                "secno">2.10.1</bdi> Home WoT devices synchronize
                to TV programs</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#education"><bdi class=
            "secno">2.11</bdi> Education</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href="#education"><bdi class=
                "secno">2.11.1</bdi> Shared Devices</a>
              </li>
            </ol>
          </li>
        </ol>
      </li>
      <li class="tocline">
        <a class="tocxref" href="#sec-horizontal-ucs"><bdi class=
        "secno">3.</bdi> Use Cases for multiple domains</a>
        <ol class="toc">
          <li class="tocline">
            <a class="tocxref" href="#Discovery"><bdi class=
            "secno">3.1</bdi> Discovery</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#multi-vendor"><bdi class=
            "secno">3.2</bdi> Multi-Vendor System Integration - Out
            of the box interoperability</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#digital-twin"><bdi class=
            "secno">3.3</bdi> Digital Twin</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href=
            "#X-Protocol-Interworking"><bdi class="secno">3.4</bdi>
            Cross Protocol Interworking</a>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#multimodal"><bdi class=
            "secno">3.5</bdi> Multimodal System Integration</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-5-1_multimodal-recognition-support"><bdi class="secno">
                3.5.1</bdi> Multimodal Recognition Support</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-5-2_enhancement-of-synergistic-interactions"><bdi class="secno">
                3.5.2</bdi> Enhancement of Synergistic
                Interactions</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#accessibility"><bdi class=
            "secno">3.6</bdi> Accessibility</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-1-1_audiovisual-devices-as-smartphone-extensions">
                <bdi class="secno">3.6.1</bdi> Audiovisual Devices
                Acting as Smartphone Extensions</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#mmi-1-2_unified-smart-home-control-and-status"><bdi class="secno">
                3.6.2</bdi> Unified Smart Home Control and Status
                Interface</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#Security"><bdi class=
            "secno">3.7</bdi> Security</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href="#oauth"><bdi class=
                "secno">3.7.1</bdi> OAuth2 Flows</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#lifecycle"><bdi class=
            "secno">3.8</bdi> Lifecycle</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#device-lifecycle"><bdi class="secno">3.8.1</bdi>
                Device Lifecycle</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#VR/AR"><bdi class=
            "secno">3.9</bdi> VR/AR</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href="#ar-guide"><bdi class=
                "secno">3.9.1</bdi> AR Virtual Guide</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#edge-computing"><bdi class=
            "secno">3.10</bdi> Edge Computing</a>
          </li>
        </ol>
      </li>
      <li class="tocline">
        <a class="tocxref" href="#requirements"><bdi class=
        "secno">4.</bdi> Requirements</a>
        <ol class="toc">
          <li class="tocline">
            <a class="tocxref" href=
            "#sec-functional-requirement"><bdi class=
            "secno">4.1</bdi> Functional Requirements</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-principles"><bdi class=
                "secno">4.1.1</bdi> Common Principles</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-thing-functionalities"><bdi class="secno">
                4.1.2</bdi> Thing Functionalities</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-search-and-discovery"><bdi class="secno">
                4.1.3</bdi> Search and Discovery</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-description-mechanism"><bdi class="secno">
                4.1.4</bdi> Description Mechanism</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-description-of-attributes"><bdi class="secno">
                4.1.5</bdi> Description of Attributes</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-description-of-functionalities"><bdi class="secno">
                4.1.6</bdi> Description of Functionalities</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-network"><bdi class=
                "secno">4.1.7</bdi> Network</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-deployment"><bdi class=
                "secno">4.1.8</bdi> Deployment</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-application"><bdi class=
                "secno">4.1.9</bdi> Application</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#sec-requirements-legacy-adoption"><bdi class=
                "secno">4.1.10</bdi> Legacy Adoption</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href=
            "#sec-technical-requirements"><bdi class=
            "secno">4.2</bdi> Technical Requirements</a>
            <ol class="toc">
              <li class="tocline">
                <a class="tocxref" href=
                "#components-in-the-web-of-things-and-the-web-of-things-architecture">
                <bdi class="secno">4.2.1</bdi> Components in the
                Web of Things and the Web of Things
                Architecture</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href="#devices"><bdi class=
                "secno">4.2.2</bdi> Devices</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href="#applications"><bdi class=
                "secno">4.2.3</bdi> Applications</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#digital-twins"><bdi class="secno">4.2.4</bdi>
                Digital Twins</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href="#discovery"><bdi class=
                "secno">4.2.5</bdi> Discovery</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href="#security"><bdi class=
                "secno">4.2.6</bdi> Security</a>
              </li>
              <li class="tocline">
                <a class="tocxref" href=
                "#accessibility-0"><bdi class="secno">4.2.7</bdi>
                Accessibility</a>
              </li>
            </ol>
          </li>
          <li class="tocline">
            <a class="tocxref" href="#acknowledgements"><bdi class=
            "secno">4.3</bdi> Acknowledgments</a>
          </li>
        </ol>
      </li>
      <li class="tocline">
        <a class="tocxref" href="#references"><bdi class=
        "secno">A.</bdi> References</a>
        <ol class="toc">
          <li class="tocline">
            <a class="tocxref" href=
            "#normative-references"><bdi class="secno">A.1</bdi>
            Normative references</a>
          </li>
        </ol>
      </li>
    </ol>
  </nav>
  <section id="intro">
    <h2 id="x1-introduction"><bdi class="secno">1.</bdi>
    Introduction<a class="self-link" aria-label="§" href=
    "#intro"></a></h2>
    <p>The World Wide Web Consortium (<abbr title=
    "World Wide Web Consortium">W3C</abbr>) has published the Web
    of Things (WoT) Architecture and Web of Things (WoT) Thing
    Description (TD) as official <abbr title=
    "World Wide Web Consortium">W3C</abbr> Recommendations in May
    2020. These specifications enable easy integration across
    Internet of Things platforms and applications.</p>
    <p>The <abbr title="World Wide Web Consortium">W3C</abbr> Web
    of Thing Architecture [<cite><a class="bibref" data-link-type=
    "biblio" href="#bib-wot-architecture" title=
    "Web of Things (WoT) Architecture">wot-architecture</a></cite>]
    defines an abstract architecture, the WoT Thing Description
    [<cite><a class="bibref" data-link-type="biblio" href=
    "#bib-wot-thing-description" title=
    "Web of Things (WoT) Thing Description">wot-thing-description</a></cite>]
    defines a format to describes a broad spectrum of very
    different devices, which may be connected over various
    protocols.</p>
    <p>During the inception phase of the WoT 1.0 specifications in
    2017-2018 the WoT IG collected use cases and requirements to
    enable interoperability of Internet of Things (IoT) services on
    a worldwide basis. These released specifications have been
    created to address the use cases and requirements for the first
    version of the WoT specifications, which are documented in
    <a href=
    "https://w3c.github.io/wot/ucr-doc/">https://w3c.github.io/wot/ucr-doc/</a></p>
    <p>The present document gathers and describes new use cases and
    requirements for future standardization work in the WoT
    standard.</p>
    <p>This document contains chapters describing the use cases
    that were contributed by multiple authors, functional and
    technical requirements on the Web of Things standards.
    Additionally it contains a summary of the liaisons, where
    active collaboration is taking place at the time of writing.
    Since this document is a WG note, additional use cases will be
    added in future revisions of this document.</p>
    <section id="domains">
      <h3 id="x1-1-domains"><bdi class="secno">1.1</bdi>
      Domains<a class="self-link" aria-label="§" href=
      "#domains"></a></h3>
      <p>The collection of use cases can be separated into two
      categories:</p>
      <ul>
        <li>Domain specific (vertical) use cases for a single
            application domain.
        <ul>
          <li>Agriculture</li>
          <li>Smart City</li>
          <li>Smart Buildings</li>
          <li>Manufacturing</li>
          <li>Retail</li>
          <li>Health</li>
          <li>Energy</li>
          <li>Transportation</li>
          <li>Automotive</li>
          <li>Smart Home</li>
          <li>Education</li>
        </ul>
        </li>
        <li>Horizontal use cases that address multiple
            domains.
        <ul>
          <li>Discovery</li>
          <li>Multi-Vendor System Integration</li>
          <li>Digital Twin</li>
          <li>Cross Protocol Interworking</li>
          <li>Multimodal System Integration</li>
          <li>Accessibility</li>
          <li>Security</li>
          <li>Lifecycle</li>
          <li>VR/AR</li>
          <li>Edge Computing</li>
        </ul>
        </li>
      </ul>Domain specific use cases are described in <a href=
      "#sec-vertical-ucs" class="sec-ref">§&nbsp;<bdi class=
      "secno">2.</bdi> Domain specific Use Cases</a>, horizontal
      use cases are described in <a href="#sec-horizontal-ucs"
      class="sec-ref">§&nbsp;<bdi class="secno">3.</bdi> Use Cases
      for multiple domains</a>
    </section>
    <section id="conformance">
      <h3 id="x1-2-conformance"><bdi class="secno">1.2</bdi>
      Conformance<a class="self-link" aria-label="§" href=
      "#conformance"></a></h3>
      <p>As well as sections marked as non-normative, all authoring
      guidelines, diagrams, examples, and notes in this
      specification are non-normative. Everything else in this
      specification is normative.</p>
      <p>The key words <em class="rfc2119">MUST NOT</em>,
      <em class="rfc2119">SHOULD</em>, and <em class=
      "rfc2119">SHOULD NOT</em> in this document are to be
      interpreted as described in <a href=
      "https://datatracker.ietf.org/doc/html/bcp14">BCP 14</a>
      [<cite><a class="bibref" data-link-type="biblio" href=
      "#bib-rfc2119" title=
      "Key words for use in RFCs to Indicate Requirement Levels">RFC2119</a></cite>]
      [<cite><a class="bibref" data-link-type="biblio" href=
      "#bib-rfc8174" title=
      "Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words">RFC8174</a></cite>]
      when, and only when, they appear in all capitals, as shown
      here.</p>
    </section>
    <section id="definitions">
      <h3 id="x1-3-terminology-stakeholders-and-roles"><bdi class=
      "secno">1.3</bdi> Terminology, Stakeholders and
      Roles<a class="self-link" aria-label="§" href=
      "#definitions"></a></h3>
      <section id="terminology">
        <h4 id="x1-3-1-terminology"><bdi class="secno">1.3.1</bdi>
        Terminology<a class="self-link" aria-label="§" href=
        "#terminology"></a></h4>The present document uses the
        terminology from WoT Architecture [<cite><a class="bibref"
        data-link-type="biblio" href="#bib-wot-architecture" title=
        "Web of Things (WoT) Architecture">wot-architecture</a></cite>].
      </section>
      <section id="stakeholders">
        <h4 id="x1-3-2-stakeholders-and-roles"><bdi class=
        "secno">1.3.2</bdi> Stakeholders and Roles<a class=
        "self-link" aria-label="§" href="#stakeholders"></a></h4>
        <p>The following stakeholders and actors were identified
        when the use cases have been collected and requirements
        were identified. Note that these stakeholders and roles may
        overlap in some use cases.</p>
      </section>
      <ul>
        <li>device owners</li>
        <li>device user</li>
        <li>cloud provider</li>
        <li>service provider</li>
        <li>device manufacturer</li>
        <li>gateway manufacturer</li>
        <li>identity provider</li>
        <li>directory service operator</li>
      </ul>
    </section>
  </section>
  <section id="sec-vertical-ucs">
    <h2 id="x2-domain-specific-use-cases"><bdi class=
    "secno">2.</bdi> Domain specific Use Cases<a class="self-link"
    aria-label="§" href="#sec-vertical-ucs"></a></h2>
    <section id="agriculture">
      <h3 id="x2-1-smart-agriculture"><bdi class="secno">2.1</bdi>
      Smart Agriculture<a class="self-link" aria-label="§" href=
      "#agriculture"></a></h3>
      <section id="smart-agriculture">
        <h4 id="x2-1-1-greenhouse-horticulture"><bdi class=
        "secno">2.1.1</bdi> Greenhouse Horticulture<a class=
        "self-link" aria-label="§" href=
        "#smart-agriculture"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Ryuichi Matsukura, Takuki Kamiya</dd>
          <dt>Target Users</dt>
          <dd>Agricultural corporation, Farmer, Manufacturers
          (Sensor, other facilities), Cloud provider</dd>
          <dt>Motivation</dt>
          <dd>Greenhouse Horticulture controlled by computers can
          create an optimal environment for growing plants. This
          enables to improve productivity and ensure stable
          vegetable production throughout the year, independent of
          the weather. This is the result of research on the growth
          of plants in the 1980s. For example, in tomatoes,
          switching to hydroponics and optimizing the temperature,
          humidity and CO2 concentration required for
          photosynthesis resulted in a five times increase in
          yield. The growth conditions for other vegetables also
          have been investigated, and this control system is
          applied now.</dd>
          <dt>Expected Devices</dt>
          <dd>Sensors (temperature, humidity, brightness, UV
          brightness, air pressure, and CO2) Heating, CO2
          generator, open and close sunlight shielding sheet.</dd>
          <dt>Expected Data</dt>
          <dd>Sensors values to clarify the gaps between conditions
          for maximizing photosynthesis and the current
          environment. Following sensors values at one or some
          points in the greenhouse: temperature, humidity,
          brightness, and CO2.</dd>
          <dt>Dependencies</dt>
          <dd>WoT Architecture<br>
          WoT Thing Description</dd>
          <dt>Description</dt>
          <dd>Sensors and some facilities like heater, CO2
          generator, sheet controllers are connected to the gateway
          via wired or wireless networks. The gateway is connected
          to the cloud via the Internet. All sensors and facilities
          can be accessed and controlled from the cloud. To
          maximize photosynthesis, the temperature, CO2
          concentration, and humidity in the greenhouse are mainly
          controlled. When the sunlight comes in the morning and
          CO2 concentration inside decreases, the application turns
          on the CO2 generator to keep over 400 ppm, the same as
          the air outside. The temperature in the greenhouse is
          adjusted by controlling the heater and the sunlight
          shielding sheet. The cloud gathers all sensor data and
          the status of the facilities. The application makes the
          best configuration for the region of the greenhouse
          located.</dd>
          <dt>Gaps</dt>
          <dd>In the case of the wireless connection to the
          sensors, the gateway should keep the latest value of the
          sensors since the wireless connection is sometimes
          broken. The gateway can create a virtual entity
          corresponding to the sensor and allow the application to
          access this virtual entity having the actual sensor
          status like sleeping.</dd>
        </dl>
      </section>
      <section id="smart-agriculture-openfield">
        <h4 id="x2-1-2-open-field-agriculture"><bdi class=
        "secno">2.1.2</bdi> Open-field agriculture<a class=
        "self-link" aria-label="§" href=
        "#smart-agriculture-openfield"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Cristiano Aguzzi</dd>
          <dt>Target Users</dt>
          <dd>Agricultural corporation, Farmer, Manufacturers
          (Sensor, other facilities), Cloud provider, Middleware
          provider, Network providers, service provider.</dd>
          <dt>Motivation</dt>
          <dd>Water is vital for ensuring food security to the
          world’s population, and agriculture is the biggest
          consumer amounting for 70% of freshwater. Field
          irrigation application methods are one of the main causes
          of water wastage. The most common technique, surface
          irrigation, wastes a high percentage of the water by
          wetting areas where no plants benefit from it. On the
          other hand, localized irrigation can use water more
          efficiently and effectively, avoiding both
          under-irrigation and over-irrigation. However, in an
          attempt to avoid under-irrigation, farmers feed more
          water than is needed resulting not only to productivity
          losses, but also water wastages. Therefore, technology
          should be developed and deployed for sensing water needs
          and automatically manage water supply to crops. However,
          open field agriculture is characterized by a quite
          dynamic range of requirements. Usually, solutions
          developed for one particular crop type cannot be reused
          in other cultivations. Moreover, the same field can have
          different crop types or different sizes/shapes during the
          years, meaning that technology to monitor the state of
          crop growth should be highly configurable and adaptive.
          Even agriculture and irrigation methods can change and
          also they are very different depending on the size of the
          field and its clime type. Consequently, silos
          applications are deployed leveraging on IoT technologies
          to gather data about the crop growth state and irrigation
          needs. The Web of Things may help to create a single
          platform where cost-effective applications could adapt
          seamlessly between different scenarios, breaking the
          silos and giving value both to the environment and the
          market.</dd>
          <dt>Expected Devices</dt>
          <dd>
            <p>Sensors:</p>
            <ul>
              <li>Weather sensors (maybe collected together inside
              a <a href=
              "https://en.wikipedia.org/wiki/Weather_station)">weather
              station</a>)
              </li>
              <ul>
                <li>temperature</li>
                <li>air humidity</li>
                <li>air pressure</li>
                <li>pluviometer</li>
                <li>global solar radiation</li>
                <li>anemometer (wind speed)</li>
                <li>wind direction</li>
                <li>global solar radiation and photosynthetically
                active radiation</li>
                <li>gas/air quality sensor (i.e. CO2)</li>
              </ul>
              <li>Soil sensors (usually packed together in soil
              probes)</li>
              <ul>
                <li>soil temperature</li>
                <li>soil moisture/water content</li>
                <li>soil conductivity (detecting salt levels in the
                soil)</li>
                <li>water table sensor</li>
              </ul>
              <li>Drone sensors</li>
              <ul>
                <li>camera</li>
                <li>temperature sensitive camera</li>
                <li>multispectral camera</li>
              </ul>
            </ul>
            <p>Actuators:</p>
            <ul>
              <li>drones: used for data collection or
              pesticed/impollination</li>
              <li>sprinklers</li>
              <li>pumps</li>
              <li>central pivot sprinklers</li>
              <li>hose-reel irrigation machine</li>
            </ul>
            <p>Additional devices:</p>
            <ul>
              <li>Solar panels</li>
              <li>Loggers: units that collect data from close
              sensors.</li>
              <li>Gateways</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            Sensor data plays a central role in Smart Agriculture.
            In particular, it is critical that the information
            sensed is associated with a timestamp. Common
            algorithms use *time series* to calculate the water
            needs of a crop. Furthermore, soil sensors usually are
            calibrated over a specific soil type (which may differ
            even in the same geographic region). For example, the
            calibration data for a soil moisture sensor is
            represented by a function that maps sensor output to
            soil water content. In literature, this function is
            knowns as a *calibration curve*. Commercial sensors are
            precalibrated with a "standard" curve but on most
            occasions, it fails to accurately measure the water
            content. Therefore, it can be configured during the
            installation phase (which may happen every time the
            soil is plowed). Finally, a crucial aspect is
            forecasting. Farmers use this information to actively
            change their management procedures. Services exploit it
            to suggest irrigation schedule or change device
            settings to behave accordingly to environmental
            changes. To summarize here it is a list of most
            important expected data from Open field agriculture:
            <ul>
              <li>Calibration curve</li>
              <li>Time series</li>
              <li>Forecast data</li>
              <li>Geolocations: sensor data must be contextualized
              in geolocation. Also, geolocation is critical in
              massive open fields to localize instrument
              position.</li>
              <li>Weather data</li>
              <li>Unit of measure: commercial soli sensor may
              output their value in a different unit of measures
              (i.e. volts or % water in an m^3 of soil)</li>
              <li>Relative values</li>
              <li>Depth position: geolocation is not sufficient to
              describe the parameters of the soil. Depth is an
              additional context that should be added to an
              observed value.</li>
              <li>Device owner information</li>
              <li>Battery level and energy consumption</li>
            </ul>
          </dd>
          <dt>Dependencies</dt>
          <dd>WoT Architecture, WoT Thing Description</dd>
          <dt>Description</dt>
          <dd>
            In open-field agriculture, the IoT solutions leverage
            on different radio protocols and devices. Usually,
            radio protocols should cover long distances (even
            kilometers) and be energy efficient. Devices too need
            to be energy saving as they are deployed for months and
            sometimes even years in harsh environments. A
            sleeping-cycle is one mechanism they use to save energy
            usually coordinated by *loggers/gateways* or
            preprogrammed. *Loggers* are deployed closed to sensor
            devices and have more storage space. They serve as
            buffers between sensors and higher services. Often
            *loggers* and sensors are embedded in the same board,
            otherwise, they are connected using cables or
            close-ranged radio protocols. On the other hand,
            *gateways* serve as a collection point for data of an
            entire field or farm. They are much more capable
            devices and usually are more energy-consuming. In some
            deployment scenarios, they host a full operating system
            with multiple software facilities installed. Otherwise,
            gateways only serve as relays of data sent from the
            loggers and sensors to cloud services and vice-versa.
            The cloud services may be partially hosted in edge
            servers to preserve data privacy and responsiveness of
            the whole IoT solution. Possible cloud services are:
            <ul>
              <li>Weather forecasting/local weather
              forecasting</li>
              <li>Soil digital twin to simulate and predict water
              content</li>
              <li>Plant digital twin (growth and water needs
              prediction)</li>
              <li>Irrigation advice service: combining the previous
              services and knowing the irrigation system topology
              is possible to advise farms with the best times to
              irrigate a crop.</li>
              <li>Pesticide and fertilize planning</li>
            </ul>The complete deployment topology of an open field
            agriculture solution is described in the diagram
            below:<br>
            <br>
            <img src="./images/Agriculture.svg" width="100%"
            height="100%"><br>
            <br>
          </dd>
          <dt>Variants</dt>
          <dd>
            Open-field agriculture varies a lot between
            geographical location and methods. For example in the
            <a href="http://swamp-project.org/">SWAMP project</a>
            there three different pilots with different
            requirement/constraints:
            <ul>
              <li>
                <a href=
                "http://swamp-project.org/cbec/)%20(Reggio%20Emilia%20region">
                Italian pilot</a>:
              </li>
              <ul>
                <li>Relative small field size</li>
                <li>Multiple connectivity solutions available: 4G,
                LPWAN, and WiFi</li>
                <li>Variance in crop types, sometimes even inside
                the same farm</li>
                <li>Small soil type variance</li>
                <li>Precise model soil behavior</li>
                <li>A great influence of the water table</li>
                <li>Variance in the irrigation system</li>
                <li>Channel-based water distribution</li>
                <li>The main goal is to optimize water
                consumption</li>
              </ul>
              <li>
                <a href=
                "http://swamp-project.org/matopiba/)%20(Matopiba%20and%20Guaspari%20location">
                Brazilian pilot</a>:
              </li>
              <ul>
                <li>Huge field size</li>
                <li>Centra pivot irrigation systems: need to
                optimize each sprinkler output</li>
                <li>Soil type variance within the same field</li>
                <li>A low number of connectivity options: no 4G,
                only radio communication base on LPWAN</li>
                <li>Low crop type variance</li>
                <li>the main goal is to optimize energy
                consumption</li>
              </ul>
              <li>
                <a href="http://swamp-project.org/intercrop/">Spain
                pilot</a>
              </li>
              <ul>
                <li>Efficient localized irrigation and application
                of the right amount of water to the crop</li>
                <li>arid location</li>
                <li>The goal is to minimize water consumption but
                maintaining a good field yield.</li>
              </ul>
            </ul>
          </dd>
          <dt>Gaps</dt>
          <dd>Currently, there is no specification on how to model
          device status (i.e. connected/disconnected) Examples of
          how to handle a device calibration phase may help
          developers to use a standardized approach. Possibly
          define standard links types to define the relation
          between loggers and sensors Handle both geographical
          position and depth information. Ontology class for
          battery and energy consumption Model historical and
          forecast data</dd>
          <dt>Existing Standards</dt>
          <dd>
            <ul>
              <li>
                <a href=
                "https://tools.ietf.org/html/rfc8376">LPWAN</a>
              </li>
              <li>
                <a href=
                "http://www.sdi-12.org/current_specification/SDI-12_version-1_4-Jan-10-2019.pdf">
                SDI 12</a>
              </li>
              <li>
                <a href=
                "https://tools.ietf.org/html/rfc7252">CoAP</a>
              </li>
              <li>
                <a href=
                "https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html">
                MQTT</a>
              </li>
            </ul>
          </dd>
          <dt>Comments</dt>
          <dd>
            This use case is designed using the experience gained
            in the European-Brazil Horizon 2020 SWAMP project.
            Please follow the <a href=
            "http://swamp-project.org/">link</a> for further
            information. Since SWAMP is heavily oriented to
            optimize water consumption, this document just
            mentioned issues like plant feeding, fertilizing,
            pollination, yield prediction, crop quality
            measurement, etc. Nevertheless, WoT technologies may be
            employed also in these scenarios.
          </dd>
        </dl>
      </section>
      <section id="Agricultural-irrigation">
        <h4 id="x2-1-3-irrigation-in-outdoor-environment">
        <bdi class="secno">2.1.3</bdi> Irrigation in outdoor
        environment<a class="self-link" aria-label="§" href=
        "#Agricultural-irrigation"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>
            <ul>
              <li>Catherine Roussey</li>
              <li>Jean-Pierre Chanet</li>
            </ul>
          </dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device users: farmers</li>
              <li>service provider</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>Depending on the type of crops (e.g. maize),
          cultivated plots may need specific irrigation processes
          in outdoor environments. Depending on the country there
          exist some specific pedo-climatic conditions and some
          water consumption restrictions. Thus an irrigation system
          is installed on the plot. It is used on a several days
          basis (e.g. every 7 days), for each plot. The goal is to
          optimize the irrigation decision based on the crop
          development stage and the quantity of rain that has
          already fallen down on the plot. For example an important
          rain may postpone the irrigation decision.<br>
          <br>
          This use case aims to evaluate the number of days to
          delay the irrigation system, in addition to the basis
          irrigation frequency (e.g. 2 delay days means 9 days
          between two irrigations).</dd>
          <dt>Expected Devices</dt>
          <dd>
            <ul>
              <li>6 tensiometers in the plot (soil moisture):</li>
              <ul>
                <li>3 tensiometers at 30 cm depth</li>
                <li>3 tensiometer at 60 cm depth</li>
              </ul>
              <li>1 weather station:</li>
              <ul>
                <li>thermometer (outdoor temperature)</li>
                <li>pluviometer (rain quantity)</li>
              </ul>
              <li>1 mobile pluviometer (quantity of water provided
              by the watering system)</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            To decide when to water a cultivated plot, we evaluate
            the crop growth stage, the root zone moisture level and
            the number of delay days:
            <ul>
              <li>To evaluate the <b>Crop growth stage</b>, we
              need:</li>
              <ul>
                <li>Min and max temperature per day: the <b>min
                temperature per day</b> is evaluated on the period
                [d-1 18:00, d 18:00[. The <b>*max temperature per
                day</b> is evaluated on the period [d 06:00:00, d+1
                06:00:00[.i</li>
                <li><b><a href=
                "https://en.wikipedia.org/wiki/Growing_degree-day">Growing
                degree day</a></b> values uses min and max
                temperature per day, the sowing day and the type of
                seed. The Growing degree day is compared to some
                thresholds to evaluate the crop growth stage</li>
              </ul>
              <li>To evaluate the <b>Root zone moisture level</b>,
              we need:</li>
              <ul>
                <li>Mean moisture per day per probe: in order to
                get reliable values, each tensiometer sends several
                measurements of soil moisture, at fixed hours of
                the day (usually in the morning), that are
                aggregated; their mean value is considered</li>
                <li>For the set of 3 tensiometers localised at the
                same level of depth, the median value is evaluated
                from their mean per day moisture measurements. One
                tensiometer may not provide accurate values (the
                soil around the probe is too dry and the soil
                matter is not connected to the probe). The median
                value of three different tensiometers at the same
                depth will improve the accuracy of the moisture
                measurement.</li>
                <li>Then the sum of the two median values at two
                different depths is evaluated, to take into account
                the quantity of water available in the root zone
                volume. This aggregated value estimates the root
                zone moisture level.</li>
                <li>The root zone moisture level is compared to
                some thresholds (dependent on the crop growth
                stage) to evaluate if the crop needs water or not
                at the end of the basis irrigation period.</li>
              </ul>
              <li>To determine the <b>number of delay days</b>, we
              need:</li>
              <ul>
                <li>The time period between two waterings of the
                same plot is dependent on the farm and known by the
                farmer. When a watering is launched, no new
                watering should be planned during the basic
                irrigation frequency. The quantity of rain that
                falls down on the plot may postpone the watering
                plan. The total quantity of rain per day is
                compared to some thresholds to determine the number
                of delay days.</li>
              </ul>
            </ul>The mobile pluviometer is used to validate that
            the quantity of water received by the crop actually
            corresponds to the quantity of water provided by the
            watering system.<br>
            <br>
            At the end, the farmer may decide if he follows the
            irrigation recommendations or not. He could force the
            watering for one of the next days.<br>
          </dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>
            <ul>
              <li>WoT Architecture: wireless communication in
              outdoor environments presents some issues:
              communication consumes lots of energy, sensor nodes
              have limited energy, weather conditions impact
              communication quality</li>
              <li>WoT Thing Description: the affordance should be
              precise enough to describe the soil at a specific
              depth or the root zone volume or the min temperature
              per day</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>To avoid Property right and consent management issues
          between farmers and cloud service providers on these
          computed data, sensors are connected to the farm
          infrastructure and the services that evaluate aggregated
          data are executed locally on this infrastructure.<br>
          <br>
          The weather station may be located outside of the
          farm.<br>
          <br>
          The tensiometers are located inside the farm. The
          tensiometers and the mobile pluviometer are connected
          using wireless communication to the gateway. The gateway
          sends the measurements to the farm infrastructure.</dd>
          <dt>Variants:</dt>
          <dd>The crop growth stage may be observed by the farmer.
          In this case, he can force this value to update the
          service inputs.</dd>
          <dt>Security Considerations</dt>
          <dd>The 6 tensiometers and 1 pluviometer are installed on
          the plot, but only the farmer should be able to change
          their configurations (frequency of communication).
          Wireless communication should be used but the measurement
          data should only be accessible through the farm network
          infrastructure.</dd>
          <dt>Privacy Considerations</dt>
          <dd>Data concerning quantity of water, type of seed,
          sowing day should be protected.</dd>
          <dt>Gaps</dt>
          <dd>The main potential issues come from tensiometers
          located in the plot, as they are known to be cheap and
          easy to use probes but not always reliable. They can face
          multiple issues: if the soil gets too dry or the probe is
          improperly installed, there may be air between the probe
          and the soil, therefore preventing the probe from
          providing accurate conductivity measurements.<br>
          <br>
          To be sure of the quality of those measurements each
          tensiometer sends its measurements several times (3 to 5)
          per day. The tensiometer may send an inappropriate value
          due to the bad connection between the soil and the probe,
          that is the reason why three tensiometers are used and
          the median value is computed. If the gateway does not
          receive the value of one sensor during a whole day, an
          alert should be sent. To take an irrigation decision, at
          least one measurement per sensor and per day should be
          provided.<br>
          <br>
          The gateway can create a virtual entity corresponding to
          the sensor and allow the application to access this
          virtual entity having the actual sensor status like
          sleeping.<br>
          <br>
          Sensor nodes deployed in outdoor environments may take
          into account that their energy supply device (battery,
          solar panel) constrains the lifetime of the device. Thus
          they should be able to alert that they may not be able to
          provide a service due to lack of energy or they should be
          able to change their configuration and switch
          communication protocols to save as much energy as
          possible.<br>
          <br>
          Moreover wireless communication can be impacted by
          weather conditions or any outdoor conditions. For example
          a tractor that comes too close to the sensor node may
          move the communication device and destroy some
          components. Some kind of network supervision must be
          achieved (for instance by the gateway) to check node
          availability.</dd>
          <dt>Existing Standards</dt>
          <dd>
            <ul>
              <li>
                <a href="https://www.w3.org/TR/vocab-ssn/">Semantic
                Sensor Network</a>
              </li>
              <li>
                <a href=
                "https://saref.etsi.org/saref4agri/v1.1.2/">SAREF4Agri</a>
              </li>
              <li>
                <a href="https://www.w3.org/TR/prov-o/">PROV-O</a>
              </li>
              <li>
                <a href=
                "https://irstea.github.io/caso/OnToology/ontology/caso.owl/documentation/index-en.html">
                CASO</a>
              </li>
              <li>
                <a href="http://www.w3id.org/def/irrig">IRRIG</a>
              </li>
            </ul>The CASO and IRRIG ontologies extend SSN, PROV-O
            and SAREF4AGRI to implement an irrigation expert
            system.<br>
            <br>
            A thesaurus climate and forecast that describes the
            weather properties and associated phenomenon is
            available at <a href=
            "http://vocab.nerc.ac.uk/collection/P07/">http://vocab.nerc.ac.uk/collection/P07/</a>.<br>

            <br>
            The weather measurements provided by the agricultural
            weather station of Agrotechnopole is available at
            <a href=
            "http://ontology.irstea.fr/weather/snorql/">http://ontology.irstea.fr/weather/snorql/</a>.
            [5]
          </dd>
          <dt>Comments</dt>
          <dd>
            This use case has been implemented in France, following
            local conditions and regulations. There exists an open
            manual irrigation decision method called <a href=
            "http://www.irrinov.arvalisinstitutduvegetal.fr/irrinov.asp">
            IRRINOV®</a> developed by Arvalis [2] and INRAE
            dedicated to France and some specific crops: maize,
            wheat and cereals, potatoes and beans.<br>
            <br>
            IRRINOV® can be automated using wireless sensor
            networks and semantic web technologies. The considered
            network is of star type: all sensors can communicate
            with a common gateway, which is connected to the
            Internet. The IRRINOV® implementation was developed in
            [3]. This work presents an expert system for maize
            using drools. It automates the irrigation decision for
            maize based on sensor measurements.<br>
            <br>
            To measure weather properties, we use the
            recommendation provided by the French National Weather
            Institute: Météo France[4]. Its web site defines how to
            evaluate the min and max temperatures per day in
            <a href=
            "http://www.meteofrance.fr/publications/glossaire/154123-temperature-minimale">
            http://www.meteofrance.fr/publications/glossaire/154123-temperature-minimale</a>
            (in French, we found no equivalent description in
            English).
          </dd>
          <dt>References</dt>
          <dd>
            [1] <a href=
            "https://www.inrae.fr/">https://www.inrae.fr/</a><br>
            [2] <a href=
            "https://www.arvalisinstitutduvegetal.fr/">https://www.arvalisinstitutduvegetal.fr/</a><br>

            [3] Q-D. Nguyen, C. ROUSSEY, M. Poveda-Villalón, C. de
            Vaulx , J-P. Chanet. Development Experience of a
            Context-Aware System for Smart Irrigation Using CASO
            and IRRIG Ontologies. Applied Science 2020, 10(5),
            1803; <a href=
            "https://doi.org/10.3390/app10051803">https://doi.org/10.3390/app10051803</a><br>

            [4] <a href=
            "http://www.meteofrance.fr/">http://www.meteofrance.fr/</a><br>

            [5] C. ROUSSEY,S. BERNARD, G. ANDRÉ, D. BOFFETY.
            Weather Data Publication on the LOD using SOSA/SSN
            Ontology.Semantic Web Journal, 2019 <a href=
            "http://www.semantic-web-journal.net/content/weather-data-publication-lod-using-sosassn-ontology-0">
            http://www.semantic-web-journal.net/content/weather-data-publication-lod-using-sosassn-ontology-0</a>
          </dd>
        </dl>
      </section>
    </section>
    <section id="smart-city">
      <h3 id="x2-2-smart-city"><bdi class="secno">2.2</bdi> Smart
      City<a class="self-link" aria-label="§" href=
      "#smart-city"></a></h3>
      <section id="smartcity-geolocation">
        <h4 id="x2-2-1-geolocation"><bdi class="secno">2.2.1</bdi>
        Geolocation<a class="self-link" aria-label="§" href=
        "#smartcity-geolocation"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Jennifer Lin, Michael McCool</dd>
          <dt>Target Users</dt>
          <dd>
            <p>A Smart City managing mobile devices and sensors,
            including passively mobile sensor packs, packages,
            vehicles, and autonomous robots, where their location
            needs to be determined dynamically.</p>
          </dd>
          <dt>Motivation</dt>
          <dd>
            <p>Smart Cities need to track a large number of mobile
            devices and sensors. Location information may be
            integrated with a logistics or fleet management system.
            A reusable geolocation module is needed with a common
            network interface to include in these various
            applications. For outdoor applications, GPS could be
            used, but indoors other geolocation technologies might
            be used, such as WiFi triangulation or vision-based
            navigation (SLAM). Therefore the geolocation
            information should be technology-agnostic.</p>
            <p>NOTE: we prefer the term "geolocation", even
            indoors, over "localization" to avoid confusion with
            language localization.</p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <p>One of the following:</p>
            <ul>
              <li>A geolocation system on a personal device, such
              as a smart phone.</li>
              <li>A geolocation system to be attached to some other
              portable device.</li>
              <li>A geolocation system attached to a mobile
              vehicle.</li>
              <li>A geolocation system on a payload transported by
              a vehicle.</li>
              <li>A geolocation system on an indoor mobile
              robot.</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <ul>
              <li>Sensor ID</li>
              <li>Timestamp of last geolocation</li>
              <li>2D location</li>
              <ul>
                <li>typically latitude and longitude</li>
                <li>May also be semantic, i.e. room in a building,
                exit</li>
              </ul>
            </ul>
            <p>Optional:</p>
            <ul>
              <li>Semantic location</li>
              <ul>
                <li>Possibly in addition to numerical lat/long
                location.</li>
              </ul>
              <li>Altitude</li>
              <ul>
                <li>May also be semantic, i.e. floor of a
                building</li>
              </ul>
              <li>Heading</li>
              <li>Speed</li>
              <li>Accuracy information</li>
              <ul>
                <li>Confidence interval, e.g. distance that true
                location will be within some probability.</li>
                <li>Gaussian covariance matrix</li>
                <li>For each measurement</li>
                <li>For lat/long, may be a single value (see web
                browser API; radius?)</li>
              </ul>
              <li>Geolocation technology (GPS, SLAM, etc.).</li>
              <ul>
                <li>Note that multiple technologies might be used
                together.</li>
                <li>Include parameters such as sample interval,
                accuracy</li>
              </ul>
              <li>For each geolocation technology, data specific to
              that technology:</li>
              <ul>
                <li>GPS: NMEA type</li>
              </ul>
              <li>Historical data</li>
            </ul>
            <p>Note: the system should be capable of notifying
            consumers of changes in location. This may be used to
            implement geofencing by some other system. This may
            require additional parameters, such as the maximum
            distance that the device may be moved before a
            notification is sent, or the maximum amount of time
            between updates. Notifications may be sent by a variety
            of means, some of which may not be traditional push
            mechanisms (for example, email might be used). For
            geofencing applications, it is not necessary that the
            device be aware of the fence boundaries; these can be
            managed by a separate system.</p>
          </dd>
          <dt>Dependencies</dt>
          <dd>node-wot</dd>
          <dt>Description</dt>
          <dd>
            <p>Smart Cities have the need to observe the physical
            locations of large number of mobile devices in use in
            the context of a Fleet or Logistics Management System,
            or to place sensor data on a map in a Dashboard
            application. These systems may also include geofencing
            notifications and mapping (visual tracking)
            capabilities.</p>
          </dd>
          <dt>Variants</dt>
          <dd>
            <ul>
              <li>A version of the system may log historical data
              so the past</li>locations of the devices can be
              recovered.
              <li>Geolocation technologies other than GPS may be
              used. The payload</li>may contain additional
              information specific to the geolocation technology
              used. In particular, in indoor situations
              technologies such as WiFi triangulation or (V)SLAM
              may be more appropriate.
              <li>Geofencing may be implemented using event
              notifications and will require setting of additional
              parameters such as maximum distance.</li>
            </ul>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            <p>High-resolution timestamps can be used in
            conjunction with cache manipulation to access protected
            regions of memory, as with the SPECTRE exploit. Certain
            geolocation APIs and technologies can return
            high-resolution timestamps which can be a potential
            problem. Eventually these issues will be addressed in
            cache architecture but in the meantime a workaround is
            to artificially limit the resolution of timestamps.</p>
          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <p>Location is generally considered private information
            when it is used with a device that may be associated
            with a specific person, such as a phone or vehicle, as
            it can be used to track that person and infer their
            activities or who they associate with (if multiple
            people are being tracked at once). Therefore APIs to
            access geographic location in sensitive contexts are
            often restricted, and access is allowed only after
            confirming permission from the user.</p>
          </dd>
          <dt>Gaps</dt>
          <dd>
            <p>There is no single standardized semantic vocabulary
            for representing location data. Location data can be
            point data, a path, an area or a volumetric object.
            Location information can be expressed using multiple
            standards, but the reader of location data in a TD or
            in data returned by an IoT device must be able to
            unambiguously describe location information.</p>
            <p>There are both dynamic (data returned by a mobile
            sensor) and static (fixed installation location)
            applications for geolocation data. For dynamic location
            data, some recommended vocabulary to annotate data
            schemas would be useful. For static location data, a
            standard format for metadata to be included in a TD
            itself would be useful.</p>
          </dd>
          <dt>Existing Standards</dt>
          <dd>
            <ul>
              <li>NMEA: defines sentences from GPS devices</li>
              <li>
                <a href=
                "https://en.wikipedia.org/wiki/World_Geodetic_System">
                WGS84</a>:
              </li>
              <ul>
                <li>World Geodetic System</li>
                <li>Defines lat/long/alt coordinate system used by
                most other geolocation standards</li>
                <li>More complicated than you would think (need to
                deal with deviations of Earth from a true sphere,
                gravitational irregularities, position of centroid,
                etc. etc.)</li>
              </ul>
              <li>
                <a href="https://www.w3.org/2003/01/geo/">Basic Geo
                Vocabulary</a>:
              </li>
              <ul>
                <li>Very basic RDF definitions for lat, long, and
                alt</li>
                <li>Does not define heading or speed</li>
                <li>Does not define accuracy</li>
                <li>Does not define timestamps</li>
                <li>Uses string as a data model (rather than a
                number)</li>
              </ul>
              <li>
                <a href=
                "https://www.w3.org/TR/geolocation-API/"><abbr title="World Wide Web Consortium">
                W3C</abbr> Geolocalization API</a>:
              </li>
              <ul>
                <li><abbr title=
                "World Wide Web Consortium">W3C</abbr> Devices and
                Sensors WG is now handling</li>
                <li>There is an updated proposal: <a href=
                "https://w3c.github.io/geolocation-sensor/#geolocationsensor-interface">
                  https://w3c.github.io/geolocation-sensor/#geolocationsensor-interface</a>
                </li>
                <li>Data schema of updated proposal is similar to
                existing API, but all elements are now
                optional</li>
                <li>Data includes latitude, longitude, altitude,
                heading, and speed</li>
                <li>Accuracy is included for latitude/longitude
                (single number in meters, 95% confidence,
                interpretation a little ambiguous, but probably
                intended to be a radius) and altitude, but not for
                heading or speed.</li>
              </ul>
              <li>
                <a href="https://www.ogc.org/">Open Geospatial
                Consortium</a>:
              </li>
              <ul>
                <li>See <a href=
                "http://docs.opengeospatial.org/as/18-005r4/18-005r4.html">
                  http://docs.opengeospatial.org/as/18-005r4/18-005r4.html</a>
                </li>
                <li>Referring to locations by coordinates</li>
                <li>Has standards defining semantics for
                identifying locations</li>
                <li>Useful for mapping</li>
              </ul>
              <li>ISO</li>
              <ul>
                <li>
                  <a href=
                  "https://www.iso.org/standard/74039.html">ISO19111</a>:
                </li>
                <ul>
                  <li>Standard for referring to locations by
                  coordinates</li>
                  <li>Related to OGS standard above and WGS84</li>
                </ul>
                <li>Various other standards that relate to remote
                sensing, geolocation, etc.</li>
                <li>Here is an example (see references): <a href=
                "https://www.iso.org/obp/ui/fr/#iso:std:iso:ts:19159:-2:ed-1:v1:en">
                  https://www.iso.org/obp/ui/fr/#iso:std:iso:ts:19159:-2:ed-1:v1:en</a>
                </li>
              </ul>
              <li>
                <a href="https://www.w3.org/TR/vocab-ssn/">SSN</a>:
              </li>
              <ul>
                <li>Defines "accuracy": <a href=
                "https://www.w3.org/TR/vocab-ssn/#SSNSYSTEMAccuracy">
                  https://www.w3.org/TR/vocab-ssn/#SSNSYSTEMAccuracy</a>
                </li>
                <li>Definition of accuracy is consistent with how
                it is used in Web Geolocation API</li>
                <li>Also defines related terms Precision,
                Resolution, Latency, Drift, etc.</li>
              </ul>
              <li>Timestamps:</li>
              <ul>
                <li>
                  <abbr title=
                  "World Wide Web Consortium">W3C</abbr> standard
                  in proposed new web geolocation API: <a href=
                  "https://w3c.github.io/hr-time/#dom-domhighrestimestamp">
                  https://w3c.github.io/hr-time/#dom-domhighrestimestamp</a>
                </li>
                <li>See also related issues such as latency defined
                in SSN</li>
              </ul>
              <p>Note that accuracy and time are issues that apply
              to all kinds of sensors, not just geolocation.
              However, the specific geolocation technology of GPS
              is special since it is also a source of accurate
              time.</p>
            </ul>
          </dd>
          <dd></dd>
        </dl>
      </section>
      <section id="smartcity-dashboard">
        <h4 id="x2-2-2-dashboard"><bdi class="secno">2.2.2</bdi>
        Dashboard<a class="self-link" aria-label="§" href=
        "#smartcity-dashboard"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Target Users</dt>
          <dd>
            <p>A Smart City managing a large number of devices
            whose data needs to be visualized and understood in
            context.</p>
            <p>Stakeholders include:</p>
            <ul>
              <li>device owners: need to make data from devices
              available to dashboard system.</li>
              <li>device user: users of the dashboard system, such
              as members of city management, are indirectly "using"
              the devices by accessing their data, and in one
              variant, sending commands to actuators.</li>
              <li>cloud provider: the dashboard system itself or
              components of it (such as a database or data
              ingestion system) may be hosted in the cloud.</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>
            <p>In order to facilitate Smart City planning and
            decision-making, a Smart City dashboard interface makes
            it possible for city management to view and visualize
            all sensor data through the entire city in real time,
            with data identified as to geographic source
            location.</p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <p>Actuators can include robots; for these, commands
            might be given to robots to move to new locations, drop
            off or pick up sensor packages, etc. However, it could
            also include other kinds of actuators, such as flood
            gates, traffic signals, lights, signs, etc. For
            example, posting a public message on an electronic
            billboard might be one task possible through the
            dashboard.</p>
            <p>Sensors can include those for the environment and
            for people and traffic management (density counts,
            thermal cameras, car speeds, etc.). status of robots,
            other actuators, and sensors, data visualization, and
            (optionally) historical comparisons.</p>
            <p>Dashboard would include mapping functionality.
            Mapping implies a need for location data for every
            actuator and sensor, which could be acquired through
            geolocation sensors (e.g. GPS) or assigned statically
            during installation.</p>
            <p>This use case also includes images from cameras and
            real-time image and data streaming.</p>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <ul>
              <li>Environmental data for temperature, humidity, UV
              levels, pollution levels, etc.</li>
              <li>Infrastructure status (water flow, electrical
              grid, road integrity, etc.)</li>
              <li>Emergency sensing (flooding, earthquake, fire,
              etc.)</li>
              <li>Traffic (both people and vehicles)</li>
              <li>Health monitoring (e.g.fever tracking, mask
              detection, social distancing)</li>
              <li>Safety monitoring (e.g.wearing construction
              helmets on a construction site)</li>
              <li>Reports from non-IoT sources (for example, police
              reports of crimes, hospital emergency case
              reports)</li>
              <li>Images and data derived from images (people
              traffic and density can be derived from image
              analysis)</li>All data would need an associated
              geolocation and timestamp so it can be placed in time
              and space.
            </ul>
          </dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>
            <ul>
              <li>Thing description - support for data ingestion
              and normalization, geolocation and timestamp
              standards.</li>
              <li>Discovery - directories capable of tracking and
              managing a large number of devices on a large and
              possibly segmented network</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>
            <p>Data from a large number and wide variety of sensors
            needs to be integrated into a single database and
            normalized, then placed in time and space, and finally
            visualized.</p>
            <p>The user, a member of city management responsible
            for making planning decisions, sees data visualized on
            a map suitable for planning decisions.</p>
            <p>Variants:</p>
            <ul>
              <li>Historical data may also be available (allowing
              an analysis of trends over time).</li>
              <li>It may be possible to also issue commands to
              actuators through the interface.</li>
              <li>The system may be used for emergency response
              (for instance, closing floodgates in response to an
              expected tsunami)</li>
              <li>A subset of the data visualization capabilities
              may be made available to the public (for example,
              traffic)</li>
              <li>Filtering based on parameters such as location
              (area, state, county, country, zip code, etc.),
              sensor type, subject matter, etc.</li>
              <li>Ability to generate alerts off of various
              parameters</li>
              <li>Ability to produce logs off historical data</li>
            </ul>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            <ul>
              <li>Access to data should only be provided to
              authorized users, although some may be made available
              publicly</li>
              <li>Access to actuators should only be provided to
              authorized users, and commands should be recorded for
              auditing.</li>
            </ul>
          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <ul>
              <li>Management of privacy-sensitive information, for
              example images of people,</li>should be controlled
              and ideally not associated with specific individuals
              <li>Data that can be used to track movements of
              particular individuals should be controlled or
              eliminated.</li>
              <li>Data purge functions should be supported to allow
              the permanent deletion of private data.</li>
            </ul>
          </dd>
          <dt>Gaps</dt>
          <dd>
            <ul>
              <li>Geolocation data standards</li>
              <li>Timestamp data standards</li>
              <li>Scalable Discovery</li>
            </ul>
          </dd>
        </dl>
      </section>
      <section id="mmi-3-1_interactive-public-spaces">
        <h4 id="x2-2-3-interactive-public-spaces"><bdi class=
        "secno">2.2.3</bdi> Interactive Public Spaces<a class=
        "self-link" aria-label="§" href=
        "#mmi-3-1_interactive-public-spaces"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Motivation</dt>
          <dd>Public spaces provide many opportunities for
          engaging, social and fun interaction. At the same time,
          preserving privacy while sharing tasks and activities
          with other people is a major issue in ambient systems.
          These systems may also deliver personalized information
          in combination with more general services presented
          publicly. A trustful discovery of the services and
          devices available in such environments is a necessity to
          guarantee personalization and privacy in public-space
          applications.</dd>
          <dt>Expected Devices</dt>
          <dd>Public spaces supporting personalizable services and
          device access.</dd>
          <dt>Expected Data</dt>
          <dd>Command and status information transferred between
          the personal mobile device application and the public
          space's services and devices.<br>
          <br>
          Profile data for user preferences.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on
              mobile personal device and possibly in IoT
              orchestration services in the public space.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>Interactive installations such as touch-sensitive or
          gesture-tracking billboards may be set up in public
          places. Objects that present public information (e.g. a
          map of a shopping mall) can use a multimodal interface
          (built-in or in tandem with the user's mobile devices) to
          simplify user interaction and provide faster access.
          Other setups can stimulate social activities, allowing
          multiple people to enter an interaction simultaneously to
          work together towards a certain goal (for a prize) or
          just for fun (e.g. play a musical instrument or control a
          lighting exhibition). In a context where privacy is an
          issue (for example, with targeted/personalized alerts or
          advertisements), the user's mobile device acts as a
          mediator for the services running on the public network.
          This allows the user to receive relevant information in
          the way she sees fit. Notifications can serve as triggers
          for interaction with public devices and services if the
          user chooses to do so.</dd>
          <dt>Variants</dt>
          <dd>The user may have additional mobile devices they want
          to incorporate into an interaction, for example a headset
          acting as an auditory aid or personal speech output
          device.</dd>
          <dt>Gaps</dt>
          <dd>Data format describing user interface
          preferences.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 3.1.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case.</dd>
        </dl>
      </section>
      <section id="mmi-3-2_meeting-room-event-assistance">
        <h4 id="x2-2-4-meeting-room-event-assistance"><bdi class=
        "secno">2.2.4</bdi> Meeting Room Event Assistance<a class=
        "self-link" aria-label="§" href=
        "#mmi-3-2_meeting-room-event-assistance"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Expected Devices</dt>
          <dd>Meeting space supporting personalizable services and
          device access.</dd>
          <dt>Expected Data</dt>
          <dd>Command and status information transferred between
          the personal mobile device application and the meeting
          space's services and devices. Profile data for user
          preferences.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on
              mobile personal device and possibly in IoT
              orchestration services in the meeting space.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>A conference room where a series of meetings will
          take place. People can go in and out of the room before,
          after and during the meeting. The door is "touched" by a
          badge. An application on the user's mobile device can
          activate any available display in the room and the room
          and can access and receive notification from devices and
          services in the room. The chair of the meeting is
          notified by a dynamically composed graphic animation,
          audio notification or a mobile phone notification, about
          available devices and services, and can install
          applications indicated by links. The chair of the meeting
          selects a setup procedure by text amongst the provided
          links. These options could be, for example: photo
          step-by-step instructions (smartphone, HDTV display, Web
          site), audio instructions (MP3 audio guide, room speakers
          reproduction, HDTV audio) or RFID enhanced instructions
          (mobile SmartTag Reader, RFID Reader for smartphone). The
          chair of the meeting chooses the room speakers
          reproduction, then the guiding Service is activated and
          he starts to set the video projector. After some
          attendees arrive, the chair of the meeting changes to the
          slide show option and continues to follow the
          instructions at the same step it was paused but with
          another more private modality for example, a smartphone
          slideshow.</dd>
          <dt>Variants</dt>
          <dd>The user may have additional mobile devices they want
          to incorporate into an interaction, for example a headset
          acting as an auditory aid or personal speech output
          device.</dd>
          <dt>Gaps</dt>
          <dd>Data format describing user interface preferences.
          Ability to install applications based on links that can
          access IoT services.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 3.2.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case.</dd>
        </dl>
      </section>
      <section id="smart-campus">
        <h4 id="x2-2-5-cross-domain-discovery-in-a-smart-campus">
        <bdi class="secno">2.2.5</bdi> Cross-Domain Discovery in a
        Smart Campus<a class="self-link" aria-label="§" href=
        "#smart-campus"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Andrea Cimmino and Raúl García Castro</dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device owners</li>
              <li>service provider</li>
              <li>network operator (potentially transparent for WoT
              use cases)</li>
              <li>directory service operator</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>In this use case a network full of IoT devices is
          presented, in which these devices are registered in
          several Middle-Nodes. The challenge presented in this
          scenario is to be able to discover the different sensors,
          by issues a SPARQL query, and without having prior
          knowledge of where those devices are allocated.
          Therefore, the discovery SPARQL query must start from a
          specific Middle-Node and reach all those Middle-Nodes
          that are relevant to answer the query. This scenario
          requires that discovery does not only happen locally when
          a Middle-Node receives the query and checks if some Thing
          Description registered is suitable to answer the query.
          Instead, the scenario requires also that the Middle-Node
          forwards the query through the network (topology
          conformed by the middle-nodes) in order to find those
          Middle-Nodes that actually contain relevant Thing
          Descriptions. Notice from the following example that the
          query is not broadcasted in the network to prevent
          flooding, instead the Middle-Nodes follow some discovery
          heuristic to know where the query should be forwarded.
          Also, notice that in this scenario not all the
          Middle-Nodes have the IoT devices registered directly
          within, they are Middle-Nodes collectors, such as
          Middle-Node C, I, G, and D.</dd>
          <dt>Expected Devices</dt>
          <dd>Any device from the energy context (e.g. solar
          panels, smart plugs, or smart energy meters), devices
          from the building context (e.g. light bulbs, light
          switches, occupancy sensors, or thermostats), devices
          from the environmental context (e.g. soil moisture, flood
          detection, or air humidity), devices from the wearables
          context (e.g. smart bands), and/or devices from the water
          context (e.g. water valves, or water quality
          sensors)</dd>
          <dt>Expected Data</dt>
          <dd>Data coming from different contexts, such as Energy,
          Building, Environmental Wearables and Water.</dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>Current WoT-Discovery approach</dd>
          <dt>Description</dt>
          <dd>
            A campus has a wide range of IoT devices distributed
            across their grounds. These IoT devices belong to very
            different domains in a smart city, such as, energy,
            buildings, environment, water, wearable, etc. The IoT
            devices are distributed across the campus and belong to
            different infrastructures or even to individuals. A
            sample topology of this scenario could be the
            following:<br>
            <br>
            <img src="images/smart-campus-topology.svg" width=
            "100%" height="100%"><br>
            <br>
            In this scenario, energy-related IoT devices monitor
            the energy use and income in the campus, among other
            things. From these measurements, an Energy Management
            System may predict a negative peak of incoming energy
            that would entail the failure of the whole system. In
            this case, a Service or a User needs to discover all
            those IoT devices that are not critical for the normal
            functioning of the campus (such as indoor or outdoor
            illumination, HVAC systems, or water heaters) and
            interact with them in order to save energy, by
            switching them off or reducing their consumption.
            Besides, the same Service or User will look for those
            IoT devices that are critical for the well-functioning
            of the campus (such as magnetic locks, water
            distribution system, or fire/smoke sensors) and ensure
            that they are up and running. Additionally, the Service
            or the User, will discover relevant people's wearable
            to warn them about the situation.
            <p>Sample flow:</p>A service, or a user, sends a
            (SPARQL) query to the discovery endpoint of a known
            Middle-Node (which can be wrapped by a GUI). The
            Middle-Node will try to answer the query first checking
            the Thing Descriptions of the IoT devices registered in
            such Middle-Node. Then, if the query requires further
            discovery, or it was not successfully answered the
            Middle-Node will forward the query to its *known*
            Middle-Nodes. Recursively, the Middle-Nodes will try to
            answer the query and/or forward the query to their
            known Middle-Nodes. When one Middle-Node is able to
            answer the query it will forward back to the former
            Middle-Node the partial query answer. Finally, when the
            discovery task finishes, the former Middle-Node will
            join all the partial query answers producing an unified
            view (which could be synchronous or asynchronous).<br>
            <br>
            For instance, assuming Middle-Node F receives a query
            that asks about all the discoverable Building IoT
            devices in the campus. First, the Middle-Node F will
            try to answer the query with the Thing Descriptions of
            the IoT registered within. Since Middle-Node F contains
            some Building IoT devices a partial query answer is
            achieved. However, since they query asked about all the
            discoverable Building IoT devices Middle-Node F should
            forward the query to its other known Middle-Nodes,
            i.e., Middle-Node G. This process will be repeated by
            the Middle-Nodes until the query reaches the
            Middle-Nodes H and B which are the ones that have
            registered Thing Descriptions about IoT buildings.
            Therefore, the query will travel through the topology
            as follows:<br>
            <br>
            <img src="images/smart-campus-sample.svg" width="100%"
            height="100%"><br>
            <br>
            Finally, when Middle Nodes B and H compute two partial
            query answers, those answers will be forwarded back to
            Middle-Node F which will join them with its former
            partial query answer obtained from its registered Thing
            Descriptions. Finally, a global query answer will be
            provided.
          </dd>
          <dt>Security Considerations</dt>
          <dd>None, in this case an underneath infrastructure that
          handles security is assumed</dd>
          <dt>Privacy Considerations</dt>
          <dd>None, although relevant in this case the core of the
          use case relies on the feature of finding across the
          network different IoT devices. It is assumed that there
          is an underneath infrastructure that handles privacy</dd>
          <dt>Gaps</dt>
          <dd>Been able to find suitable Middle-Nodes that are
          relevant to answer the query, with no prior
          knowledge</dd>
          <dt>Existing Standards</dt>
          <dd>None</dd>
          <dt>Comments</dt>
          <dd>None</dd>
        </dl>
      </section>
    </section>
    <section id="smart-buildings">
      <h3 id="x2-3-building-technologies"><bdi class=
      "secno">2.3</bdi> Building Technologies<a class="self-link"
      aria-label="§" href="#smart-buildings"></a></h3>
      <section id="smart-building">
        <h4 id="x2-3-1-smart-building"><bdi class=
        "secno">2.3.1</bdi> Smart Building<a class="self-link"
        aria-label="§" href="#smart-building"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Sebastian Kaebisch</dd>
          <dt>Target Users</dt>
          <dd></dd>
          <dt>Motivation and Description</dt>
          <dd>Buildings such as office buildings, hotels, airports,
          hospitals, train stations and sports stadiums typically
          consist of heterogeneous IoT systems such as lightings,
          elevators, security (e.g. door control),
          air-conditionings, fire warnings, heatings, pools,
          parking control, etc. Monitoring, controlling, and
          management of such a heterogeneous IoT landscape is quite
          challenging in terms of engineering and maintenance.</dd>
          <dt>Expected Devices</dt>
          <dd>All kind of sensors and actuators (e.g. HVAC).</dd>
          <dt>Expected Users</dt>
          <dd>
            <ul>
              <li>systems engineers</li>
              <li>system administrators</li>
              <li>third party user</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>Heterogeneous data models from different IoT systems
          such as BACnet, KNX, and Modbus.</dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>WoT Thing Description and Thing Model, WoT
          Architecture, WoT Binding Templates (covering protocol
          specifica)</dd>
          <dt>Existing Standards</dt>
          <dd>BACnet, KNX, OPC-UA, Modbus</dd>
        </dl>
      </section>
      <section id="connected-building-energy-efficiency">
        <h4 id="x2-3-2-connected-building-energy-efficiency">
        <bdi class="secno">2.3.2</bdi> Connected Building Energy
        Efficiency<a class="self-link" aria-label="§" href=
        "#connected-building-energy-efficiency"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Farshid Tavakolizadeh</dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>directory service operator</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>Construction and renovation companies often deal with
          the challenge of delivering target energy-efficient
          buildings given specific budget and time constraints.
          Energy efficiency, as one of the key factors for
          renovation investments, depends on the availability of
          various data sources to support the renovation design and
          planning. These include climate data and building
          material along with residential comfort and energy
          consumption profiles. The profiles are created using a
          combination of manual inputs and sensory data collected
          from residents.</dd>
          <dt>Expected Devices</dt>
          <dd>
            <ul>
              <li>Gateway (e.g. Single-board computer with a Z-Wave
              controller)</li>
            </ul>Z-wave Sensors:
            <ul>
              <li>Power Meter</li>
              <li>Gas Meter</li>
              <li>Smart Plug</li>
              <li>Heavy Duty Switch</li>
              <li>Door/Window Sensors</li>
              <li>CO2 Sensor</li>
              <li>Thermostat</li>
              <li>Multi-sensors (Motion, Temperature, Light,
              Humidity, Vibration, UV)</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <ul>
              <li>Ambient conditions</li>
              <li>Occupancy model</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>Renovation of residential buildings to improve energy
          efficiency depend on a wide range of sensory information
          to understand the building conditions and consumption
          models. As part of the pre-renovation activities, the
          renovation companies deploy various sensors to collect
          relevant data over a period of time. Such sensors become
          part of a wireless sensor network (WSN) and expose data
          endpoint with the help of one or more gateway devices.
          Depending on the protocols, the endpoints require
          different interaction flows to securely access the
          current and historical measurements. The renovation
          applications need to discover the sensors, their
          endpoints and how to interact with them based on search
          criteria such as the physical location, mapping to the
          building model or measurement type.</dd>
          <dt>Privacy Considerations</dt>
          <dd>The TD may expose personal information about the
          building layout and residents.</dd>
          <dt>Gaps</dt>
          <dd>
            There is no standard vocabulary for embedding
            application-specific meta data inside the TD. It is
            possible to extend the TD context and add additional
            fields but with too much flexibility, every application
            may end up with a completely different structure,
            making such information more difficult to discover. In
            this use-case, the application specific data are:
            <ul>
              <li>the mapping between each thing and the space in
              the building model</li>
              <li>various identifiers for each thing (e.g. sensor
              serial number, z-wave ID, SenML name)</li>
              <li>indoor coordinates</li>
            </ul>There is no standard API specification for the WoT
            Thing Directory to maintain and query TDs.
          </dd>
          <dt>Existing Standards</dt>
          <dd>
            <ul>
              <li>
                <a href=
                "https://www.ogc.org/standards/sensorthings">OGC
                SensorThings</a> model includes a
                <code>properties</code> property for each Thing
                which is a non-normative JSON Object for
                application-specific information (not to be
                confused with TD's <code>properties</code> which is
                a Map of instances of PropertyAffordance
              </li>
            </ul>
          </dd>
        </dl>
      </section>
      <section id="smart-building-things">
        <h4 id="x2-3-3-automated-smart-building-management">
        <bdi class="secno">2.3.3</bdi> Automated Smart Building
        Management<a class="self-link" aria-label="§" href=
        "#smart-building-things"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Edison Chung, Hervé Pruvost, Georg Ferdinand
          Schneider</dd>
          <dt>Category</dt>
          <dd>Smart Building</dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>service provider</li>
              <li>device manufacturer</li>
              <li>gateway manufacturer</li>
              <li>identity provider</li>
              <li>directory service operator</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>
            <p>When operating smart buildings, aggregating and
            managing all data provided by heterogeneous devices in
            these buildings still require a lot of manual effort.
            Besides the hurdles of data acquisition that relies on
            multiple protocols, the acquired data generally lacks
            contextual information and metadata about its location
            and purpose. Usually, each service or application that
            consumes data from building things requires information
            about its content and its context like, e.g.:</p>
            <ul>
              <li>which thing produces the data (sensor, meter,
              actuator, other technical component...) in a
              building;</li>
              <li>which physical quantity or process is represented
              (temperature, energy supply, monitoring,
              actuation);</li>
              <li>which other building things are involved (e.g.
              sensor hosted by a duct or a space).</li>
            </ul>
            <p>Through the increased use of model-based data
            exchange over the whole life cycle of a building, often
            referred to as Building Information Modeling (BIM)
            (Sacks et al., 2018), a curated source for data
            describing the building itself is available including,
            amongst others, the topology of the building structured
            into e.g. sites, stores and spaces.</p>
            <p>Automatically tracking down data and their related
            things in a building would especially ease the
            configuration and operation of Building Automation and
            Control Systems (BACS) and Heating Ventilation and
            Air-Conditioning (HVAC) services during commissioning,
            operation, maintenance and retrofitting. To tackle
            these challenges, still, building experts make use of
            metadata and naming conventions which are manually
            implemented in Building Management Systems (BMS)
            databases to annotate data and things. An important
            property of a thing is its location within the topology
            of a building as well as where its related data are
            produced or used. For example, this applies to the
            temperature sensor of a space, the temperature setpoint
            of a zone, a mixing damper flap actuator of a HVAC
            component, etc. In addition, other attributes of things
            are of interest, such as cost or specific manufacturer
            data. One difficulty is especially the lack of a
            standardized way of creating, linking and sharing this
            information in an automated manner. On the contrary,
            manufacturers, service providers and users introduce
            their own metadata for their own purpose. As a
            solution, the Web of Things (WoT) Thing Description
            (TD) aims at providing normalized and syntactic
            interoperability between things.</p>
            <p>To support this effort, this use case is motivated
            by the need to enhance semantic interoperability
            between things in smart buildings and to provide them
            with contextual links to building information. This
            building information is usually obtained from a BIM
            model. The use case builds on Web of Data technologies
            and reuses schemas available from the Linked Building
            Data domain. It should serve as a use case template for
            many applications in an Internet of Building Things
            (IoBT).</p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <ul>
              <li>Actuators</li>
              <li>Sensors</li>
              <li>Devices from the building context</li>
              <li>Devices from the HVAC system</li>
              <li>Smart devices</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <ul>
              <li>Sensor ID</li>
              <li>Thing Descriptions</li>
              <li>Protocol integrations</li>
              <li>Sensor readings</li>
              <li>Building topology</li>
              <li>Semantic location</li>
              <li>Geolocation</li>
            </ul>
          </dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>
            <ul>
              <li>
                <a href=
                "https://www.w3.org/TR/wot-thing-description/">Web
                of Things Thing Description (WoT TD)</a>
              </li>
              <li>
                <a href=
                "https://w3c.github.io/wot-binding-templates/">Web
                of Things Binding Templates</a>
              </li>
              <li>
                <a href=
                "https://github.com/w3c/wot-discovery/blob/main/proposals/geolocation.md">
                WoT Discovery and Geolocation</a>
              </li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>
            <p>The goal of this use case is to show the potential
            to automate workflows and address the heterogeneity of
            data as observed in the smart building domain. The
            examples show the potential benefits of combining WoT
            TD with contextual data obtained from BIM.</p>
            <p>The use cases is based on the <a href=
            "https://github.com/TechnicalBuildingSystems/OpenSmartHomeData">
            Open Smart Home Dataset</a>, which introduces a BIM
            model for a residential flat combined with observations
            made by typical smart home sensors. We extend the
            dataset with Thing Descriptions of some of the items.
            The respective Thing Description of a temperature
            sensor in the kitchen of the considered flat is as
            follows:</p>
            <div class="example" id="TemperatureSensor">
              <div class="marker">
                <a class="self-link" href=
                "#TemperatureSensor">Example <bdi>1</bdi></a>
              </div>
              <pre class=""><code class="lang-json hljs" aria-busy=
              "false">{
    "id": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#TemperatureSensor",
    "@context": [
        "https://www.w3.org/2019/wot/td/v1",
        {
            "osh": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#",
            "bot": "https://w3id.org/bot#",
            "sosa": "http://www.w3.org/ns/sosa/",
            "om": "http://www.ontology-of-units-of-measure.org/resource/om-2/",
            "ssns": "http://www.w3.org/ns/ssn/systems/",
            "brick": "https://brickschema.org/schema/Brick#",
            "schema": "http://schema.org"
        }
    ],
    "title": "TemperatureSensor",
    "description": "Kitchen Temperature Sensor",
    "@type": ["sosa:Sensor", "brick:Zone_Air_Temperature_Sensor", "bot:element"],
    "@reverse": {
        "bot:containsElement": {
            "@id": "osh:Kitchen"
        }
    },
    "securityDefinitions": {
        "basic_sc": {
            "scheme": "basic",
            "in": "header"
        }
    },
    "security": [
        "basic_sc"
    ],
    "properties": {
        "Temperature": {
            "type": "number",
            "unit": "om:degreeCelsius",
            "forms": [
                {
                    "href": "https://kitchen.example.com/temp",
                    "contentType": "application/json",
                    "op": "readproperty"
                }
            ],
            "readOnly": true,
            "writeOnly": false
        }
    },
    "sosa:observes": {
        "@id": "osh:Temperature",
        "@type": "sosa:ObservableProperty"
    },
    "ssns:hasSystemCapability": {
        "@id": "osh:SensorCapability",
        "@type": "ssns:SystemCapability",
        "ssns:hasSystemProperty": {
            "@type": ["ssns:MeasurementRange"],
            "schema:minValue": 0.0,
            "schema:maxValue": 40.0,
            "schema:unitCode": "om:degreeCelsius"
        }
    }
}
</code></pre>
            </div>
            <p>Where the contextual information on the measurement
            range of the sensor is specified using the <a href=
            "http://www.w3.org/ns/ssn/systems/">SSNS</a> schema.
            The location information of the thing <a href=
            "#TemperatureSensor">TemperatureSensor</a> is provided
            based on the <a href="https://w3id.org/bot">Building
            Topology Ontology (BOT)</a>, a minimal ontology
            developed by the <a href=
            "https://www.w3.org/community/lbd/"><abbr title=
            "World Wide Web Consortium">W3C</abbr> Linked Building
            Data Community Group (<abbr title=
            "World Wide Web Consortium">W3C</abbr> LBD CG)</a> to
            describe the topology of buildings in the semantic web.
            Additionally, the thing description of the
            corresponding actuator is given below.</p>
            <div class="example" id="TemperatureActuator">
              <div class="marker">
                <a class="self-link" href=
                "#TemperatureActuator">Example <bdi>2</bdi></a>
              </div>
              <pre class="">    <code class="lang-json hljs"
              aria-busy="false">{
    "id": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#TemperatureActuator",
    "@context": [
        "https://www.w3.org/2019/wot/td/v1",
        {
            "osh": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#",
            "bot": "https://w3id.org/bot#",
            "sosa": "http://www.w3.org/ns/sosa/",
            "ssn": "http://www.w3.org/ns/ssn/",
            "brick": "https://brickschema.org/schema/Brick#"
        }
    ],
    "title": "TemperatureActuator",
    "description": "Kitchen Temperature Actuator",
    "@type": ["sosa:Actuator", "brick:Zone_Air_Temperature_Setpoint", "bot:element"],
    "@reverse": {
        "bot:containsElement": {
            "@id": "osh:Kitchen"
        }
    },
    "securityDefinitions": {
        "basic_sc": {
            "scheme": "basic",
            "in": "header"
        }
    },
    "security": [
        "basic_sc"
    ],
    "actions": {
        "TemperatureSetpoint": {
            "forms": [
                {
                    "href": "https://kitchen.example.com/tempS"
                }
            ]
        }
    },
    "ssn:forProperty": {
        "@id": "osh:Temperature",
        "@type": "sosa:ActuatableProperty"
    }
}
</code></pre>
            </div>
          </dd>
          <dt><em>Combining Topological Context and Thing
          Descriptions</em></dt>
          <dd>
            <p>The scenario considered is related to the
            replacement of a temperature sensor in a BACS. The
            topological information localizing the things, e.g. the
            <a href="#TemperatureSensor">temperature sensor</a> can
            be used to automatically commission the newly replaced
            sensor and link it to existing control algorithms. For
            this purpose, the identifiers of suitable sensors and
            actuators are needed and can be, for example, queried
            via <a href=
            "https://www.w3.org/TR/sparql11-query/">SPARQL</a>.
            Here the query uses some additional classification of
            sensors from <a href=
            "https://brickschema.org/ontology/1.1">BRICK
            schema</a>.</p>
            <div class="example" id="sparql">
              <div class="marker">
                <a class="self-link" href="#sparql">Example
                <bdi>3</bdi></a>
              </div>
              <pre class=""><code class="lang-sparql hljs"
              aria-busy=
              "false">PREFIX bot: &lt;https://w3id.org/bot&gt;
PREFIX brick: &lt;https://brickschema.org/schema/Brick#&gt;
PREFIX osh: &lt;https://w3id.org/ibp/osh/OpenSmartHomeDataSet#&gt;
SELECT ?sensor ?actuator
WHERE{
  ?space a bot:Space .
  ?space bot:containsElement ?sensor .
  ?space bot:containsElement ?actuator .
  ?sensor a brick:Zone_Air_Temperature_Sensor .
  ?actuator a brick:Zone_Air_Temperature_Setpoint .
}
</code></pre>
            </div>
            <p>Similarly this data can be obtained via a REST API
            built upon the <a href=
            "https://tools.ietf.org/html/rfc7231#section-4">HTTP</a>
            protocol. Below is an example endpoint applying
            <a href="https://roy.gbiv.com/pubs/dissertation/top.htm">
            REST</a> style for getting the same information for a
            specific space name:</p>
            <div class="example" id="RESTApiCall">
              <div class="marker">
                <a class="self-link" href="#RESTApiCall">Example
                <bdi>4</bdi></a>
              </div>
              <pre class=""><code class="lang-json hljs" aria-busy=
              "false">GET "https://server.example.com/api/locations?space=osh:Kitchen&amp;sensorType=brick:Zone_Air_Temperature_Sensor&amp;actuatorType=brick:Zone_Air_Temperature_Setpoint"
API response:
{
  "location": {
    "site": {
      "id": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#Site1",
      "name": "Site1"
    },
    "building": {
      "id": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#Building1",
      "name": "Building1"
    },
    "zone": null,
    "storey": {
      "id": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#Level2",
      "name": "Level2"
    },
    "space": {
      "id": "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#Kitchen",
      "name": "Kitchen"
    },
  "sensors": [
    "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#TemperatureSensor"
  ],
  "actuators": [
    "https://w3id.org/ibp/osh/OpenSmartHomeDataSet#TemperatureActuator"
  ]
}
</code></pre>
            </div>
            <p>In this example query, the REST endpoint has been
            defined using the <a href=
            "https://www.openapis.org/">OpenAPI specification</a>
            and is provided by a RESTful server. A data binding is
            needed between the server and the underlying backend
            storage, here the triple store that contains the
            involved ontologies (osh, bot, ssn, brick...). The data
            binding relies on similar SPARQL queries as the one
            shown above. As a result the endpoint can deliver
            information to a target application that consumes
            custom JSON rather than triples. Similar implmentation
            could be achieved using <a href=
            "https://www.w3.org/community/graphql-rdf/">GraphQL</a>.</p>
          </dd>
          <dt><em>Automated Update of Fault Detection Rule based on
          Thing Description</em></dt>
          <dd>
            <p>Another related use case in smart buildings, which
            would greatly benefit from harmonised thing
            descriptions and attached location information is
            related to the detection of unexpected behavior, errors
            and faults. An example for such a detection of faults
            is the rule-based surveillance of sensor values. A
            generic rule applicable to sensors is that the
            observation values stay within the measurement range of
            the sensor. Again, in the case of maintenance as
            described above a sensor is replaced.</p>
            <p>Some agent configuring fault detection rules can
            obtain the measurement range from the sensor's TD (see
            above) to obtain the parameters to configure the
            mentioned rule. Again, a query or API call retrieving
            this information (schema:minValue/ schema:maxValue) can
            be used to update the upper and lower bound of the
            values provided by the <a href=
            "#TemperatureSensor">sensor</a>.</p>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            <p>Security in smart buildings is of importance. In
            particular, access control needs to be properly
            secured. This applies also for data access which can be
            secured using existing security schemes (API Keys,
            OAuth2...). Moreover, from certain observations, e.g.
            electricity consumption, clues can be indirectly given
            such as presence in a home. Hence, security needs must
            be defined and properly addressed.</p>
          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <p>Privacy considerations can be of a concern if
            observations of sensors can be matched to individuals.
            It is of the responsibility of building owners,
            managers and users to define their own privacy policies
            for their data and share necessary consents if
            necessary.</p>
          </dd>
          <dt>Accessibility Considerations</dt>
          <dd>
            <p>Accessibility is a major concern in the buildings
            domain. Efforts exist in also providing accessibility
            data in a electronic format. The <abbr title=
            "World Wide Web Consortium">W3C</abbr> LBD CG is in
            contact with the <a href=
            "https://www.w3.org/community/lda/"><abbr title=
            "World Wide Web Consortium">W3C</abbr> Linked Data for
            Accessbility Community Group</a>.</p>
          </dd>
          <dt>Internationalization (i18n) Considerations</dt>
          <dd>
            <p>Internationalization is a concern as the Buildings
            industry is a global industry. This is reflected in
            some efforts, e.g. BOT used in the examples above does
            provide multilanguage labels in up to 16 different
            languages including english, french and chinese.</p>
          </dd>
          <dt>Existing Standards</dt>
          <dd>
            <ul>
              <li>
                <a href=
                "https://saref.etsi.org/saref4bldg/">SAREF4Bldg an
                ETSI Standard</a>
              </li>
              <li>
                <a href="https://www.w3.org/TR/vocab-ssn/">SOSA/SSN
                a <abbr title=
                "World Wide Web Consortium">W3C</abbr>
                Recommendation</a>
              </li>
              <li>
                <a href=
                "https://standards.buildingsmart.org/IFC/DEV/IFC4/ADD2/OWL/index.html">
                Industry Foundation Classes (IFC) an ISO
                standard</a>
              </li>
              <li>
                <a href="https://w3id.org/bot">Building Topology
                Ontology (BOT)</a>
              </li>
              <li>
                <a href="https://brickschema.org">BRICK</a>
              </li>
            </ul>
          </dd>
          <dd>
            <p>References:</p>
            <ul>
              <a href=
              "https://doi.org/10.1002/9781119287568">Sacks, R.,
              Eastman, C., Lee, G., & Teicholz, P. (2018). BIM
              handbook: A guide to building information modeling
              for owners, designers, engineers, contractors, and
              facility managers. John Wiley & Sons.</a>
            </ul>
          </dd>
        </dl>
      </section>
    </section>
    <section id="manufacturing">
      <h3 id="x2-4-manufacturing"><bdi class="secno">2.4</bdi>
      Manufacturing<a class="self-link" aria-label="§" href=
      "#manufacturing"></a></h3>
      <section id="big-data">
        <h4 id="x2-4-1-manufacturing"><bdi class=
        "secno">2.4.1</bdi> Manufacturing<a class="self-link"
        aria-label="§" href="#big-data"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael Lagally</dd>
          <dt>Target Users</dt>
          <dd>Device owners, cloud provider.</dd>
          <dt>Motivation</dt>
          <dd>
            <p>Production lines for industrial manufacturing
            consist of multiple machines, where each machine
            incorporates sensors for various values. A failure of a
            single machine can cause defective products or a stop
            of the entire production.</p>
            <p>Big data analysis enables to identify behavioral
            patterns across multiple production lines of the entire
            production plant and across multiple plants.</p>
            <p>The results of this analysis can be used for
            optimizing consumption of raw materials, checking the
            status of production lines and plants and predicting
            and preventing fault conditions.</p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>Various sensors, e.g. temperature, light, humidity,
          vibration, noise, air quality.</dd>
          <dt>Expected Data</dt>
          <dd>Discrete sensor values, such as temperature, light,
          humidity, vibration, noise, air quality readings. The
          data can be delivered periodically or on demand.</dd>
          <dt>Dependencies</dt>
          <dd>Thing Description: groups of devices, aggregation /
          composition mechanism, thing models Discovery/Onboarding:
          Onboarding of groups of devices</dd>
          <dt>Description</dt>
          <dd>
            <p>A company owns multiple factories which contain
            multiple production lines. Examples are production
            lines and environment sensors. These devices collect
            data from multiple sensors and transmit this
            information to the cloud. Sensor data is stored in the
            cloud, can be visualized and analyzed using machine
            learning / AI.</p>
            <p>The cloud service allows to manage single and groups
            of devices. Combining the data streams from multiple
            devices allows to get an easy overview of the state of
            all connected devices in the user's realm.</p>
            <p>In many cases there are groups of devices of the
            same kind, so the aggregation of data across devices
            can serve to identify anomalies or to predict impending
            outages.</p>
            <p>The cloud service allows to manage single and groups
            of devices and can help to identify abnormal
            conditions. For this purpose a set of rules can be
            defined by the user, which raises alerts towards the
            user or triggers actions on devices based on these
            rules.</p>
            <p>This enables the early detection of pending problems
            and reduces the risk of machine outages, quality
            problems or threats to the environment or life of
            humans. It increases production efficiency, improves
            production logistics (such as raw material delivery and
            production output).</p>
          </dd>
          <dt>Comments</dt>
          <dd>See also Digital Twin use case.</dd>
        </dl>
      </section>
    </section>
    <section id="retail">
      <h3 id="x2-5-retail"><bdi class="secno">2.5</bdi>
      Retail<a class="self-link" aria-label="§" href=
      "#retail"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>David Ezell, Michael Lagally, Michael McCool</dd>
        <dt>Target Users</dt>
        <dd>Retailers, customers, suppliers.</dd>
        <dt>Motivation</dt>
        <dd>
          <p>Integrating and interconnecting multiple devices into
          the common retail workflow (i.e., transaction log)
          drastically improves retail business operations at
          multiple levels. It brings operational
          visibility,including consumer behavior and environmental
          information, that was not previously possible or viable
          in a meaningful way.</p>
          <p>It drastically speeds up the process of root cause
          analysis of operational issues and simplifies the work of
          retailers.</p>
        </dd>
        <dt>Expected Devices</dt>
        <dd>Connected sensors, such as people counters, presence
        sensors, air quality, room occupancy, door sensors. Cloud
        services. Video analytics edge services.</dd>
        <dt>Expected Data</dt>
        <dd>Inventory data, supply chain status information,
        discrete sensor data or data streams.</dd>
        <dt>Description</dt>
        <dd>Falling costs of sensors, communications, and handling
        of very large volumes of data combined with cloud computing
        enable retail business operations with increased
        operational efficiency, better customer service, and even
        increased revenue growth and return on investment. Accurate
        forecasts allow retailers to coordinate demand-driven
        outcomes that deliver connected customer interactions. They
        drive optimal strategies in planning, increasing inventory
        productivity in retail supply chains, decreasing
        operational costs and driving customer satisfaction from
        engagement, to sale, to fulfilment. Understanding of store
        activity juxtaposed with traditional information streams
        can boost worker and consumer safety, comply better with
        work safety regulations, enhance system and site security,
        and improve worker efficiency by providing real-time
        visibility into worker status, location, and work
        environment.</dd>
        <dt>Variants</dt>
        <dd>
          <ul>
            <li>Use edge computing, in particular video analytics,
            in combination with IoT devices to deliver an enhanced
            customer experience, better manage inventory, or
            otherwise improve the store workflow.</li>
          </ul>
        </dd>
        <dt>Security Considerations</dt>
        <dd>
          <ul>
            <li>In retail, replay attacks can cause monetary loss,
            customers may be incorrectly charged or
            over-charged.</li>
            <li>To avoid replay attacks, "Things" should implement
            a sequence number for each message and digital
            signature.</li>
            <li>Servers ("Things" or "Cloud") should verify the
            signature and disallow for duplicated messages.</li>
            <li>For "Things" relying on electronic payments,
            "Things" must comply with PCI-DSS requirements.</li>
            <li>"Things" must never store credit card
            information.</li>
            <li>Customer satisfaction and trust depends on
            availability, so attacks such as Denial-of-Service
            (DoS) need to be prevented or mitigated.</li>
            <li>To prevent DoS, implement "Things" with early DoS
            detection.</li>
            <li>Have an automated DoS system that will notify the
            controlling unit of the problem.</li>
            <li>Implement IP white list, that could be part of the
            DoS early detection system.</li>
            <li>Make sure your network perimeter is defended with
            up to date firewall software.</li>
          </ul>
        </dd>
        <dt>Privacy Considerations</dt>
        <dd>As a general rule, personal consumer information should
        not be stored. That is especially true in the retail
        industry where a security breach could cause financial,
        reputation, and brand damage. If personal or information
        that can identify a consumer is to be stored, it should be
        to conduct business and with the explicit acknowledgment of
        the consumer. WoT vendors and integrators should always
        have a privacy policy and make it easily available. By
        default, devices should adopt an opt-out policy. That
        means, unless the consumer explicitly allowed for the data
        capture and storage, avoid doing it.</dd>
      </dl>
    </section>
    <section id="health">
      <h3 id="x2-6-health"><bdi class="secno">2.6</bdi>
      Health<a class="self-link" aria-label="§" href=
      "#health"></a></h3>
      <section id="public-health">
        <h4 id="x2-6-1-public-health"><bdi class=
        "secno">2.6.1</bdi> Public Health<a class="self-link"
        aria-label="§" href="#public-health"></a></h4>
        <section id="smartcity-health-monitoring">
          <h5 id="x2-6-1-1-public-health-monitoring"><bdi class=
          "secno">2.6.1.1</bdi> Public Health Monitoring<a class=
          "self-link" aria-label="§" href=
          "#smartcity-health-monitoring"></a></h5>
          <dl>
            <dt>Submitter(s)</dt>
            <dd>Jennifer Lin</dd>
            <dt>Target Users</dt>
            <dd>Agencies, companies and other organizations in a
            Smart City with significant pedestrian traffic in a
            pandemic situation.</dd>
            <dt>Motivation</dt>
            <dd>A system to monitor the health of people in public
            places is useful to control the spread of infectious
            diseases. In particular, we would like to identify
            individuals with temperatures outside the norm (i.e.
            running a fever) and then take appropriate action.
            Actions can include sending a notification or actuating
            a security device, such as a gate. This mechanism
            should be non-invasive and non-contact since the
            solution should not itself contribute to the spread of
            infectious diseases. Data may also be aggregated for
            statistics purposes, for example, to identify the
            number of people in an area with elevated temperatures.
            This has additional requirements to avoid
            double-counting individuals.</dd>
            <dt>Expected Devices</dt>
            <dd>
              One of the following:
              <ul>
                <li>A thermal camera.</li>
                <li>Face detection (AI) service</li>
                <ul>
                  <li>May be on device or be an edge or cloud
                  service.</li>
                </ul>
              </ul>Optional:
              <ul>
                <li>RGB and/or depth camera registered with the
                thermal camera</li>
                <li>Cloud service for data aggregation and
                analytics.</li>
                <li>Some way to identify location (optional) Note
                that location might be static and configured during
                installation, but might also be based on a
                localization technology if the device needs to be
                portable (for example, if it needs to be set up
                quickly for an event).</li>
              </ul>
            </dd>
            <dt>Expected Data</dt>
            <dd>
              <ul>
                <li>Sensor ID</li>
                <li>Timestamp</li>
                <li>Number of people identified with a fever in
                image</li>
                <li>Estimated temperature for each person</li>
                <ul>
                  <li>May be coarse, low/normal/high</li>
                </ul>
                <li>Location</li>
                <ul>
                  <li>Latitude, Longitude, Altitude, Accuracy</li>
                  <li>Semantic (e.g. a particular building
                  entrance)</li>
                </ul>
                <li>Thermal image</li>
              </ul>Optional:
              <ul>
                <li>RGB image</li>
                <li>Depth image</li>
                <li>Localization technology (see localization use
                case)</li>
                <li>Integration with local IoT devices: gates,
                lights, or people (guards)</li>
                <li>Bounding boxes around faces of identified
                people in image(s)</li>
                <li>Data that can be used to uniquely identify a
                face (distinguish it from others)</li>
                <ul>
                  <li>Aggregation system may output the total
                  number of unique faces with fever</li>
                </ul>
              </ul>Note 1: the system should be capable of
              notifying consumers (such as security personnel), of
              fever detections. This may be email, SMS, or some
              other mechanism, such as MQTT publication.<br>
              <br>
              Note 2: In all cases where images are captured,
              privacy considerations apply.<br>
              <br>
              It would also be useful to count unique individuals
              for statistics purposes, but not necessarily based on
              identifying particular people. This is to avoid
              counting the same person multiple times.
            </dd>
            <dt>Dependencies</dt>
            <dd>node-wot</dd>
            <dt>Description</dt>
            <dd>A thermal camera image is taken of a group of
            people and an AI service is used to identify faces in
            the image. The temperature of each person is then
            estimated from the registered face; for greater
            accuracy, a consistent location for sampling should be
            used, such as the forehead. The estimated temperature
            is compared to high (and optionally, low) thresholds
            and a notification (or other action) is taken if the
            temperature is outside the norm. Additional features
            may be extracted to identify unique individuals.</dd>
            <dt>Variants</dt>
            <dd>
              <ul>
                <li>Enough information is included in the
                notification that the specific person that raised
                the alarm can be identified. For example, if an RGB
                camera is also registered with the thermal camera,
                then a bounding box may be indicated via JSON and
                the RGB image included; or the bounding box could
                be actually drawn into the sent image, or the face
                could be cropped out. This is useful if, for
                example, a notification needs to be sent to health
                or security workers who need to identify the person
                in a crowd.</li>
                <li>Instead of simply a notification, an action may
                be taken, such as closing or refusing to open a
                gate at the entrance to a building, to prevent sick
                employees from entering the building.</li>
                <li>To generate statistics, for example to count
                the number of people with fevers, then unique
                individuals need to be identified to avoid counting
                the same person more than once.</li>
                <li>The same sensors might be used to determine the
                number of people in an area and send a notification
                if crowded conditions are detected, in order to
                support social distancing behavior (for instance,
                supporting an app that notifies users when a
                destination is crowded) in a pandemic
                situation.</li>
                <li>Cameras that provide video streams rather than
                still images.</li>
              </ul>
            </dd>
            <dt>Security Considerations</dt>
            <dd>
              <ul>
                <li>Because PII is involved (see below) access
                should be controlled (only provided to authorized
                users) and communications protected
                (encrypted).</li>
              </ul>
            </dd>
            <dt>Privacy Considerations</dt>
            <dd>
              <ul>
                <li>Images of people and their health status is
                involved.</li>
                <ul>
                  <li>If later these are made public then the
                  health information of a particular person would
                  be released publicly.</li>
                  <li>There is also the possibility that the camera
                  data could be in error, and should be confirmed
                  with a more accurate sensor.</li>
                  <li>This information needs to be treated as PII
                  and protected: only distributed to authorized
                  users, and deleted when no longer needed.</li>
                  <li>However, derived aggregate information can be
                  kept and published.</li>
                </ul>
              </ul>
            </dd>
            <dt>Gaps</dt>
            <dd>
              <ul>
                <li>Onboarding mechanism for rapidly deploying a
                large number of devices</li>
                <li>Standard vocabulary for geolocation
                information</li>
                <li>Implementations able to handle image payload
                formats, possibly in combination with non-image
                data (e.g. images and JSON in a single
                response)</li>
                <li>Video streaming support (if we wish to serve
                video stream from the camera instead of still
                images)</li>
                <li>Standard ways to specify notification
                mechanisms and data payloads for things like SMS
                and email (in addition to the expected MQTT, CoAP,
                and HTTP event mechanisms)</li>
              </ul>
            </dd>
            <dt>Comments</dt>
            <dd>
              <ul>
                <li>May be additional requirements for privacy
                since images of people and their health status is
                involved.</li>
                <li>Different sub-use cases: immediate alerts or
                actions vs. aggregate data gathering</li>
              </ul>
            </dd>
          </dl>
        </section>
        <section id="MedicalDevices">
          <h5 id=
          "x2-6-1-2-interconnected-medical-devices-in-a-hospital-icu">
          <bdi class="secno">2.6.1.2</bdi> Interconnected medical
          devices in a hospital ICU<a class="self-link" aria-label=
          "§" href="#MedicalDevices"></a></h5>
          <dl>
            <dt>Submitter(s)</dt>
            <dd>Taki Kamiya</dd>
            <dt>Target Users</dt>
            <dd>
              <ul>
                <li>device owners</li>
                <li>device user</li>
                <li>cloud provider</li>
                <li>service provider</li>
                <li>device manufacturer</li>
                <li>gateway manufacturer</li>
                <li>identity provider</li>
              </ul>
            </dd>
            <dt>Motivation</dt>
            <dd>Preventable medical errors may account for more
            than 100,000 deaths per year in U.S. alone. These
            errors are mainly caused by failures of communication
            such as a chart misread or the wrong data passed along
            to machines or staffs. Part of the problem could be
            solved if the machines could speak to one another.
            Manufacturers have little incentive to make their
            proprietary code and data easily to accessible and
            process able by their competitors’ machines. So the
            task of middleman falls to the hospital staffs. In
            addition to saving lives, a common framework could
            result in collecting and recording more clinical data
            on patients, making it easier to deliver precision
            medicine.</dd>
            <dt>Description</dt>
            <dd>
              <p>Physiological Closed-Loop Control (PCLC) devices
              are a group of emerging technologies, which use
              feedback from physiological sensor(s) to autonomously
              manipulate physiological variable(s) through delivery
              of therapies conventionally delivered by
              clinician(s).</p>
              <p>Clinical scenario without PCLC. An elderly female
              with end-stage renal failure was given a standard
              insulin infusion protocol to manage her blood
              glucose, but no glucose was provided. Her blood
              glucose dropped to 33, then rebounded to over 200
              after glucose was given. This scenario has not
              changed for decades.</p>
              <p>The desired state with PCLC implemented in an ICU.
              A patient is receiving an IV insulin infusion and is
              having the blood glucose continuously monitored. The
              infusion pump rate is automatically adjusted
              according to the real-time blood glucose levels being
              measured, to maintain blood glucose values in a
              target range. If the patient’s glucose level does not
              respond appropriately to the changes in insulin
              administration, the clinical staff is alerted.</p>
              <p>Medical devices do not interact with each other
              autonomously (monitors, ventilator, IV pumps, etc.)
              Contextually rich data is difficult to acquire.
              Technologies and standards to reduce medical errors
              and improve efficiency have not been implemented in
              theater or at home.</p>
              <p>In recent years, researchers have made progress
              developing PCLC devices for mechanical ventilation,
              anesthetic delivery applications, and so on. Despite
              these promises and potential benefits, there has been
              limited success in the translation of PCLC devices
              from <a href=
              "https://today.duke.edu/2014/07/benchbedside">bench
              to bedside</a>. A key challenge to bringing PCLC
              devices to a level required for a clinical trials in
              humans is risk management to ensure device
              reliability and safety.</p>
              <p>The United States Food and Drug Administration
              (FDA) classifies new hazards that might be introduced
              by PCLC devices into three categories. Besides
              clinical factors (e.g. sensor validity and
              reliability, inter- and intra-patient physiological
              variability) and usability/human factors (e.g. loss
              of situational awareness, errors, and lapses in
              operation), there are also engineering challenges
              including robustness, availability, and integration
              issues.</p>
            </dd>
            <dt>Variants</dt>
            <dd>
              US military developed ONR SBIR (Automated Critical
              Care System Prototype), and found those issues.
              <ul>
                <li>No plug and play, i.e. cannot swap O2 Sat with
                another manufacturer.</li>
                <li>No standardization of data outputs for devices
                to interoperate.</li>
                <li>Must have the exact make/model to replace a
                faulty device or system will not work.</li>
              </ul>
            </dd>
            <dt>Security Considerations</dt>
            <dd>
              <p>Security considerations for interconnected and
              dynamically composable medical systems are critical
              not only because laws such as <a href=
              "https://www.hhs.gov/hipaa/index.html">HIPAA</a>
              mandate it, but also because security attacks can
              have serious safety consequences for patients. The
              systems need to support automatic verification that
              the system components are being used as intended in
              the clinical context, that the components are
              authentic and authorized for use in that environment,
              that they have been approved by the hospital’s
              biomedical engineering staff and that they meet
              regulatory safety and effectiveness requirements.</p>
              <p>For security and safety reasons, <a href=
              "https://www.astm.org/Standards/F2761.htm">ICE
              F2761-09(2013)</a> compliant medical devices never
              interact directly each other. All interaction is
              coordinated and controlled via the applications.</p>
              <p>While transport-level security such as TLS
              provides reasonable protection against external
              attackers, they do not provide mechanisms for
              granular access control for data streams happening
              within the same protected link. Transport-level
              security is also not sufficiently flexible to balance
              between security and performance. Another issue with
              widely used transport-level security solutions is the
              lack of support for multicast.</p>
            </dd>
            <dt>Gaps</dt>
            <dd>Multicast support. It has proven useful for
            efficient and scalable discovery and information
            exchange in industrial systems.</dd>
            <dt>Existing Standards</dt>
            <dd>
              <p><a href=
              "https://www.astm.org/Standards/F2761.htm">F2761-09
              (2013)</a></p>Medical Devices and Medical Systems -
              Essential safety requirements for equipment
              comprising the patient-centric integrated clinical
              environment (ICE) - Part 1: General requirements and
              conceptual model. The idea behind ICE is to allow
              medical devices that conform to the ICE standard,
              either natively or using an adapter, to interoperate
              with other ICE-compliant devices regardless of
              manufacturer.
              <p><a href=
              "https://www.openice.info/">OpenICE</a></p>OpenICE is
              an initiative to create a community implementation of
              F2761-09 (ICE - Integrated Clinical Environment)
              based on <a href=
              "https://www.omg.org/spec/DDS/About-DDS/">DDS</a>.
              <p><a href=
              "https://secwww.jhuapl.edu/mdira/documents">MDIRA
              Specification Document Version 1.0</a>.</p>MDIRA
              Version 1.0 provides requirements and implementation
              guidance for MDIRA-compliant systems focused on
              trauma and critical care in austere environments.
              Johns Hopkins University Applied Physics Laboratory
              (JHU-APL) lead a research project in collaboration
              with US military to develop a framework of autonomous
              / closed loop prototypes for military health care
              which are dual use for the civilian healthcare
              system.
            </dd>
          </dl>
        </section>
      </section>
      <section id="private-health">
        <h4 id="x2-6-2-private-health"><bdi class=
        "secno">2.6.2</bdi> Private Health<a class="self-link"
        aria-label="§" href="#private-health"></a></h4>
        <section id="mmi-4-1_health-notifiers">
          <h5 id="x2-6-2-1-health-notifiers"><bdi class=
          "secno">2.6.2.1</bdi> Health Notifiers<a class=
          "self-link" aria-label="§" href=
          "#mmi-4-1_health-notifiers"></a></h5>
          <dl>
            <dt>Submitter(s)</dt>
            <dd>Michael McCool</dd>
            <dt>Target Users</dt>
            <dd>End user with a health problem they wish to
            monitor. Health services provider (doctor, nurse,
            paramedic, etc.).</dd>
            <dt>Motivation</dt>
            <dd>In critical situations regarding health, like a
            medical emergency, media multimodality may be the most
            effective way to communicate alerts, When the goal is
            to monitor the health evolution of a person in both
            emergency and non-emergency contexts, access via
            networked devices may be the most effective way to
            collect data and monitor a patient's status.</dd>
            <dt>Expected Devices</dt>
            <dd>Medical facilities supporting device and service
            access.</dd>
            <dt>Expected Data</dt>
            <dd>Command and status information transferred between
            the personal mobile device application and the meeting
            space's services and devices. Profile data for user
            preferences.</dd>
            <dt>Dependencies</dt>
            <dd>
              <ul>
                <li>WoT Thing Description</li>
                <li>WoT Discovery</li>
                <li>Optional: WoT Scripting API in application on
                mobile personal device and possibly in IoT
                orchestration services.</li>
              </ul>
            </dd>
            <dt>Description</dt>
            <dd>In medical facilities, a system may provide
            multiple options to control sensor operations by voice
            or gesture ("start reading my blood pressure now").
            These interactions may be mediated by an application
            installed into a smartphone. The system integrates
            information from multiple sensors (for example, blood
            pressure and heart rate); reports medical sensor
            readings periodically (for example, to a remote medical
            facility) and sends alerts when unusual readings/events
            are detected.</dd>
            <dt>Variants</dt>
            <dd>The user may have additional mobile devices they
            want to incorporate into an interaction, for example a
            headset acting as an auditory aid or personal speech
            output device.</dd>
            <dt>Gaps</dt>
            <dd>Data format describing user interface preferences.
            Ability to install applications based on links that can
            access IoT services.</dd>
            <dt>Existing Standards</dt>
            <dd>This use case is based on MMI UC 3.2.</dd>
            <dt>Comments</dt>
            <dd>Does not include Requirements section from original
            MMI use case.</dd>
          </dl>
        </section>
      </section>
    </section>
    <section id="energy">
      <h3 id="x2-7-energy"><bdi class="secno">2.7</bdi>
      Energy<a class="self-link" aria-label="§" href=
      "#energy"></a></h3>
      <section id="smart-grid">
        <h4 id="x2-7-1-smart-grids"><bdi class="secno">2.7.1</bdi>
        Smart Grids<a class="self-link" aria-label="§" href=
        "#smart-grid"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Christian Glomb</dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>Grid operators on all voltage levels line
              Distribution System Operators (DSO), Transmission
              System Operators (TSO)</li>
              <li>Plant operators (centralized as well as
              de-centralized producers)</li>
              <li>Virtual Power Plant (VPP) operators</li>
              <li>Energy grid markets</li>
              <li>Cloud providers where grid backend services are
              hosted and where Operation Technology bridges to
              Information Technology</li>
              <li>Device manufacturers, owners, and users; devices
              include communication gateways, monitoring and
              control units</li>
            </ul>
          </dd>
          <dt>Expected Devices</dt>
          <dd>A smart grid integrates all players in the
          electricity market into one overall system through the
          interaction of generation, storage, grid management and
          consumption. Power and storage plants are already
          controlled today in such a way that only as much
          electricity is produced as is needed. Smart grids include
          consumers as well as small, decentralized energy
          suppliers and storage locations in this control system,
          so that on the one hand, consumption is more homogeneous
          in terms of time and space (see also intelligent
          electricity consumption) and on the other hand, in
          principle inhomogeneous producers (e.g. wind power) and
          consumers (e.g. lighting) can be better integrated.</dd>
          <dt>Expected Data</dt>
          <dd>
            <ul>
              <li>Weather and climate data</li>
              <li>Metering data (both production as well as
              consumption as well as storage, e.g. 15 min.
              intervals)</li>
              <li>Real time data from PMUs (Phasor Measurement
              Units)</li>
              <li>Machine and equipment monitoring data (enabling
              health checks)</li>
              <li>...</li>
            </ul>
          </dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>WoT Architecture, WoT Binding Templates (covering
          protocol specifica)</dd>
          <dt>Description</dt>
          <dd>The term Smart Grid refers to the communicative
          networking and control of power generators, storage
          facilities, electrical consumers, and grid equipment in
          power transmission and distribution networks for
          electricity supply. This enables the optimization and
          monitoring of the interconnected components. The aim is
          to secure the energy supply on the basis of efficient and
          reliable system operation.</dd>
          <dt>Variants</dt>
          <dd></dd>
          <dt>Decentralized Power Generation</dt>
          <dt>While electricity grids with centralized power
          generation have dominated up to now, the trend is moving
          towards decentralized generation plants, both for
          generation from fossil primary energy through small CHP
          plants and for generation from renewable sources such as
          photovoltaic systems, solar thermal power plants, wind
          turbines and biogas plants. This leads to a much more
          complex structure, primarily in the area of load control,
          voltage maintenance in the distribution grid and
          maintenance of grid stability. In contrast to medium to
          large power plants, smaller, decentralized generation
          plants also feed directly into the lower voltage levels
          such as the low-voltage grid or the medium-voltage grid.
          This use case variants also includes operation and
          control of energy storages like batteries.</dt>
          <dt>Virtual Power Plants</dt>
          <dt>A Virtual Power Plant (VPP) is an aggregation of
          Distributed Energy Resources (DERs) that can act as an
          entity on energy markets or as an ancillary service to
          grid operation. The individual DERs often have a primary
          use on their own, with electric generation/consumption
          being a side-effect resp. secondary use. This results in
          negotiations/collaborations between many different
          parties e.g. such as the DER owner, the VPP operator, the
          grid operator and others.</dt>
          <dt>Smart Metering</dt>
          <dt>For consumers, a major change is the installation of
          smart meters. Their core tasks are remote reading and the
          possibility to realize fluctuating prices within a day at
          short notice. All electricity meters must therefore be
          replaced by those with remote data transmission.</dt>
          <dt>Other variants</dt>
          <dt>Emergency response, grid synchronization, grid black
          start</dt>
          <dt>Building Blocks</dt>
          <dd>
            <ul>
              <li>Multi-Stakeholder Operation: Multiple involved
              parties have to find a common mode of operation</li>
              <li>Device Lifecycle Management: Since the VPP is a
              dynamic system of loosely coupled DERs, the
              appearance and disappearance of DERs as well as the
              software management on the devices itself requires a
              means to orchestrate the lifecycle of individual
              device's respective components.</li>
              <li>Embedded Runtime: Especially for DERs in remote
              locations, maintaining a close couple control loop
              can be expensive if feasible at all. Therefore, it is
              desirable to be able to offload control logic to the
              DER itself.</li>
              <li>Ensemble Discovery: In order to dynamically find
              matching DERs needed for the operational goal of a
              VPP, a registry with different options of DER
              discovery is needed.</li>
              <li>Content-Negotiation: The different stakeholders
              have to interact and therefore need a common data
              format.</li>
              <li>Resource Description: The DER has to describe
              itself to enable discovery of single DERs and
              ensembles, also the operational data needs to be
              understood by the different stakeholders without
              engineering effort.</li>
              <li>Push Services: As there is a fan-out with many
              devices that probably have a rate-limited connection
              connecting to one single command center, a
              bidirectional communication mechanism is needed
              rather than polling for the reverse direction</li>
              <li>Object Memory: As multiple and interchangeable
              stakeholders are involved in the application, a
              backlog of the object is beneficial for
              scrollkeeping</li>
            </ul>
          </dd>
          <dt>Non-Functionals</dt>
          <dd>
            <ul>
              <li>Privacy: As fine-grained metering information
              provides sensitive data about a household, the system
              should show a high degree of privacy</li>
              <li>Trust: Since the data exchange between the
              virtual power plant and the distributed energy
              resource leads to a physical action that invokes high
              currents and monetary flows, the integrity of both
              parties and the exchange's data is crucial</li>
              <li>Layered L7 Communication: Since multiple
              different links are used for monitoring and control,
              integration requires a clear and consistent
              separation of information from the used serialization
              and application protocols to enable the exchange of
              homogenous information over heterogenous application
              layer protocols</li>
            </ul>
          </dd>
          <dt>Existing Standards</dt>
          <dd>IEC 61850 - International standard for data models
          and communication protocols<br>
          IEEE 1547 - US standard for interconnecting distributed
          resources with electric power systems</dd>
        </dl>
      </section>
    </section>
    <section id="transportation">
      <h3 id="x2-8-transportation"><bdi class="secno">2.8</bdi>
      Transportation<a class="self-link" aria-label="§" href=
      "#transportation"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>Zoltan Kis</dd>
        <dt>Sub-categories</dt>
        <dd>Transportation - Infrastructure Transportation - Cargo
        Transportation - People</dd>
        <dt>Target Users</dt>
        <dd>
          <p>Smart Cities: managing roads, public transport and
          commuting, autonomous and human driven vehicles,
          transportation tracking and control systems, route
          information systems, commuting and public transport,
          vehicles, on-demand transportation, self driving fleets,
          vehicle information and control systems, infrastructure
          sharing and payment system, smart parking, smart vehicle
          servicing, emergency monitoring, etc.</p>
          <p>Transport companies: managing shipping, air cargo,
          train cargo and last mile delivery transportation systems
          including automated systems.</p>
          <p>Commuters: Mobility as a service, booking systems,
          route planning, ride sharing, self-driving,
          self-servicing infrastructure, etc.</p>
        </dd>
        <dt>Motivation</dt>
        <dd>
          <p>Provide common vocabulary for describing transport
          related services and solutions that can be reused across
          sub-categories, for easier interoperability between
          various systems owned by different stakeholders.</p>
          <p>Thing models could be defined in many subdomains to
          help integration or interworking between multiple
          systems.</p>
          <p>Transportation of goods can be optimized at global
          level by enhancing interoperability between vertical
          systems.</p>
        </dd>
        <dt>Expected Devices</dt>
        <dd>Road information system (routes, conditions,
        navigation). Road control system (e.g. virtual rails).
        Traffic management services, e.g. intelligent traffic light
        system with localization and identification (by satellite,
        radio frequency identification, cameras etc.). Emergency
        monitoring and data/location sharing. Airport management.
        Shipping docks and ports management. Train networks
        management. Public transport vehicles (train, metro, tram,
        bus, minibus), mobility as a service (ride sharing, bicycle
        sharing, scooters etc.). Transportation network planning
        and management (hubs, backbones, sub-networks, last mile
        network). Electronic timetable management system. Vehicles
        (human driven, self-driving, isolated or part of fleet).
        Connected vehicles (cars, ships, airplanes, trains, buses
        etc). Devices needed for cargo.</dd>
        <dt>Expected Data</dt>
        <dd>Vehicle data (identification, location, speed, route,
        selected vehicle data). Weather and climate data.
        Contextual data (representing various risk factors, delays,
        etc.).</dd>
        <dt>Dependencies</dt>
        <dd>Localization technologies. Automotive data. Contextual
        data. Cloud integration.</dd>
        <dt>Description</dt>
        <dd>Transportation system implementers will be able to use
        a unified data description model across various
        systems.</dd>
        <dt>Variants</dt>
        <dd>
          There will be different verticals, such as:
          <ul>
            <li>Smart City public transport</li>
            <li>Smart City traffic management</li>
            <li>Smart city vehicle management</li>
            <li>Cargo traffic management</li>
            <li>Cargo vehicle management</li>
          </ul>
        </dd>
        <dd></dd>
      </dl>
    </section>
    <section id="automotive">
      <h3 id="x2-9-automotive"><bdi class="secno">2.9</bdi>
      Automotive<a class="self-link" aria-label="§" href=
      "#automotive"></a></h3>
      <section id="mmi-2-1_smart-car-configuration-management">
        <h4 id="x2-9-1-smart-car-configuration-management">
        <bdi class="secno">2.9.1</bdi> Smart Car Configuration
        Management<a class="self-link" aria-label="§" href=
        "#mmi-2-1_smart-car-configuration-management"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Motivation</dt>
          <dd>User interface personalization is a task that most
          often needs to be repeated for all Devices a user wishes
          to interact with recurringly. With complex devices, this
          task can also be very time-consuming, which is
          problematic if the user regularly accesses similar, but
          not identical devices, as in the case of several cars
          rented over a month. A standardized set of personal
          information and preferences that could be used to
          configure personalizable devices automatically would be
          very helpful for all these cases in which the interaction
          becomes a customary practice.</dd>
          <dt>Expected Devices</dt>
          <dd>Personal mobile device running an application
          providing command mediation capabilities. IoT-enabled
          smart car supporting remote sensing, actuation, and
          configuration functionality.</dd>
          <dt>Expected Data</dt>
          <dd>Command and status information transferred between
          the personal mobile device application and the car's
          services and devices. Profile data for user
          preferences.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on
              mobile personal device and possibly in IoT
              orchestration services in the car.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>Basic in-car functionality is standardized to be
          managed by other devices. A user can control seat, radio
          or AC settings through a personalized multimodal
          interface shared by the car and her personal mobile
          device. User preferences are stored on the mobile Device
          (or in the cloud), and can be transferred across
          different car models handling a specific functionality
          (e.g. all cars with touchscreens should be able to adapt
          to a "high contrast" preference). The car can make itself
          available as a complex modality component that wraps
          around all functionality and supported modalities, or as
          a collection of modality components such as touchscreen,
          speech recognition system, or audio player. In the latter
          case, certain user preferences may be shared with other
          environments. For example, a user may opt to select the
          "high contrast" scheme at night on all of her displays,
          in the car or at home. A car that provides a set of
          modalities can be also adapted by the mobile device to
          compose an interface for its functionality, for example
          to manage playback of music tracks through the car's
          voice control system. Sensor data provided by the phone
          can be mixed with data recorded by the car's own sensors
          to profile user behavior which can be used as context in
          multimodal interaction.</dd>
          <dt>Variants</dt>
          <dd>Additional portable devices may be brought into the
          car and also be incorporated into an application, for
          example, a GPS navigation system.</dd>
          <dt>Gaps</dt>
          <dd>Data format describing user interface
          preferences.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 2.1.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case.</dd>
        </dl>
      </section>
    </section>
    <section id="smart-home">
      <h3 id="x2-10-smart-home"><bdi class="secno">2.10</bdi> Smart
      Home<a class="self-link" aria-label="§" href=
      "#smart-home"></a></h3>
      <section id="nhk-device-tv-sync">
        <h4 id=
        "x2-10-1-home-wot-devices-synchronize-to-tv-programs">
        <bdi class="secno">2.10.1</bdi> Home WoT devices
        synchronize to TV programs<a class="self-link" aria-label=
        "§" href="#nhk-device-tv-sync"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Hiroki Endo, Masaya Ikeo, Shinya Abe, Hiroshi
          Fujisawa</dd>
          <dt>Target Users</dt>
          <dd>Person watching TV, Broadcasters</dd>
          <dt>Motivation</dt>
          <dd>
            A lot of home devices, such as TV, cleaner, and home
            lighting, connect to an IP network. When you watch a
            content program, these devices should cooperate for
            enhancing your experience. If the cleaning robot makes
            a loud noise while watching the TV program, it will
            hinder viewing. Also, even if you set up the theater
            environment with smart lights, it is troublesome to
            operate it yourself each time the TV program switches.
            Therefore, by WoT device to operate in accordance with
            the TV program being viewed, thereby improving the user
            experience. WoT devices work according to TV programs:
            <ul>
              <li>Cleaning robot stops at an important
              situation,</li>
              <li>Color of smart lights are changed according to TV
              programs,</li>
              <li>Smart Mirror is notified that favorite TV show
              will start.</li>
            </ul>
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <ul>
              <li>Hybridcast TV</li>
              <li>Hybridcast Connect application (in a smartdevice
              such as smartphone)</li>
              <li>Cleaning Robot</li>
              <li>Smart Light (such as Philips Hue)</li>
              <li>Smart Mirror</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>The trigger value of the scene of the TV program.
          Hybridcast connect application know the Thing Description
          of the devices in home. (Discovery?)</dd>
          <dt>Description</dt>
          <dd>
            <p>Home smart devices behave according to TV
            programs.</p>
            <p>Hybridcast applications in TV emit information about
            TV programs for smart home devices. (Hybridcast is a
            Japanese Integrated Broadcast-Broadband system.
            Hybridcast applications are HTML5 applications that
            work on Hybridcast TV.)</p>
            <p>Hybridcast Contact application receives the
            information and controlls smart home devices.<br>
            <br>
            <img src="images/scenario_nhk.png" width="100%" height=
            "100%"><br></p>
          </dd>
        </dl>
      </section>
    </section>
    <section id="education">
      <h3 id="x2-11-education"><bdi class="secno">2.11</bdi>
      Education<a class="self-link" aria-label="§" href=
      "#education"></a></h3>
      <section id="education">
        <h4 id="x2-11-1-shared-devices"><bdi class=
        "secno">2.11.1</bdi> Shared Devices<a class="self-link"
        aria-label="§" href="#education"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Ege Korkan</dd>
          <dt>Target Users</dt>
          <dd>
            For the education category:
            <ul>
              <li>device owners : The university -&gt; Research
              Group -&gt; Specific Lab</li>
              <li>device user : Students and potentially anyone who
              participates in plugfests</li>
              <li>service provider : The university -&gt; Research
              Group</li>
              <li>network operator : The university</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>This use case motivates a standardized use of shared
          resources. One example is when a physical resource of the
          Thing should not be used by multiple Consumers at the
          same time like the arm of the robot but its position can
          be read my multiple Consumers.</dd>
          <dt>Expected Devices</dt>
          <dd>
            Concrete devices are irrelevant for this use case but
            devices with a physical state is required. However, we
            have currently the following devices that are connected
            to Raspberry Pis where the WoT stack (node-wot or
            similar) is running. Concrete device models can be
            given upon request.
            <ul>
              <li>Robotic arms</li>
              <li>Conveyor belts</li>
              <li>Motorized sliders where the robots or devices can
              be mounted on</li>
              <li>Philips Hue devices: Light bulbs, LED Strips,
              Motion sensors, Switch. We do not have the source
              code of these devices (brownfield)</li>
              <li>Various sensors (brightness, humidity,
              temperature, gyroscopic sensors)</li>
              <li>LED Screen to display messages</li>
            </ul>There are also IP Cameras but they are not WoT
            compatible and are not planned to be made compatible.
          </dd>
          <dt>Expected Data</dt>
          <dd>Atmospheric data of a room, machine sensors</dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>Thing Description, Scripting API, possibly
          security</dd>
          <dt>Description</dt>
          <dd>
            We are offering a practical course for the students
            where they can interact fully remotely with WoT devices
            and verify their physical actions via video streams. We
            have sensors and actuators like robots. Students then
            build mashup applications to deepen their knowledge of
            WoT technologies. Official page of the course is
            <a href=
            "https://campus.tum.de/tumonline/wbLv.wbShowLVDetail?pStpSpNr=950504601&amp;pSpracheNr=1">
            here</a>.
          </dd>
          <dt>Security Considerations</dt>
          <dd>The devices are connected to the Internet and are
          secured behind a router and proxy.</dd>
          <dt>Privacy Considerations</dt>
          <dd>None from the WoT point of view since we want the
          devices to be used by anyone and the devices do not share
          any information that is related to the students or us as
          the provider of the devices. However, there are cameras
          which can show humans entering the room as a side effect
          (they are meant to monitor the devices). The streams are
          accessible only to authorized users, the room has signs
          on the door and there is a cage around the area that is
          filmed.</dd>
          <dt>Gaps</dt>
          <dd>
            <p>Thing Description</p>
            <ul>
              <li>How to give hints that a particular action should
              not be used by others at the same time. A new keyword
              (like <code>"shared":true</code>) would be needed for
              devices that do not implement a describable
              mechanism.</li>
              <li>How to describe the mechanism that the Thing
              implements to manage the shared resources. Does it
              happen in the security level?</li>
            </ul>
            <p>Scripting API</p>
            <ul>
              <li>How does the Consumer code change when this
              mechanism is used. Does it get settled in the
              implementation or scripting level.</li>
            </ul>
          </dd>
        </dl>
      </section>
    </section>
  </section>
  <section id="sec-horizontal-ucs">
    <h2 id="x3-use-cases-for-multiple-domains"><bdi class=
    "secno">3.</bdi> Use Cases for multiple domains<a class=
    "self-link" aria-label="§" href="#sec-horizontal-ucs"></a></h2>
    <section id="Discovery">
      <h3 id="x3-1-discovery"><bdi class="secno">3.1</bdi>
      Discovery<a class="self-link" aria-label="§" href=
      "#Discovery"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>Michael McCool</dd>
        <dt>Target Users</dt>
        <dd>
          All stakeholders:
          <ul>
            <li>device owners</li>
            <li>device user</li>
            <li>cloud provider</li>
            <li>service provider</li>
            <li>device manufacturer</li>
            <li>gateway manufacturer</li>
            <li>network operator (potentially transparent for WoT
            use cases)</li>
            <li>identity provider</li>
            <li>directory service operator</li>
          </ul>
        </dd>
        <dt>Motivation</dt>
        <dd>Discovery defines a distribution mechanism for the
        metadata contained in WoT Things Descriptions, and allows
        Things to advertise their capabilities and for potential
        consumers to find Things that match their needs. A
        standardized discovery mechanism is an enabler for
        convenient and ad-hoc orchestration of combinations of
        Things from different vendors while supporting appropriate
        security and privacy controls.</dd>
        <dt>Expected Devices</dt>
        <dd>
          <ul>
            <li>Thing - any device or service that wishes to
            distribute (advertise) its metadata.</li>
            <li>Consumer - any device or service that wishes to
            find Things whose location and metadata satisfies
            specified constraints.</li>
            <li>Discovery Service - Mechanism by which metadata is
            distributed, which can involve a variety of services to
            handle spatial and semantic queries, register Thing
            Descriptions, provide access controls, etc.</li>
          </ul>
        </dd>
        <dt>Expected Data</dt>
        <dd>
          <ul>
            <li>Thing Descriptions - metadata describing a
            Thing</li>
          </ul>
        </dd>
        <dt>Affected WoT deliverables and/or work items</dt>
        <dd>
          <ul>
            <li>WoT Discovery</li>
          </ul>Note: this is a "horizontal" use case, and is driven
          by requirements in multiple verticals.
        </dd>
        <dt>Description</dt>
        <dd>A user wishing to build or instantiate an IoT service
        needs access to Thing Descriptions of installed and running
        devices satisfying specific requirements. These
        requirements can include being in or near a certain
        location, accessible using particular protocols or on a
        certain network, satisfying certain semantic categories,
        having certain capabilities, or having specific sub-APIs
        (interfaces). Discovery is the general process whereby WoT
        Thing Descriptions satisfying a specific set of such
        constraints are retrieved by a running system.</dd>
        <dt>Variants</dt>
        <dd>
          <ul>
            <li>Run-time discovery allows late binding of
            orchestration services to particular devices and
            requires that consumers be able to adapt to Thing
            Descriptions discovered when a service is
            deployed.</li>
            <li>Development-time discovery may be useful during
            system development to build services that can interface
            to a particular class of Thing Descriptions. In this
            case what actually needs to be discovered Thing Models,
            not specific Thing Descriptions.</li>
          </ul>
        </dd>
        <dt>Security Considerations</dt>
        <dd>
          <ul>
            <li>The distribution mechanism needs to be able to
            clearly authenticate potential users.</li>
            <li>The distribution mechanism for metadata should only
            provide metadata to authorized users.</li>
            <li>The distribution mechanism should be able to resist
            denial-of-service attacks seeking to overwhelm it
            within spurious requests.</li>
            <li>The distribution mechanism should be able to
            preserve the integrity of metadata.</li>
          </ul>
        </dd>
        <dt>Privacy Considerations</dt>
        <dd>
          <ul>
            <li>Metadata should only be distributed to appropriate
            sets of requesters, with the definition of
            "appropriate" configurable by the source of the
            metadata.</li>
            <li>Unauthorized users should not be able to access or
            infer information that they do not have access rights
            to.</li>
            <li>Providers of metadata should be able to withdraw
            metadata from distribution at any time.</li>
            <li>Metadata should not be retained indefinitely.</li>
          </ul>
        </dd>
        <dt>Gaps</dt>
        <dd>
          <ul>
            <li>The current WoT standards define a metadata format
            (the Thing Description) but not a means of distributing
            it.</li>
          </ul>
        </dd>
        <dt>Existing Standards</dt>
        <dd>
          <ul>
            <li>WoT Thing Description</li>
            <li>CoreRD</li>
            <li>DID</li>
          </ul>
        </dd>
        <dt>Comments</dt>
        <dd>
          <ul>
            <li>Many discovery mechanisms already exist but many do
            not satisfy all the requirements above, e.g. they may
            have insufficient privacy controls. A standards
            solution that builds upon prior work in this area is
            desirable.</li>
          </ul>
        </dd>
      </dl>
    </section>
    <section id="multi-vendor">
      <h3 id=
      "x3-2-multi-vendor-system-integration-out-of-the-box-interoperability">
      <bdi class="secno">3.2</bdi> Multi-Vendor System Integration
      - Out of the box interoperability<a class="self-link"
      aria-label="§" href="#multi-vendor"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>Michael Lagally</dd>
        <dt>Target Users</dt>
        <dd>
          <ul>
            <li>device owner</li>
            <li>service provider</li>
            <li>cloud provider</li>
            <li>device manufacturer</li>
            <li>gateway manufacturer</li>
          </ul>
        </dd>
        <dt>Motivation</dt>
        <dd>
          <ul>
            <li>As a device owner, I want to know whether a device
            will work with my system before I purchase it to avoid
            wasting money.</li>
            <ul>
              <li>Installers of IoT devices want to be able to
              determine if a given device will be compatible with
              the rest of their installed systems and whether they
              will have access to its data and affordances.</li>
            </ul>
          </ul>
          <ul>
            <li>As a developer, I want TDs to be as simple as
            possible so that I can efficiently develop them.</li>
            <ul>
              <li>Here "simple" should relate to the end goal,
              "efficiently develop"; that is, TDs should be
              straightforward for the average developer to complete
              and validate.</li>
            </ul>
          </ul>
          <ul>
            <li>As a developer, I want to be able to validate that
            a Thing will be compatible with a Consumer without
            having to test against every possible consumer.</li>
          </ul>
          <ul>
            <li>As a cloud provider I want to onboard, manage and
            communicate with as many devices as possible out of the
            box.</li>This should be possible without device
            specific customization.
          </ul>
        </dd>
        <dt>Expected Devices</dt>
        <dd>sensors, actuators, gateways, cloud, directory
        service.</dd>
        <dt>Expected Data</dt>
        <dd>discrete or streaming data.</dd>
        <dt>Affected WoT deliverables and/or work items</dt>
        <dd>WoT Profile, WoT Thing Description</dd>
        <dt>Description</dt>
        <dd>As a consumer of devices I want to be able to process
        data from any device that conforms to a class of devices. I
        want to have a guarantee that I'm able to correctly
        interact with all affordances of the Thing that complies
        with this class of devices. Behavioral ambiguities between
        different implementations of the same description should
        not be possible. I want to integrate it into my existing
        scenarios out of the box, i.e. with close to zero
        configuration tasks.</dd>
        <dt>Comments</dt>
        <dd>
          The profile specification is currently in development by
          the architecture task force. The current draft of the
          specification is available at: <a href=
          "https://github.com/w3c/wot-profile">https://github.com/w3c/wot-profile</a><br>

          Recommendations for commonalities and interoperability
          profiles of IoT platforms:<a href=
          "https://european-iot-pilots.eu/wp-content/uploads/2018/11/D06_02_WP06_H2020_CREATE-IoT_Final.pdf">https://european-iot-pilots.eu/wp-content/uploads/2018/11/D06_02_WP06_H2020_CREATE-IoT_Final.pdf</a>
        </dd>
      </dl>
    </section>
    <section id="digital-twin">
      <h3 id="x3-3-digital-twin"><bdi class="secno">3.3</bdi>
      Digital Twin<a class="self-link" aria-label="§" href=
      "#digital-twin"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>Michael Lagally</dd>
        <dt>Target Users</dt>
        <dd>Device owners, cloud provider.</dd>
        <dt>Motivation</dt>
        <dd>
          <p>A digital twin is the virtual representation of a
          physical asset such as a machine, a vehicle, robot,
          sensor. Using a digital twin allows businesses to analyze
          their physical assets to troubleshoot in real time,
          predict future problems, minimize downtime, and perform
          simulations to create new business opportunities.</p>
          <p>A digital twin may also be called a twin or a shadow.
          Digital twin technology may be referred to as device
          virtualization.</p>
          <p>Digital twins can be located in the edge or in the
          cloud.</p>
        </dd>
        <dt>Expected Devices</dt>
        <dd>
          <p>Various devices such as sensors, machines, vehicles,
          production lines, industry robots.</p>
          <p>Digital twin platforms at the edge or in the
          cloud.</p>
        </dd>
        <dt>Expected Data</dt>
        <dd>Machine status information, discrete sensor data or
        data streams.</dd>
        <dt>Dependencies</dt>
        <dd>
          <ul>
            <li>WoT Architecture</li>
            <li>WoT Thing Description</li>
            <li>WoT Profile</li>
            <li>WoT Scripting?</li>
          </ul>
        </dd>
        <dt>Description</dt>
        <dd>
          The user benefits from using digital twins with the
          following scenarios:
          <ul>
            <li>Better visibility: Continually view the operations
            of the machines or devices, and the status of their
            interconnected systems.</li>
          </ul>
          <ul>
            <li>Accurate prediction: Retrieve the future state of
            the machines from the digital twin model by using
            modeling.</li>
          </ul>
          <ul>
            <li>What-if analysis: Easily interact with the model to
            simulate unique machine conditions and perform what-if
            analysis using well-designed interfaces.</li>
          </ul>
          <ul>
            <li>Documentation and communication: Use of the digital
            twin model helps to understand, document, and explain
            the behavior of a specific machine or a collection of
            machines.</li>
          </ul>
          <ul>
            <li>Integration of disparate systems: Connect with
            back-end applications related to supply chain
            operations such as manufacturing, procurement,
            warehousing, transportation, or logistics.</li>
          </ul>
        </dd>
        <dt>Variants</dt>
        <dd></dd>
        <dt>Virtual Twin</dt>
        <dd>
          <p>The virtual twin is a representation of a physical
          device or an asset. A virtual twin uses a model that
          contains observed and desired attribute values and also
          uses a semantic model of the behavior of the device.</p>
          <p>Intermittent connectivity: An application may not be
          able to connect to the physical asset. In such a
          scenario, the application must be able to retrieve the
          last known status and to control the operation states of
          other assets.</p>
          <p>Protocol abstraction: Typically, devices use a variety
          of protocols and methods to connect to the IoT network.
          From a users perspective this complexity should not
          affect other business applications such as an enterprise
          resource planning (ERP) application.</p>
          <p>Business rules: The user can specify the normal
          operating range of a property in a semantic model.
          Business rules can be declaratively defined and actions
          can be automatically invoked in the edge or on the
          device.</p>
          <p>Example: In a fleet of connected vehicles, the user
          monitors a collection of operating parameters, such as
          fuel level, location, speed and others. The
          semantics-based virtual twin model enables the user to
          decide whether the operating parameters are in normal
          range. In out of range conditions the user can take
          appropriate actions.</p>
        </dd>
        <dt>Predictive Twin</dt>
        <dd>
          <p>In a predictive twin, the digital twin implementation
          builds an analytical or statistical model for prediction
          by using a machine-learning technique. It need not
          involve the original designers of the machine. It is
          different from the physics-based models that are static,
          complex, do not adapt to a constantly changing
          environment, and can be created only by the original
          designers of the machine.</p>
          <p>A data analyst can easily create a model based on
          external observation of a machine and can develop
          multiple models based on the user’s needs. The model
          considers the entire business scenario and generates
          contextual data for analysis and prediction.</p>
          <p>When the model detects a future problem or a future
          state of a machine, the user can prevent or prepare for
          them. The user can use the predictive twin model to
          determine trends and patterns from the contextual machine
          data. The model helps to address business problems.</p>
        </dd>
        <dt>Twin Projections</dt>
        <dd>
          <p>In twin projections, the predictions and the insights
          integrate with back-end business applications, making IoT
          an integral part of business processes. When projections
          are integrated with a business process, they can trigger
          a remedial business workflow.</p>
          <p>Prediction data offers insights into the operations of
          machines. Projecting these insights into the back-end
          applications infrastructure enables business applications
          to interact with the IoT system and transform into
          intelligent systems. ^</p>
        </dd>
        <dt>Gaps</dt>
        <dd>WoT does not define a way to describe the behavior of a
        thing to use for a simulation.</dd>
      </dl>
    </section>
    <section id="X-Protocol-Interworking">
      <h3 id="x3-4-cross-protocol-interworking"><bdi class=
      "secno">3.4</bdi> Cross Protocol Interworking<a class=
      "self-link" aria-label="§" href=
      "#X-Protocol-Interworking"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>Michael Lagally</dd>
        <dt>Target Users</dt>
        <dd>Device owners, cloud providers.</dd>
        <dt>Motivation</dt>
        <dd>In smart city, home and industrial scenarios various
        devices are connected to a common network. These devices
        implement different protocols. To enable interoperability,
        an "agent" needs to communicate across different protocols.
        Platforms for this agent can be edge devices, gateways or
        cloud services. Interoperability across protocols is a must
        for all user scenarios that integrate devices from more
        than one protocol.</dd>
        <dt>Expected Devices</dt>
        <dd>Various sensors, e.g. temperature, light, humidity,
        vibration, noise, air quality, edge devices, gateways,
        cloud servers and services.</dd>
        <dt>Expected Data</dt>
        <dd>Discrete sensor values, such as temperature, light,
        humidity, vibration, noise, air quality readings. A/V
        streams. The data can be delivered periodically or on
        demand.</dd>
        <dt>Dependencies</dt>
        <dd>WoT Profiles.</dd>
        <dt>Description</dt>
        <dd>
          <p>There are multiple user scenarios that are addressed
          by this use case.</p>
          <p>An example in the smart home environment is an
          automatic control lamps, air conditioners, heating,
          window blinds in a household based on sensor data, e.g.
          sunlight, human presence, calendar and clock, etc.</p>
          <p>In an industrial environment individual actuators and
          production devices use different protocols. Examples
          include MQTT, OPC-UA, Modbus, Fieldbus, and others.
          Gathering data from these devices, e.g. to support
          digital twins or big data use cases requires an "Agent"
          to bridge across these protocols. To provide
          interoperability and to reduce implementation complexity
          of this agent a common set of (minimum and maximum)
          requirements need to be supported by all interoperating
          devices.</p>
          <p>A smart city environment is similar to the industrial
          scenario in terms of device interoperability. Devices
          differ however, they include smart traffic lights,
          traffic monitoring, people counters, cameras.</p>
        </dd>
        <dt>Gaps</dt>
        <dd>A common profile across protocols is required to
        address this use case.</dd>
        <dt>Existing Standards</dt>
        <dd>MQTT, OPC-UA, BACNet, CoAP, various other home and
        industrial protocols.</dd>
      </dl>
    </section>
    <section id="multimodal">
      <h3 id="x3-5-multimodal-system-integration"><bdi class=
      "secno">3.5</bdi> Multimodal System Integration<a class=
      "self-link" aria-label="§" href="#multimodal"></a></h3>
      <section id="mmi-5-1_multimodal-recognition-support">
        <h4 id="x3-5-1-multimodal-recognition-support"><bdi class=
        "secno">3.5.1</bdi> Multimodal Recognition Support<a class=
        "self-link" aria-label="§" href=
        "#mmi-5-1_multimodal-recognition-support"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Motivation</dt>
          <dd>Recognizer system development has arrived at a point
          of maturity where if we want to dramatically enhance
          recognition performance, sensor fusion from multiple
          modalities is needed. In order to achieve this, an image
          recognizer should incorporate results coming from other
          kinds of recognizers (e.g. audio recognizer) within the
          network engaged in the same interaction cycle.</dd>
          <dt>Expected Devices</dt>
          <dd>Audio sensing device (microphone). Video sensing
          device (camera). Audio recognition service. Video
          recognition service. Devices capable of presenting alerts
          in various modalities.</dd>
          <dt>Expected Data</dt>
          <dd>Command and status information transferred between
          the sensing devices, the recognition services, and the
          alert devices. Profile data for user preferences.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on
              mobile personal device and possibly in IoT
              orchestration services.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>An audio recognizer has been trained with the more
          common sounds in the house, in order to provide alerts in
          case of an emergency. In the same house a security system
          uses a video recognizer to identify people at the front
          door. These two systems need to cooperate with a remote
          home management system to provide integrated
          services.</dd>
          <dt>Gaps</dt>
          <dd>Support for video and audio recognition
          services.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 5.1.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case.</dd>
        </dl>
      </section>
      <section id=
      "mmi-5-2_enhancement-of-synergistic-interactions">
        <h4 id="x3-5-2-enhancement-of-synergistic-interactions">
        <bdi class="secno">3.5.2</bdi> Enhancement of Synergistic
        Interactions<a class="self-link" aria-label="§" href=
        "#mmi-5-2_enhancement-of-synergistic-interactions"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Motivation</dt>
          <dd>One of the main indicators concerning the usability
          of a system is the corresponding level of accessibility
          provided by it. The opportunity for all the users to
          receive and to deliver all kinds of information,
          regardless of the information format or the type of user
          profile, state or impairment is a recurrent need in web
          applications. One of the means to achieve accessibility
          is the design of a more synergic interaction based on the
          discovery of multimodal Modality Components. Synergy is
          two or more entities functioning together to produce a
          result that is not obtainable independently. It means
          "working together". For example, how to avoid disruptive
          interactions in nomadic systems (always affected by the
          changing context) is an important issue. In these
          applications, user interaction is difficult, distracted
          and less precise. Discovery and use of alternative input
          and output devices can increase synergic interaction
          offering new possibilities more adapted to the current
          context. Such a system can also enhance the fusion
          process for target groups of users experiencing permanent
          or temporary learning difficulties or with sensorial,
          emotional or social impairments.</dd>
          <dt>Expected Devices</dt>
          <dd>A normal client computer with I/O devices that need
          to be emulated. Alternative I/O devices that need to be
          interfaced to the client system.</dd>
          <dt>Expected Data</dt>
          <dd>Command and status information transferred between
          the client computer and the alternative I/O devices.
          Profile data for user preferences.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on
              mobile personal device and possibly in IoT
              orchestration services.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>A person working mostly with a PC is having a problem
          with his right arm and hands. He is unable to use a mouse
          or a keyboard for a few months. He can point at things,
          sketch, clap, make gestures, but he can not make any
          precise movements. A generic interface allows this person
          to perform his most important tasks in his personal
          devices: to call someone, open a mailbox, access his
          agenda or navigate over some Web pages. The generic
          interface can propose child-oriented intuitive interfaces
          like a clapping-based interface, a very articulated TTS
          component, or reduced gesture input widgets. Other
          specialized devices might include phones with very big
          numbers, very simple remote controls, screens displaying
          text at high resolution, or voice command devices.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 5.2.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case.</dd>
        </dl>
      </section>
    </section>
    <section id="accessibility">
      <h3 id="x3-6-accessibility"><bdi class="secno">3.6</bdi>
      Accessibility<a class="self-link" aria-label="§" href=
      "#accessibility"></a></h3>
      <section id=
      "mmi-1-1_audiovisual-devices-as-smartphone-extensions">
        <h4 id=
        "x3-6-1-audiovisual-devices-acting-as-smartphone-extensions">
        <bdi class="secno">3.6.1</bdi> Audiovisual Devices Acting
        as Smartphone Extensions<a class="self-link" aria-label="§"
        href=
        "#mmi-1-1_audiovisual-devices-as-smartphone-extensions"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Motivation</dt>
          <dd>
            <p>Many of today's home IoT-enabled devices can provide
            similar functionality (e.g. audio/video playback),
            differing only in certain aspects of the user
            interface. This use case would allow continuous
            interaction with a specific application as the user
            moves from room to room, with the user interface
            switched automatically to the set of devices available
            in the user's present location.</p>
            <p>On the other hand, some devices can have specific
            capabilities and user interfaces that can be used to
            add information to a larger context that can be reused
            by other applications and devices. This drives the need
            to spread an application across different devices to
            achieve a more user-adapted and meaningful interaction
            according to the context of use. Both aspects provide
            arguments for exploring use cases where applications
            use distributed multimodal interfaces.</p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>Mobile phone or other client running an application
          requiring a extended and more accessible user interface.
          IoT-enabled audio-visual devices providing audio and
          visual information display capabilities that can be used
          to augment the user interface of the application.
          Possible edge computation services providing
          speech-to-text or described video (e.g. object detection)
          capabilities.</dd>
          <dt>Expected Data</dt>
          <dd>Visual display information mapping information from
          audio to visual modalities, for example text generated
          from voice recognition. Text from an application that
          needs to be displayed at a larger size. Visual alerts
          corresponding to audio stimuli, e.g. sound effects in a
          game mapped to visual icons. Visual information mapped to
          audio information, for example, described video based on
          an AI service providing object recognition.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API accessible from
              application for interacting with devices.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>A home entertainment system is adapted by a mobile
          device as a set of user interface components. In addition
          to media rendering and playback, these Devices also act
          as input or output modalities for an application, for
          example an application running on a smartphone. The
          native user interface on the application does not have to
          be manipulated directly at all. A wall-mounted
          touch-sensitive TV could be used to navigate
          applications, and a wide-range microphone can handle
          speech input. Spatial (Kinect-style) gestures may also be
          used to control application behavior. Accessibility
          support software on the smartphone discovers available
          modalities and arranges them to best serve the user's
          purpose. One display can be used to show photos and
          movies, another for navigation. As the user walks into
          another room, this configuration is adapted dynamically
          to the new location. User intervention may be sometimes
          required to decide on the most convenient modality
          configuration. The state of the interaction is maintained
          while switching between modality sets. For example, if
          the user was navigating a GUI menu in the living room, it
          is carried over to another screen when she switches
          rooms, or replaced with a different modality such as
          voice if there are no displays in the new location.</dd>
          <dt>Variants</dt>
          <dd>Modalities may be translated from one form to another
          to accommodate accessibility issues, for example, visual
          cues into audio cues and vice-versa, as appropriate.</dd>
          <dt>Gaps</dt>
          <dd>An AI service may be require to perform modality
          mapping, for example, object recognition.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 1.1.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case. Variant supporting modality conversion is
          not included in the original MMI use case.</dd>
        </dl>
      </section>
      <section id="mmi-1-2_unified-smart-home-control-and-status">
        <h4 id=
        "x3-6-2-unified-smart-home-control-and-status-interface">
        <bdi class="secno">3.6.2</bdi> Unified Smart Home Control
        and Status Interface<a class="self-link" aria-label="§"
        href=
        "#mmi-1-2_unified-smart-home-control-and-status"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool</dd>
          <dt>Category</dt>
          <dd>Accessibility</dd>
          <dt>Target Users</dt>
          <dd></dd>
          <dt>Motivation</dt>
          <dd>The increase in the number of controllable devices in
          an intelligent home creates a problem with controlling
          all available services in a coherent and useful manner.
          Having a shared context, built from information collected
          through sensors and direct user input, would improve
          recognition of user intent, and thus simplify
          interactions. In addition, multiple input mechanisms
          could be selected by the user based on device type, level
          of trust and the type of interaction required for a
          particular task.</dd>
          <dt>Expected Devices</dt>
          <dd>Mobile phone or other client running an application
          providing command mediation capabilities. IoT-enabled
          smart home devices supporting remote sensing and
          actuation functionality.</dd>
          <dt>Expected Data</dt>
          <dd>Command and status information transferred between
          the command mediation application and one or more
          devices.</dd>
          <dt>Dependencies</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API accessible from
              application for interacting with devices.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>
            <p>Smart home functionality (window blinds, lights, air
            conditioning etc.) is controlled through a multimodal
            interface, composed from modalities built into the
            house itself (e.g. speech and gesture recognition) and
            those available on the user's personal devices (e.g.
            smartphone touchscreen). The system may automatically
            adapt to the preferences of a specific user, or enter a
            more complex interaction if multiple people are
            present.</p>
            <p>Sensors built into various devices around the house
            can act as input modalities that feed information to
            the home and affect its behavior. For example, lights
            and temperature in the gym room can be adapted
            dynamically as workout intensity recorded by the
            fitness equipment increases. The same data can also
            increase or decrease volume and tempo of music tracks
            played by the user's mobile device or the home's media
            system.</p>
          </dd>
          <dt>Variants</dt>
          <dd>The intelligent home in tandem with the user's
          personal devices can additionally monitor user behavior
          for emotional patterns such as 'tired' or 'busy' and
          adapt further.</dd>
          <dt>Gaps</dt>
          <dd>A service may be needed to recognize gestures and
          emotional states.</dd>
          <dt>Existing Standards</dt>
          <dd>This use case is based on MMI UC 1.2; original title
          was Intelligent Home Apparatus.</dd>
          <dt>Comments</dt>
          <dd>Does not include Requirements section from original
          MMI use case.</dd>
        </dl>
      </section>
    </section>
    <section id="Security">
      <h3 id="x3-7-security"><bdi class="secno">3.7</bdi>
      Security<a class="self-link" aria-label="§" href=
      "#Security"></a></h3>
      <section id="oauth">
        <h4 id="x3-7-1-oauth2-flows"><bdi class="secno">3.7.1</bdi>
        OAuth2 Flows<a class="self-link" aria-label="§" href=
        "#oauth"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael McCool, Cristiano Aguzzi</dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device owner</li>
              <li>device user</li>
              <li>device application</li>
              <li>service provider</li>
              <li>identity provider</li>
              <li>directory service</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>
            OAuth 2.0 is an authorization protocol widely known for
            its usage across several web services. It enables
            third-party applications to obtain limited access to
            HTTP services on behalf of the resource owner or of
            itself. The protocol defines the following actors:
            <ul>
              <li>Client: an application that wants to use a
              resource owned by the resource owner.</li>
              <li>Authorization Server: An intermediary that
              authorizes the client for a particular
              <code>scope</code>.</li>
              <li>Resource: a web resource</li>
              <li>Resource Server: the server where the resource is
              stored</li>
              <li>Resource Owner: the owner of a particular web
              resource. If it is a human is usually referred to as
              an end-user. More specifically from the RFC:</li>
              <ul>
                <li>An entity capable of granting access to a
                protected resource.</li>
              </ul>
            </ul>These actors can be mapped to WoT entities:
            <ul>
              <li>Client is a WoT Consumer</li>
              <li>Authorization Server is a third-party
              service</li>
              <li>Resource is an interaction affordance</li>
              <li>Resource Server is a Thing described by a Thing
              Description acting as a server.</li>May be a device
              or a service.
              <li>Resource Owner might be different in each use
              case. A Thing Description may also combine resources
              from different owners or web server.</li>
            </ul>TO DO: Check the OAuth 2.0 spec to determine
            exactly how Resource Owner is defined. Is it the actual
            owner of the resource (e.g. running the web server) or
            simply someone with the rights to access that
            resource?<br>
            The OAuth 2.0 protocol specifies an authorization layer
            that separates the client from the resource owner. The
            basic steps of this protocol are summarized in the
            following diagram:
            <pre aria-busy="false"><code class=
            "hljs">+--------+                               +---------------+
|        |--(A)- Authorization Request -&gt;|   Resource    |
|        |                               |     Owner     |
|        |&lt;-(B)-- Authorization Grant ---|               |
|        |                               +---------------+
|        |
|        |                               +---------------+
|        |--(C)-- Authorization Grant --&gt;| Authorization |
| Client |                               |     Server    |
|        |&lt;-(D)----- Access Token -------|               |
|        |                               +---------------+
|        |
|        |                               +---------------+
|        |--(E)----- Access Token ------&gt;|    Resource   |
|        |                               |     Server    |
|        |&lt;-(F)--- Protected Resource ---|               |
+--------+                               +---------------+</code></pre>Steps
A and B defines what is known as authorization grant type or flow.
What is important to realize here is that not all of these
interactions are meant to take place over a network protocol. In
some cases, interaction with with a human through a user interface
may be intended. OAuth2.0 defines 4 basic flows plus an extension
mechanism. The most common of which are:
            <ul>
              <li><code>code</code></li>
              <li><code>implicit</code></li>
              <li><code>password</code> (of resource owner)</li>
              <li><code>client</code> (credentials of the
              client)</li>
            </ul>In addition, a particular extension which is of
            interest to IoT is the <code>device</code> flow.
            Further information about the OAuth 2.0 protocol can be
            found in <a href=
            "https://tools.ietf.org/html/rfc6749#section-1">IETF
            RFC6749</a>. In addition to the flows, OAuth 2.0 also
            supports scopes. Scopes are identifiers which can be
            attached to tokens. These can be used to limit
            authorizations to particular roles or actions in an
            API. Each token carries a set of scopes and these can
            be checked when an interaction is attempted and access
            can be denied if the token does not include a scope
            required by the interaction. This document describes
            relevant use cases for each of the OAuth 2.0
            authorization flows.
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            To support OAuth 2.0, all devices must have the
            capability of:
            <ul>
              <li>Both the producer and consumer must be able to
              create and participate in a TLS connection.</li>
              <li>The producer must be able to verify an access
              (bearer) token (i.e. have sufficient computational
              power/connectivity).</li>
            </ul>Comment:
            <ul>
              <li>Investigate whether DTLS can be
              used.</li>Certainly the connection needs to be
              encrypted; this is required in the OAuth 2.0
              specification.
              <li>Investigate whether protocols other than HTTP can
              be used, e.g. CoAP.</li>
              <ul>
                <li>found an interesting IETF draft RFC about CoAP
                support(encrypted using various mechanisms like
                DTLS or CBOR Object Signing and Encryption):
                <a href=
                "https://tools.ietf.org/html/draft-ietf-ace-oauth-authz-35">
                  draft-ietf-ace-oauth</a>
                </li>
              </ul>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>Depending on the OAuth 2.0 flow specified, various
          URLs and elements need to be specified, for example, the
          location of an authorization token server. OAuth 2.0 is
          also based on bearer tokens and so needs to include the
          same data as those, for example, expected encryption
          suite. Finally, OAuth 2.0 supports scopes so these need
          to be defined in the security scheme and specified in the
          form.</dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>Thing Description, Scripting API, Discovery, and
          Security.</dd>
          <dt>Description</dt>
          <dd>A general use case for OAuth 2.0 is when a WoT
          consumer wants to access restricted interaction
          affordances. In particular, when those affordances have a
          specific resource owner which may grant some temporary
          permissions to the consumer. The WoT consumer can either
          be hosted in a remote device or interact directly with
          the end-user inside an application.</dd>
          <dt>Variants</dt>
          <dd>
            For each OAuth 2.0 flow, there is a corresponding use
            case variant. We also include the experimental "device"
            flow for consideration.<br>
            <br>
            code A natural application of this protocol is when the
            end-user wants to interact directly with the consumed
            thing or to grant his authorization to a remote device.
            In fact from the <a href=
            "https://tools.ietf.org/html/rfc6749#section-4.1">RFC6749</a>
            <ul>
              <li>Since this is a redirection-based flow, the
              client must be capable of interacting with the
              resource owner's user-agent (typically a web browser)
              and capable of receiving incoming requests (via
              redirection) from the authorization server.</li>
            </ul>This implies that the code flow can be only used
            when the resource owner interacts directly with the WoT
            consumer at least once. Typical scenarios are:
            <ul>
              <li>In a home automation context, a device owner uses
              a third party software to interact with/orchestrate
              one or more devices</li>
              <li>Similarly, in a smart farm, the device owner
              might delegate its authorization to third party
              services.</li>
              <li>In a smart home scenario, Thing Description
              Directories might be deployed using this
              authorization mechanism. In particular, the list of
              the registered TDs might require an explicit read
              authorization request to the device owner (i.e. an
              human who has bought the device and installed
              it).</li>
              <li>...</li>
            </ul>The following diagram shows the steps of the
            protocol adapted to WoT idioms and entities. In this
            scenario, the WoT Consumer has read the Thing
            Description of a Remote Device and want to access one
            of its WoT Affordances protected with OAuth 2.0 code
            flow.
            <pre aria-busy="false"><code class=
            "hljs">                                                 +-----------+
  +----------+                                   |           |
  | Resource |                                   |  Remote   |
  |   Owner  |                                   |  Device   +&lt;-------+
  |          |                                   |           |        |
  +----+-----+                                   +-----------+        |
       ^                                                              |
       |                                                              |
      (B)                                                             |
+------------+          Client Identifier      +---------------+      |
|           ------(A)-- & Redirection URI ----&gt;+               |      |
|   User-    |                                 | Authorization |      |
|   Agent   ------(B)-- User authenticates ---&gt;+     Server    |      |
|            |                                 |               |      |
|           ------(C)-- Authorization Code ---&lt;+               |      |
+---+----+---+                                 +---+------+----+      |
    |    |                                         ^      v           |
   (A)  (C)                                        |      |           |
    |    |                                         |      |           |
    ^    v                                         |      |           |
+---+----+---+                                     |      |           |
|            |&gt;-+(D)-- Authorization Code ---------'      |           |
|    WoT     |         & Redirection URI                  |           |
|  Consumer  |                                            |           |
|            |&lt;-+(E)----- Access Token -------------------'           |
+-----+------+      (w/ Optional Refresh Token)                       |
      v                                                               |
      |                                                               |
      +-----------(F)----- Access WoT --------------------------------+
                           Affordance</code></pre>Notice that steps
(A), (B) and (C) are broken in two parts as they pass through the
User-Agent.
            <p>device</p>The device flow (IETF <a href=
            "https://tools.ietf.org/html/rfc8628">RFC 8628</a>) is
            a variant of the code flow for browserless and
            input-constrained devices. Similarly, to its
            <i>parent</i> flow, it requires a close interaction
            between the resource owner and the WoT consumer.
            Therefore, the use cases for this flow are the same as
            the code authorization grant but restricted to all
            devices that do not have a rich means to interact with
            the resource owner. However, differently from
            <code>code</code>, RFC 8628 states explicitly that one
            of the actors of the protocol is an <b>end-user</b>
            interacting with a <b>browser</b> (even if <a href=
            "https://tools.ietf.org/html/rfc8628#section-6.2">section-6.2</a>
            briefly describes an authentication using a companion
            app and BLE), as shown in the following (slightly
            adapted) diagram:
            <pre aria-busy="false"><code class="hljs">+----------+
|          |
|  Remote  |
|  Device  |
|          |
+----^-----+
     |
     | (G) Access WoT Affordance
     |
+----+-----+                                +----------------+
|          +&gt;---(A)-- Client Identifier ---v+                |
|          |                                |                |
|          +&lt;---(B)-- Device Code,      ---&lt;+                |
|          |          User Code,            |                |
|   WoT    |          & Verification URI    |                |
| Consumer |                                |                |
|          |  [polling]                     |                |
|          +&gt;---(E)-- Device Code       ---&gt;+                |
|          |          & Client Identifier   |                |
|          |                                |  Authorization |
|          +&lt;---(F)-- Access Token      ---&lt;+     Server     |
+-----+----+   (& Optional Refresh Token)   |                |
      v                                     |                |
      :                                     |                |
     (C) User Code & Verification URI       |                |
      :                                     |                |
      ^                                     |                |
+-----+----+                                |                |
| End User |                                |                |
|    at    +&lt;---(D)-- End user reviews  ---&gt;+                |
|  Browser |          authorization request |                |
+----------+                                +----------------+</code></pre>Notable
mentions:
            <ul>
              <li>the protocol is heavily end-user oriented. In
              fact, the RFC states the following</li>
              <ul>
                <li>Due to the polling nature of this protocol (as
                specified in Section 3.4), care is needed to avoid
                overloading the capacity of the token endpoint. To
                avoid unneeded requests on the token endpoint, the
                client <em class="rfc2119">SHOULD</em> only
                commence a device authorization request when
                <b>prompted by the user and not automatically</b>,
                such as when the app starts or when the previous
                authorization session expires or failAs.</li>
              </ul>
              <li>TLS is required both between WoT
              Consumer/Authorization Server and between
              Browser/Authorization Server</li>
              <li>Other user interactions methods may be used but
              are left out of scope</li>
            </ul>
            <p>client credential</p>The Client Credentials grant
            type is used by clients to obtain an access token
            outside of the context of an end-user. From <a href=
            "https://tools.ietf.org/html/rfc6749#section-4.4">RFC6749</a>:
            <ul>
              <li>The client can request an access token using only
              its client credentials (or other supported means of
              authentication) when the client is requesting access
              to the protected resources under its control, or
              <b>those of another resource owner that has been
              previously arranged with the authorization server</b>
              (the method of which is beyond the scope of this
              specification).</li>
            </ul>Therefore the client credential grant can be used:
            <ul>
              <li>When the resource owner is a public authority.
              For example, in a smart city context, the authority
              provides a web service where to register an
              application id.</li>
              <li>Companion application</li>
              <li>Industrial IoT. Consider a smart factory where
              the devices or services are provisioned with client
              credentials.</li>
              <li>...</li>
            </ul>The Client Credentials flow is illustrated in the
            following diagram. Notice how the Resource Owner is not
            present.
            <pre aria-busy="false"><code class="hljs">+----------+
|          |
|  Remote  |
|  Device  |
|          |
+----^-----+
     |
     |  (C) Access WoT Affordance
     ^
+----+-----+                                  +---------------+
|          |                                  |               |
|          +&gt;--(A)- Client Authentication ---&gt;+ Authorization |
|   WoT    |                                  |     Server    |
| Consumer +&lt;--(B)---- Access Token ---------&lt;+               |
|          |                                  |               |
|          |                                  +---------------+
+----------+</code></pre>Comment: Usually client credentials are
distributed using an external service which is used by humans to
register a particular application. For example, the <code>
            npm</code> cli has a companion dashboard where a
            developer requests the generation of a token that is
            then passed to the cli. The token is used to verify the
            publishing process of <code>npm</code> packages in the
            registry. Further examples are Docker cli and OpenId
            Connect Client Credentials.
            <p>implicit</p><b>Deprecated</b> From <a href=
            "https://tools.ietf.org/html/draft-ietf-oauth-security-topics-15#section-2.1.2">
            OAuth 2.0 Security Best Current Practice</a>:
            <ul>
              <li>In order to avoid these issues, clients
              <em class="rfc2119">SHOULD NOT</em> use the implicit
              grant (response type "token") or other response types
              issuing access tokens in the authorization response,
              unless access token injection in the authorization,
              response is prevented and the aforementioned token
              leakage vectors are mitigated.</li>
            </ul>The RFC above suggests using <code>code</code>
            flow with Proof Key for Code Exchange (PKCE)
            instead.<br>
            The implicit flow was designed for public clients
            typically implemented inside a browser (i.e. javascript
            clients). As the <code>code</code> is a
            redirection-based flow and it requires direct
            interaction with the resource's owner user-agent.
            However, it requires one less step to obtain a token as
            it is returned directly in the authentication request
            (see the diagram below).<br>
            Considering the WoT context this flow is not
            particularly different from <code>code</code> grant and
            it can be used in the same scenarios.<br>
            Comment: even if the <code>implicit</code> flow is
            deprecated existing services may still using it.
            <pre aria-busy="false"><code class="hljs">+----------+
| Resource |
|  Owner   |
|          |
+----+-----+
     ^
     |
    (B)
+----------+          Client Identifier     +---------------+
|         ------(A)-- & Redirection URI ---&gt;+               |
|  User-   |                                | Authorization |
|  Agent  ------(B)-- User authenticates --&gt;+     Server    |
|          |                                |               |
|          +&lt;---(C)--- Redirection URI ----&lt;+               |
|          |          with Access Token     +---------------+
|          |            in Fragment
|          |                                +---------------+
|          +----(D)--- Redirection URI ----&gt;+   Web-Hosted  |
|          |          without Fragment      |     Client    |
|          |                                |    Resource   |
|     (F)  +&lt;---(E)------- Script ---------&lt;+               |
|          |                                +---------------+
+-+----+---+
  |    |
 (A)  (G) Access Token
  |    |
  ^    v
+-+----+---+                                   +----------+
|          |                                   |  Remote  |
|   WoT    +&gt;---------(H)--Access WoT---------&gt;+  Device  |
| Consumer |               Affordance          |          |
|          |                                   +----------+
+----------+</code></pre>
            <p>resource owner password</p><b>Deprecated</b> From
            <a href=
            "https://tools.ietf.org/html/draft-ietf-oauth-security-topics-15#section-2.1.2">
            OAuth 2.0 Security Best Current Practice</a>:
            <ul>
              <li>The resource owner password credentials grant
              <em class="rfc2119">MUST NOT</em> be used. This grant
              type insecurely exposes the credentials of the
              resource owner to the client. Even if the client is
              benign, this results in an increased attack surface
              (credentials can leak in more places than just the
              AS) and users are trained to enter their credentials
              in places other than the AS.</li>
            </ul>For completeness the diagram flow is reported
            below.
            <pre aria-busy="false"><code class="hljs"> +----------+
 | Resource |
 |  Owner   |
 |          |
 +----+-----+
      v
      |    Resource Owner
     (A) Password Credentials
      |
      v
+-----+----+                                  +---------------+
|          +&gt;--(B)---- Resource Owner -------&gt;+               |
|          |         Password Credentials     | Authorization |
|   WoT    |                                  |     Server    |
| Consumer +&lt;--(C)---- Access Token ---------&lt;+               |
|          |    (w/ Optional Refresh Token)   |               |
+-----+----+                                  +---------------+
      |
      | (D) Access WoT Affordance
      |
 +----v-----+
 |  Remote  |
 |  Device  |
 |          |
 +----------+</code></pre>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            See OAuth 2.0 security considerations in <a href=
            "https://tools.ietf.org/html/rfc6749#section-10">RFC6749</a>.
            See also <a href=
            "https://tools.ietf.org/html/rfc8628#section-5">RFC
            8628 section 5</a> for <code>device</code> flow.
          </dd>
          <dt>Comments</dt>
          <dd>
            Notice that the OAuth 2.0 protocol is not an
            authentication protocol, however <a href=
            "https://openid.net/connect/">OpenID</a> defines an
            authentication layer on top of OAuth 2.0.
          </dd>
        </dl>
      </section>
    </section>
    <section id="lifecycle">
      <h3 id="x3-8-lifecycle"><bdi class="secno">3.8</bdi>
      Lifecycle<a class="self-link" aria-label="§" href=
      "#lifecycle"></a></h3>
      <section id="device-lifecycle">
        <h4 id="x3-8-1-device-lifecycle"><bdi class=
        "secno">3.8.1</bdi> Device Lifecycle<a class="self-link"
        aria-label="§" href="#device-lifecycle"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>Michael Lagally</dd>
          <dt>Target Users</dt>
          <dd>device manufacturer, gateway manufacturer, cloud
          provider</dd>
          <dt>Motivation</dt>
          <dd>The architecture specification currently does not
          address lifecycle.</dd>
          <dt>Description</dt>
          <dd>
            Handle the entire device lifecycle: Define terminology
            for lifecycle states and transitions.
            <p>Actors (represent a physical person or group of
            persons (company))</p>Manufacturer Service Provider
            Network Provider (potentially transparent for WoT use
            cases) Device Owner (User) Others?
            <p>Roles:</p>Depending on the use case, an actor can
            have multiple roles, e.g. security maintainer. Roles
            can be delegated.
          </dd>
          <dt>Variants</dt>
          <dd>
            There are (at least) two different entities to
            consider:
            <ul>
              <li>Things / Devices</li>
              <li>Consumers, e.g. cloud services or gateways</li>
            </ul>In more complex use cases there are additional
            entities:
            <ul>
              <li>Intermediates</li>
              <li>Directories</li>
            </ul>
          </dd>
          <dt>Gaps</dt>
          <dd>The current architecture spec does not describe
          device lifecycle in detail. A common lifecycle model
          helps to clarify terminology and structures the
          discussion in different groups. Interaction of a device
          with other entities such as directories may introduce
          additional states and transitions.</dd>
          <dt>Existing Standards</dt>
          <dd>
            <ul>
              <li>WoT Security</li>
              <li>ETSI OneM2M</li>
              <li>OMA LwM2M</li>
              <li>OCF</li>
              <li>IEEE</li>
              <li>SIM cards / GSMA</li>
              <li>IETF</li>
              <li>Application Lifecycle (<abbr title=
              "World Wide Web Consortium">W3C</abbr> Multimodal
              Interaction WG)</li>
            </ul>
          </dd>
          <dt>Comments</dt>
          <dd>
            All lifecycle contributions and discussion documents
            are available at: <a href=
            "https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle">
            https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle</a><br>

            <br>
            Documents that were created / discussed in the
            architecture TF.
            <ul>
              <li>Lifecycle comparisons: <a href=
              "https://github.com/w3c/wot-architecture/blob/master/proposals/Device-lifecycle-comparisons.pdf">
                https://github.com/w3c/wot-architecture/blob/master/proposals/Device-lifecycle-comparisons.pdf</a>
              </li>
              <li>Lifecycle states: <a href=
              "https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/lifecycle-states.md">
                https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/lifecycle-states.md</a>
              </li>
              <li>Draft lifecycle diagram: <a href=
              "https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20lifecycle%20diagram-WoT%20new%20lifecycle.svg">
                https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20lifecycle%20diagram-WoT%20new%20lifecycle.svg</a>
              </li>
              <li>Layered lifecycle: <a href=
              "https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20layered%20%20lifecycle%20diagram-WoT%20new%20lifecycle.svg">
                https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20layered%20%20lifecycle%20diagram-WoT%20new%20lifecycle.svg</a>
              </li>
              <li>System lifecycle: <a href=
              "https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/unified%20device%20lifecycle.svg">
                https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/unified%20device%20lifecycle.svg</a>
              </li>
              <li>IoT Security Bootstrapping: <a href=
              "https://github.com/w3c/wot-security/blob/master/presentations/2020-03-16-Bootstrapping%20IoT%20Security%20-%20The%20IETF%20Anima%20and%20OPC-UA%20Recipes.pdf">
                https://github.com/w3c/wot-security/blob/master/presentations/2020-03-16-Bootstrapping%20IoT%20Security%20-%20The%20IETF%20Anima%20and%20OPC-UA%20Recipes.pdf</a>
              </li>
            </ul>
          </dd>
        </dl>
      </section>
    </section>
    <section id="VR/AR">
      <h3 id="x3-9-vr-ar"><bdi class="secno">3.9</bdi>
      VR/AR<a class="self-link" aria-label="§" href=
      "#VR/AR"></a></h3>
      <section id="ar-guide">
        <h4 id="x3-9-1-ar-virtual-guide"><bdi class=
        "secno">3.9.1</bdi> AR Virtual Guide<a class="self-link"
        aria-label="§" href="#ar-guide"></a></h4>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>
            <ul>
              <li>Rob Smith</li>
              <li>Kaz Ashimura</li>
            </ul>
          </dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>cloud provider</li>
              <li>service provider</li>
              <li>device manufacturer</li>
              <li>network operator (potentially transparent for WoT
              use cases)</li>
              <li>identity provider</li>
              <li>directory service operator</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>Using a wearable semi-transparent display, users can
          be guided by a virtual assistant through a physical area
          of interest with a rendered overlay to visualize events,
          annotate structures and other physical features, or
          visualize live and historical data associated with
          features of interest (which may or may not be at the same
          physical location as the sensor generating the data). An
          annotated map may provide additional geospatial guidance,
          including identification of landmarks, locations of
          devices. The system may also guide the user along a
          specific trajectory.</dd>
          <dt>Expected Devices</dt>
          <dd>
            <ul>
              <li>Wearable, semi-transparent head-mounted
              display</li>
              <li>Headphones for speakers for audio output</li>
              <li>Geopose and motion estimator (various
              technologies can be used)</li>
              <li>Data processor to integrate all data (including
              live an historical data and geopose), generate
              annotations for the display, and record/play
              scenes</li>
            </ul>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <ul>
              <li>3D Position, orientation, velocity, and
              acceleration of the user</li>
              <li>Corresponding geolocation information (latitude,
              longitude, altitude) for all features of interest,
              including but not limited to physical landmarks,
              roads and paths, and locations of sensor's
              measurement points.</li>
              <li>Timestamps to allow synchronization between the
              annotations and data streams and the user's
              movement</li>
            </ul>
          </dd>
          <dt>Affected WoT deliverables and/or work items</dt>
          <dd>
            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Binding Templates</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API accessible from
              application for interacting with devices.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>
            <ul>
              <li>The user can travel around a real space with
              guidance from virtually defined geospatial data
              projected on a head-mounted wearable display
              synchronized with the view of the physical
              environment.</li>
              <li>The wearable display can generate position and
              orientation (geopose) data so that the user's
              movement will be traced through the physical
              environment and can synchronized with virtual
              features.</li>
              <li>The user can control the video images provided by
              the system, based sensors attached the display system
              or other means of control (gestures, voice input,
              etc.)</li>
              <li>The technology should include synchronization of
              playback of stored video media and related sensors,
              displays, and devices as well as the display of
              geolocation information from the virtual map.</li>
              <li>Discovery of sensors should take into account the
              position and field of view of the user so that data
              can be retrieved only for the relevant features of
              interest.</li>
              <li>Discovery may additionally want to consider the
              motion (e.g. velocity) of the user to that data soon
              to come into view can be prefetched.</li>
              <li>Metadata for sensors needs to distinguish between
              the location of the device itself and the feature of
              interest it is measuring. For example, a camera might
              monitor traffic on a highway. The feature of interest
              is the location on the highway being monitored, while
              the location of the camera might be quite far away
              (e.g. mounted on top of a building).</li>
            </ul>See also the <a href=
            "https://w3c.github.io/sdw/proposals/geotagging/webvmt/#virtualguide">
            Use Case description from the WebVMT Editor's draft</a>
          </dd>
          <dt>Variants</dt>
          <dd>
            <ul>
              <li>Two synchronized displays (for example, a phone
              and a headset) can offer greater insight and provide
              clearer guidance to the user by showing different
              views of the same location, e.g. a top-view map on
              the phone.</li>
              <li>A VR (virtual-only) implementation may also be
              used, with a rendered scene replacing the real scene.
              This may be applicable to contexts such as a Smart
              City dashboard where sensor information from data
              needs to be viewed in context without having to
              actually visit the site.</li>
              <li>The head-mounted semi-transparent display might
              be replaced in some contexts with a handheld display
              e.g. a phone or tablet. To be useful for AR however,
              such a device needs a back camera to simulate
              transparency and capture images of the real
              environment (optional for VR), and a way to determine
              its geolocation and orientation (geopose) relative to
              the environment.</li>
              <li>The head-mounted display may use a camera rather
              than being physically transparent.</li>
              <li>A microphone may be added for voice input,
              including voice commands. This avoid having to
              clutter the view with controls.</li>
              <li>A 3D camera (e.g. LIDAR) may be used to capture a
              view of the environment, which can be helpful to
              establish geopose and align annotations with real
              features of the environment.</li>
              <li>A virtual guide for a particular geographic
              location, e.g. a historical site, which visualises
              past events and buildings in AR, or allows remote
              users to explore in VR.</li>
              <li>A medical tool which allows a patient to describe
              their symptoms using AR, e.g. identify a painful area
              on their own body, which is also modelled as a 'map'
              to show internal features and display a treatment
              guide, including any WoT medical devices.</li>
              <li>A virtual controller for a city engineer to
              visualize utilities, e.g. electrical cables or water
              pipes, and control them. For example, a maintenance
              engineer could switch off an individual street lamp
              in order to replace the bulb using an AR menu
              displayed on that WoT-enabled lamppost.</li>
              <li>These mechanisms can also be used for video
              overlay in general. The technologies are related to
              the recording, playing, and distribution of video
              content when the data is stored. Playback of stored
              data and movements would be useful for simulation and
              debugging.</li>
            </ul>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            <ul>
              <li>If an AR systems is compromised it could be used
              to guide a user into a dangerous situation while
              hiding that fact from them, e.g. encouraging them to
              step over a drop.</li>
              <li>For the above reason the system should "fail
              gracefully" if there is any sign its integrity is
              compromised, and should implement mechanisms (e.g.
              signing) to detect tampering. Standards should be
              similar to other systems than can cause physical
              harm, e.g. automobiles.</li>
              <li>For a "simulated" transparent head-mounted
              display using a camera, the system should have a
              fail-safe supporting an unfiltered view, which should
              be automatic even if the processor crashes.</li>
              <li>For all systems the user should have a simple way
              (e.g. a single button push) of viewing "baseline
              reality".</li>
            </ul>
          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <ul>
              <li>Systems that handle or display private data, e.g.
              medical applications, should respect the relevant
              regulations.</li>
              <li>Private data should not be retained by the device
              or used for purposes other than which it was
              provided. This includes the location of personal
              devices. To display information from another's
              personal device, permission needs to be explicitly
              granted by that person and this permission should be
              time and possibly space-limited.</li>
            </ul>
          </dd>
          <dt>Requirements</dt>
          <dd>
            <ul>
              <li>Geospatially aware discovery mechanisms that can
              discover features of interest close to the user.</li>
              <li>Geospatial filters for discovery that include a
              pyramid-shaped region representing the field of view
              of the user. Note: a basic cylindrical, spherical, or
              rectangular filter region can be used instead and
              then the irrelevant results filtered out, but this is
              less efficient than the filter itself supporting
              field-of-view queries.</li>
              <li>Geospatial data associate with the metadata for
              devices. Note that mobile devices may update their
              position more rapidly than a discovery service may be
              able to support. In this case the discovery service
              needs to take the velocity and last known position of
              the data source into account and compute a zone of
              uncertainty and return the metadata for sources that
              might possibly be in the field of view. For sources
              such as this with dynamic positions, the AR system
              may also communicate with data sources directly to
              determine their most recent geolocation.</li>
            </ul>
          </dd>
          <dt>Gaps</dt>
          <dd>
            <ul>
              <li>Geospatial queries for discovery.</li>
              <li>Standardized encodings of geospatial metadata in
              TDs.</li>
            </ul>
          </dd>
        </dl>
      </section>
    </section>
    <section id="edge-computing">
      <h3 id="x3-10-edge-computing"><bdi class="secno">3.10</bdi>
      Edge Computing<a class="self-link" aria-label="§" href=
      "#edge-computing"></a></h3>
      <dl>
        <dt>Submitter(s)</dt>
        <dd>Michael McCool</dd>
        <dt>Target Users</dt>
        <dd>
          Note: User should be "Stakeholder"
          <ul>
            <li>device owners - may benefit from using edge
            computing for iot orchestration and compute
            offload</li>
            <li>device user - may benefit from reduced cost of
            devices that can use compute offload</li>
            <li>cloud provider - may provide fallback for local
            edge compute services</li>
            <li>service provider - may provide edge computing
            service</li>
            <li>device manufacturer - may lower cost of device by
            depending on compute offload</li>
            <li>gateway manufacturer - may provide edge computing
            host hardware</li>
            <li>network operator - may provide edge computing
            nodes</li>
            <li>directory service operator - provides means to
            discover edge computing nodes</li>
          </ul>
        </dd>
        <dt>Motivation</dt>
        <dd>
          <ul>
            <li>IoT devices are often designed to be inexpensive
            (so they can be used at scale), small (for ease of
            installation) and are often power-limited, for example
            needing to run off a battery. For all these reasons,
            they usually have severely limited on-board
            computational capabilities.</li>
            <li>For applications that require significant
            computation and/or memory, for example computer vision,
            machine learning, or autonomous navigation, offloading
            work to another computer on the network may be
            advantageous.</li>
            <li>Offloading to the cloud typically involves
            relatively long latencies and may also have privacy
            implications. Edge computing implies offloading to a
            more "local" compute node with lower latency and
            optionally under more direct control of the user
            (improving privacy). This can be important for control
            applications (e.g. in robotics), computer graphics
            (e.g. gaming) and for applications processing imagery
            (e.g. facial recognition).</li>
            <li>An edge computer is also a convenient place to run
            persistent computations such as IoT orchestration rules
            that need to be "always on". Such an IoT orchestration
            system, in addition to needing to read from sensors and
            send commands to actuators over the network, may also
            invoke computationally-intensive services (e.g. image
            recognition). An example would be a security system
            that when a motion sensor is tripped, runs a person
            detection computation, and if a person is detected when
            and where they should not be, sounds an alarm. The
            motion sensor and alarm can be IoT devices while the
            person detection is a computationally-intensive
            service.</li>
          </ul>
        </dd>
        <dt>Expected Devices</dt>
        <dd>
          <ul>
            <li>IoT devices with Thing Descriptions for use in IoT
            orchestrations.</li>
            <li>An edge computer providing one or more fixed or
            generic compute services.</li>
            <li>A directory or other discovery mechanism that
            allows IoT devices and edge computers to advertize
            their availability.</li>
          </ul>
        </dd>
        <dt>Expected Data</dt>
        <dd>
          <ul>
            <li>Thing descriptions for IoT devices</li>
            <li>Thing descriptions for compute services</li>
            <li>Compute service configurations, e.g container
            images, WASM code, scripts, ONNX files, etc.</li>
          </ul>
        </dd>
        <dt>Affected WoT deliverables and/or work items</dt>
        <dd>
          <ul>
            <li>WoT Discovery - needs to be designed to support
            services, not just physical devices.</li>
            <li>WoT Architecture - concept of Thing needs to be
            expanded to include computational services.</li>
            <li>WoT Scripting API - essential for programming IoT
            orchestrations.</li>
          </ul>
        </dd>
        <dt>Description</dt>
        <dd>
          The WoT architecture can provide an interesting approach
          to edge computing:
          <ul>
            <li>An IoT orchestration running in an edge computer
            can consume WoT Thing Descriptions in order to
            determine how to connect to IoT devices.</li>
            <li>Fixed services (e.g. person detection) and generic
            compute nodes (a service that would allow an arbitrary
            computation to be loaded onto it) can also advertise
            themselves using Thing Descriptions, allowing an IoT
            orchestrator to interface to devices and services in a
            uniform way. This also facilitates support for "virtual
            devices", e.g. using computer vision, audio
            recognition, or other forms of analytics in place of a
            physical sensor.</li>
            <li>WoT discovery can be used to find appropriate
            compute services for IoT devices to offload
            computationally demanding tasks to, assuming those
            services describe themselves with TDs and advertise
            their availability via WoT discovery mechanisms.</li>
          </ul>
        </dd>
        <dt>Variants</dt>
        <dd>
          <ul>
            <li>An edge computer can provide facilities either for
            general-purpose computation (e.g. loading and running a
            container image, script, etc.) or special-purpose fixed
            computations (e.g. object detection and tracking,
            person detection, etc.). General-purpose computation is
            more powerful but also is more difficult to make fully
            secure.</li>
            <li>An edge computation can be stateless (function as a
            service, FaaS) or stateful. It is easier to migrate
            stateless computations transparently to new compute
            hardware but state then needs to be provided by a
            separate service, e.g. a database, and it is harder to
            program.</li>
            <li>Edge computers may provide just IoT orchestration
            without significant computational ability, just compute
            offload, or both. Many more use cases can be unlocked
            by providing both.</li>
            <li>Persistent computation can be provided in various
            ways. Rather than actually running continuously, an
            edge computation might be event-driven, for
            example.</li>
            <li>Under discussion are various ways to integrate edge
            computation with the web execution environment, for
            example by extending web and service workers.</li>
          </ul>
        </dd>
        <dt>Security Considerations</dt>
        <dd>Edge compute services supporting the specification of
        generic computation has many security challenges. In
        addition to the challenges common to cloud computing, e.g.
        protecting "tenants" from seeing each other's activity,
        additional challenges arise if the edge computer is
        offering computation as an ad-hoc service. For example,
        there needs to be a way to project the edge computer from
        denial-of-service attacks. An edge computer may also need
        to be protected from physical attacks. There is also the
        possibility that an edge computer might be physically
        compromised so approaches such as isolated containers
        (protecting the contents from the edge computer's
        hypervisor), and/or validated boot, might be necessary in
        some circumstances.</dd>
        <dt>Privacy Considerations</dt>
        <dd>Edge computers can theoretically improve privacy since
        sensitive data can be processed "locally" without having to
        be transmitted to a remote site. This however is tempered
        by edge computer's greater vulnerability to physical
        attacks. To avoid offloading work to a malicious edge
        computer, some means of evaluating the trustworthiness of
        edge computers is needed.</dd>
        <dt>Gaps</dt>
        <dd>
          <ul>
            <li>Explicit support for WoT Things that are
            services.</li>
            <li>Sufficient abstraction capability (e.g.
            "interfaces") to support virtual devices.</li>
            <li>A mechanism to package and install edge
            computations that can use the WoT scripting API for
            orchestration.</li>
            <li>A general means to manage compute nodes to provide
            offload targets (e.g. a standardized TD template for
            compute services).</li>
          </ul>
        </dd>
        <dt>Existing Standards</dt>
        <dd>
          <ul>
            <li>
              <a href=
              "https://tools.ietf.org/html/draft-hong-t2trg-iot-edge-computing-04">
              IoT Edge Challenges and Functions</a>, IETF
              Internet-Draft, Network Working Group, version 04,
              expires 26 November 2020.
            </li>
            <li>
              <a href=
              "https://www.iiconsortium.org/fog-and-edge-white-papers.htm">
              IIC/OpenFog</a>, Fog and Edge Computing White Papers
            </li>
            <li>
              <a href=
              "https://www.etsi.org/technologies/multi-access-edge-computing">
              ETSI MEC</a>, ETSI standards for Multiaccess Edge
              Computing
            </li>
          </ul>
        </dd>
      </dl>
    </section>
  </section>
  <section id="requirements">
    <h2 id="x4-requirements"><bdi class="secno">4.</bdi>
    Requirements<a class="self-link" aria-label="§" href=
    "#requirements"></a></h2>
    <section id="sec-functional-requirement">
      <h3 id="x4-1-functional-requirements"><bdi class=
      "secno">4.1</bdi> Functional Requirements<a class="self-link"
      aria-label="§" href="#sec-functional-requirement"></a></h3>
      <p>This section defines the properties required in an
      abstract Web of Things (WoT) architecture.</p>
      <section id="sec-requirements-principles">
        <h4 id="x4-1-1-common-principles"><bdi class=
        "secno">4.1.1</bdi> Common Principles<a class="self-link"
        aria-label="§" href=
        "#sec-requirements-principles"></a></h4>
        <ul>
          <li>WoT architecture should enable mutual interworking of
          different eco-systems using web technology.</li>
          <li>WoT architecture should be based on the web
          architecture using RESTful APIs.</li>
          <li>WoT architecture should allow to use multiple payload
          formats which are commonly used in the web.</li>
          <li>WoT architecture must enable different device
          architectures and must not force a client or server
          implementation of system components.</li>
          <li>Flexibility
            <p>There are a wide variety of physical device
            configurations for WoT implementations. The WoT
            abstract architecture should be able to be mapped to
            and cover all of the variations.</p>
          </li>
          <li>Compatibility
            <p>There are already many existing IoT solutions and
            ongoing IoT standardization activities in many business
            fields. The WoT should provide a bridge between these
            existing and developing IoT solutions and Web
            technology based on WoT concepts. The WoT should be
            upwards compatible with existing IoT solutions and
            current standards.</p>
          </li>
          <li>Scalability
            <p>WoT must be able to scale for IoT solutions that
            incorporate thousands to millions of devices. These
            devices may offer the same capabilities even though
            they are created by different manufacturers.</p>
          </li>
          <li>Interoperability
            <p>WoT must provide interoperability across device and
            cloud manufacturers. It must be possible to take a WoT
            enabled device and connect it with a cloud service from
            different manufacturers out of the box.</p>
          </li>
        </ul>
      </section>
      <section id="sec-requirements-thing-functionalities">
        <h4 id="x4-1-2-thing-functionalities"><bdi class=
        "secno">4.1.2</bdi> Thing Functionalities<a class=
        "self-link" aria-label="§" href=
        "#sec-requirements-thing-functionalities"></a></h4>
        <ul>
          <li>WoT architecture should allow things to have
          functionalities such as
            <ul>
              <li>reading thing's status information</li>
              <li>updating thing's status information which might
              cause actuation</li>
              <li>subscribing to, receiving and unsubscribing to
              notifications of changes of the thing's status
              information.</li>
              <li>invoking functions with input and output
              parameters which would cause certain actuation or
              calculation.</li>
              <li>subscribing to, receiving and unsubscribing to
              event notifications that are more general than just
              reports of state transitions.</li>
            </ul>
          </li>
        </ul>
      </section>
      <section id="sec-requirements-search-and-discovery">
        <h4 id="x4-1-3-search-and-discovery"><bdi class=
        "secno">4.1.3</bdi> Search and Discovery<a class=
        "self-link" aria-label="§" href=
        "#sec-requirements-search-and-discovery"></a></h4>
        <ul>
          <li>WoT architecture should allow clients to know thing's
          attributes, functionalities and their access points,
          prior to access to the thing itself.</li>
          <li>WoT architecture should allow clients to search
          things by its attributes and functionalities.</li>
          <li>WoT architecture should allow semantic search of
          things providing required functionalities based on a
          unified vocabulary, regardless of naming of the
          functionalities.</li>
        </ul>
      </section>
      <section id="sec-requirements-description-mechanism">
        <h4 id="x4-1-4-description-mechanism"><bdi class=
        "secno">4.1.4</bdi> Description Mechanism<a class=
        "self-link" aria-label="§" href=
        "#sec-requirements-description-mechanism"></a></h4>
        <ul>
          <li>WoT architecture should support a common description
          mechanism which enables describing things and their
          functions.</li>
          <li>Such descriptions should be not only human-readable,
          but also machine-readable.</li>
          <li>Such descriptions should allow semantic annotation of
          its structure and described contents.</li>
          <li>Such description should be able to be exchanged using
          multiple formats which are commonly used in the web.</li>
        </ul>
      </section>
      <section id="sec-requirements-description-of-attributes">
        <h4 id="x4-1-5-description-of-attributes"><bdi class=
        "secno">4.1.5</bdi> Description of Attributes<a class=
        "self-link" aria-label="§" href=
        "#sec-requirements-description-of-attributes"></a></h4>
        <ul>
          <li>WoT architecture should allow describing thing's
          attributes such as
            <ul>
              <li>name</li>
              <li>explanation</li>
              <li>version of spec, format and description
              itself</li>
              <li>links to other related things and metadata
              information</li>
            </ul>
          </li>
          <li>Such descriptions should support
          internationalization.</li>
        </ul>
      </section>
      <section id=
      "sec-requirements-description-of-functionalities">
        <h4 id="x4-1-6-description-of-functionalities"><bdi class=
        "secno">4.1.6</bdi> Description of Functionalities<a class=
        "self-link" aria-label="§" href=
        "#sec-requirements-description-of-functionalities"></a></h4>
        <ul>
          <li>WoT architecture should allow describing thing's
          functionalities which is shown in <a href=
          "#sec-requirements-thing-functionalities" class=
          "sec-ref">§&nbsp;<bdi class="secno">4.1.2</bdi> Thing
          Functionalities</a>
          </li>
        </ul>
      </section>
      <section id="sec-requirements-network">
        <h4 id="x4-1-7-network"><bdi class="secno">4.1.7</bdi>
        Network<a class="self-link" aria-label="§" href=
        "#sec-requirements-network"></a></h4>
        <ul>
          <li>WoT architecture should support multiple web
          protocols which are commonly used.</li>
          <li>Such protocols include
            <ol>
              <li>protocols commonly used in the internet and</li>
              <li>protocols commonly used in the local area
              network</li>
            </ol>
          </li>
          <li>WoT architecture should allow using multiple web
          protocols to access to the same functionality.</li>
          <li>WoT architecture should allow using a combination of
          multiple protocols to the functionalities of the same
          thing (e.g. HTTP and WebSocket).</li>
        </ul>
      </section>
      <section id="sec-requirements-deployment">
        <h4 id="x4-1-8-deployment"><bdi class="secno">4.1.8</bdi>
        Deployment<a class="self-link" aria-label="§" href=
        "#sec-requirements-deployment"></a></h4>
        <ul>
          <li>WoT architecture should support a wide variety of
          thing capabilities such as edge devices with resource
          restrictions and virtual things on the cloud, based on
          the same model.</li>
          <li>WoT architecture should support multiple levels of
          thing hierarchy with intermediate entities such as
          gateways and proxies.</li>
          <li>WoT architecture should support accessing things in
          the local network from the outside of the local network
          (the internet or another local network), considering
          network address translation.</li>
        </ul>
      </section>
      <section id="sec-requirements-application">
        <h4 id="x4-1-9-application"><bdi class="secno">4.1.9</bdi>
        Application<a class="self-link" aria-label="§" href=
        "#sec-requirements-application"></a></h4>
        <ul>
          <li>WoT architecture should allow describing applications
          for a wide variety of things such as edge device,
          gateway, cloud and UI/UX device, using web standard
          technology based on the same model.</li>
        </ul>
      </section>
      <section id="sec-requirements-legacy-adoption">
        <h4 id="x4-1-10-legacy-adoption"><bdi class=
        "secno">4.1.10</bdi> Legacy Adoption<a class="self-link"
        aria-label="§" href=
        "#sec-requirements-legacy-adoption"></a></h4>
        <ul>
          <li>WoT architecture should allow mapping of legacy IP
          and non-IP protocols to web protocols, supporting various
          topologies, where such legacy protocols are terminated
          and translated.</li>
          <li>WoT architecture should allow transparent use of
          existing IP protocols without translation, which follow
          RESTful architecture.</li>
          <li>WoT architecture must not enforce client or server
          roles on devices and services. An IoT device can be
          either a client or a server, or both, depending on the
          system architecture; the same is true of edge and cloud
          services.</li>
        </ul>
      </section>
    </section>
    <section id="sec-technical-requirements">
      <h3 id="x4-2-technical-requirements"><bdi class=
      "secno">4.2</bdi> Technical Requirements<a class="self-link"
      aria-label="§" href="#sec-technical-requirements"></a></h3>
      <p>The <abbr title="World Wide Web Consortium">W3C</abbr> WoT
      Thing Architecture [<cite><a class="bibref" data-link-type=
      "biblio" href="#bib-wot-architecture" title=
      "Web of Things (WoT) Architecture">wot-architecture</a></cite>]
      defines the abstract architecture of Web of Things and
      illustrates it with various system topologies. This section
      describes technical requirements derived from the abstract
      architecture.</p>
      <section id=
      "components-in-the-web-of-things-and-the-web-of-things-architecture">
        <h4 id=
        "x4-2-1-components-in-the-web-of-things-and-the-web-of-things-architecture">
        <bdi class="secno">4.2.1</bdi> Components in the Web of
        Things and the Web of Things Architecture<a class=
        "self-link" aria-label="§" href=
        "#components-in-the-web-of-things-and-the-web-of-things-architecture"></a></h4>
        <p>The use cases help to identify basic components such as
        devices and applications, that access and control those
        devices, proxies (i.e., gateways and edge devices) that are
        located between devices. An additional component useful in
        some use cases is the directory, which assists with
        discovery.</p>
        <p>Those components are connected to the internet or field
        networks in offices, factories or other facilities. Note
        that all components involved may be connected to a single
        network in some cases, however, in general components can
        be deployed across multiple networks.</p>
      </section>
      <section id="devices">
        <h4 id="x4-2-2-devices"><bdi class="secno">4.2.2</bdi>
        Devices<a class="self-link" aria-label="§" href=
        "#devices"></a></h4>
        <p>Access to devices is made using a description of their
        functions and interfaces. This description is called
        <em>Thing Description (TD)</em>. A <em>Thing
        Description</em> includes a general metadata about the
        device, information models representing functions,
        transport protocol description for operating on information
        models, and security information.</p>
        <p>General metadata contains device identifiers (URI),
        device information such as serial number, production date,
        location and other human readable information.</p>
        <p>Information models defines device attributes, and
        represent device’s internal settings, control functionality
        and notification functionality. Devices that have the same
        functionality have the same information model regardless of
        the transport protocols used.</p>
        <p>Because many systems based on Web of Things architecture
        are crossing system Domains, vocabularies and meta data
        (e.g. ontologies) used in information models should be
        commonly understood by involved parties. In addition to
        REST transports, PubSub transports are also supported.</p>
        <p>Security information includes descriptions about
        authentication, authorization and secure communications.
        Devices are required to put TDs either inside them or at
        locations external to the devices, and to make TDs
        accessible so that other components can find and access
        them.</p>
      </section>
      <section id="applications">
        <h4 id="x4-2-3-applications"><bdi class="secno">4.2.3</bdi>
        Applications<a class="self-link" aria-label="§" href=
        "#applications"></a></h4>
        <p>Applications need to be able to generate and use network
        and program interfaces based on metadata
        (descriptions).</p>
        <p>Applications have to be able to obtain these
        descriptions through the network, therefore, need to be
        able to conduct search operations and acquire the necessary
        descriptions over the network.</p>
      </section>
      <section id="digital-twins">
        <h4 id="x4-2-4-digital-twins"><bdi class=
        "secno">4.2.4</bdi> Digital Twins<a class="self-link"
        aria-label="§" href="#digital-twins"></a></h4>
        <p>Digital Twins need to generate program interfaces
        internally based on metadata (descriptions), and to
        represent virtual devices by using those program
        interfaces. A twin has to produce a description for the
        virtual device and make it externally available.</p>
        <p>Identifiers of virtual devices need to be newly
        assigned, therefore, are different from the original
        devices. This makes sure that virtual devices and the
        original devices are clearly recognized as separate
        entities. Transport and security mechanisms and settings of
        the virtual devices can be different from original devices
        if necessary. Virtual devices are required to have
        descriptions provided either directly by the twin or to
        have them available at external locations. In either case
        it is required to make the descriptions available so that
        other components can find and use the devices associated
        with them.</p>
      </section>
      <section id="discovery">
        <h4 id="x4-2-5-discovery"><bdi class="secno">4.2.5</bdi>
        Discovery<a class="self-link" aria-label="§" href=
        "#discovery"></a></h4>
        <p>For TDs of devices and virtual devices to be accessible
        from devices, applications and twins, there needs to be a
        common way to share TDs. Directories can serve this
        requirement by providing functionalities to allow devices
        and twins themselves automatically or the users to manually
        register the descriptions.</p>
        <p>Descriptions of the devices and virtual devices need to
        be searchable by external entities. Directories have to be
        able to process search operations with search keys such as
        keywords from the general description in the device
        description or information models.</p>
      </section>
      <section id="security">
        <h4 id="x4-2-6-security"><bdi class="secno">4.2.6</bdi>
        Security<a class="self-link" aria-label="§" href=
        "#security"></a></h4>
        <p>Security information related to devices and virtual
        devices needs to be described in device descriptions. This
        includes information for authentication/authorization and
        payload encryption.</p>
        <p>WoT architecture should support multiple security
        mechanism commonly used in the web, such as Basic, Digest,
        Bearer and OAuth2.0.</p>
      </section>
      <section id="accessibility-0">
        <h4 id="x4-2-7-accessibility"><bdi class=
        "secno">4.2.7</bdi> Accessibility<a class="self-link"
        aria-label="§" href="#accessibility-0"></a></h4>
        <p>The Web of Things primarily targets machine-to-machine
        communication. The humans involved are usually developers
        that integrate Things into applications. End-users will be
        faced with the front-ends of the applications or the
        physical user interfaces provided by devices themselves.
        Both are out of scope of the <abbr title=
        "World Wide Web Consortium">W3C</abbr> WoT specifications.
        Given the focus on IoT instead of users, accessibility is
        not a direct requirement, and hence is not addressed within
        this specification.</p>
        <p>There is, however, an interesting aspect on
        accessibility: Fulfilling the requirements above enables
        machines to understand the network-facing API of devices.
        This can be utilized by accessibility tools to provide user
        interfaces of different modality, thereby removing barriers
        to using physical devices and IoT-related applications.</p>
      </section>
    </section>
    <section id="acknowledgements" class="appendix normative">
      <h3 id="x4-3-acknowledgments"><bdi class="secno">4.3</bdi>
      Acknowledgments<a class="self-link" aria-label="§" href=
      "#acknowledgements"></a></h3>
      <p>Special thanks to all authors of use case descriptions (in
      alphabetical order) for their contributions to this
      document:</p>
      <ul>
        <li>Shinya Abe (NHK)</li>
        <li>Cristiano Aguzzi (University of Bologna)</li>
        <li>Kaz Ashimura (<abbr title=
        "World Wide Web Consortium">W3C</abbr>)</li>
        <li>Raúl García Castro (Universidad Politécnica de
        Madrid)</li>
        <li>Jean-Pierre Chanet (INRAE, France)</li>
        <li>Edison Chung (MINES St. Etienne)</li>
        <li>Andrea Cimmino (Universidad Politécnica de Madrid)</li>
        <li>Hiroki Endo (NHK)</li>
        <li>David Ezell (Conexxus)</li>
        <li>Hiroshi Fujisawa (NHK)</li>
        <li>Christian Glomb (Siemens)</li>
        <li>Masaya Ikeo (NHK)</li>
        <li>Sebastian Käbisch (Siemens)</li>
        <li>Takuki Kamiya (Fujitsu)</li>
        <li>Zoltan Kis (Intel)</li>
        <li>Ege Korkan (Siemens)</li>
        <li>Michael Lagally (Oracle)</li>
        <li>Jennifer Lin (GovTech Singapore)</li>
        <li>Ryuichi Matsukura (Fujitsu)</li>
        <li>Michael McCool (Intel)</li>
        <li>Hervé Pruvost (Fraunhofer IIS EAS)</li>
        <li>Catherine Roussey (INRAE, France)</li>
        <li>Georg Ferdinand Schneider (Schaeffler
        Technologies)</li>
        <li>Rob Smith (Awayteam)</li>
        <li>Farshid Tavakolizadeh (Fraunhofer)</li>
      </ul>
      <p>Many thanks to the <abbr title=
      "World Wide Web Consortium">W3C</abbr> staff and all other
      active Participants of the <abbr title=
      "World Wide Web Consortium">W3C</abbr> Web of Things Interest
      Group (WoT IG) and Working Group (WoT WG) for their support,
      technical input and suggestions that led to improvements to
      this document.</p>
      <p>Special thanks to Kazuyuki Ashimura form the <abbr title=
      "World Wide Web Consortium">W3C</abbr> for his continuous
      help and support of the work of the WoT Use Cases Task
      Force.</p>
    </section>
  </section>
  <section id="references" class="appendix">
    <h2 id="a-references"><bdi class="secno">A.</bdi>
    References<a class="self-link" aria-label="§" href=
    "#references"></a></h2>
    <section id="normative-references">
      <h3 id="a-1-normative-references"><bdi class=
      "secno">A.1</bdi> Normative references<a class="self-link"
      aria-label="§" href="#normative-references"></a></h3>
      <dl class="bibliography">
        <dt id="bib-rfc2119">[RFC2119]</dt>
        <dd>
          <a href="https://tools.ietf.org/html/rfc2119"><cite>Key
          words for use in RFCs to Indicate Requirement
          Levels</cite></a>. S. Bradner. IETF. March 1997. Best
          Current Practice. URL: <a href=
          "https://tools.ietf.org/html/rfc2119">https://tools.ietf.org/html/rfc2119</a>
        </dd>
        <dt id="bib-rfc8174">[RFC8174]</dt>
        <dd>
          <a href=
          "https://tools.ietf.org/html/rfc8174"><cite>Ambiguity of
          Uppercase vs Lowercase in RFC 2119 Key Words</cite></a>.
          B. Leiba. IETF. May 2017. Best Current Practice. URL:
          <a href=
          "https://tools.ietf.org/html/rfc8174">https://tools.ietf.org/html/rfc8174</a>
        </dd>
        <dt id="bib-wot-architecture">[wot-architecture]</dt>
        <dd>
          <a href=
          "https://www.w3.org/TR/wot-architecture/"><cite>Web of
          Things (WoT) Architecture</cite></a>. Matthias Kovatsch;
          Ryuichi Matsukura; Michael Lagally; Toru Kawaguchi;
          Kunihiko Toumura; Kazuo Kajimoto. W3C. 9 April 2020. W3C
          Recommendation. URL: <a href=
          "https://www.w3.org/TR/wot-architecture/">https://www.w3.org/TR/wot-architecture/</a>
        </dd>
        <dt id="bib-wot-thing-description">
        [wot-thing-description]</dt>
        <dd>
          <a href=
          "https://www.w3.org/TR/wot-thing-description/"><cite>Web
          of Things (WoT) Thing Description</cite></a>. Sebastian
          Käbisch; Takuki Kamiya; Michael McCool; Victor Charpenay;
          Matthias Kovatsch. W3C. 9 April 2020. W3C Recommendation.
          URL: <a href=
          "https://www.w3.org/TR/wot-thing-description/">https://www.w3.org/TR/wot-thing-description/</a>
        </dd>
      </dl>
    </section>
  </section>
  <p role="navigation" id="back-to-top"><a href=
  "#title"><abbr title="Back to Top">↑</abbr></a></p>
  <script id="respec-dfn-panel">
  (() => {
  // @ts-check
  if (document.respec) {
  document.respec.ready.then(setupPanel);
  } else {
  setupPanel();
  }

  function setupPanel() {
  const listener = panelListener();
  document.body.addEventListener("keydown", listener);
  document.body.addEventListener("click", listener);
  }

  function panelListener() {
  /** @type {HTMLElement} */
  let panel = null;
  return event => {
    const { target, type } = event;

    if (!(target instanceof HTMLElement)) return;

    // For keys, we only care about Enter key to activate the panel
    // otherwise it's activated via a click.
    if (type === "keydown" && event.key !== "Enter") return;

    const action = deriveAction(event);

    switch (action) {
      case "show": {
        hidePanel(panel);
        /** @type {HTMLElement} */
        const dfn = target.closest("dfn, .index-term");
        panel = document.getElementById(`dfn-panel-for-${dfn.id}`);
        const coords = deriveCoordinates(event);
        displayPanel(dfn, panel, coords);
        break;
      }
      case "dock": {
        panel.style.left = null;
        panel.style.top = null;
        panel.classList.add("docked");
        break;
      }
      case "hide": {
        hidePanel(panel);
        panel = null;
        break;
      }
    }
  };
  }

  /**
  * @param {MouseEvent|KeyboardEvent} event
  */
  function deriveCoordinates(event) {
  const target = /** @type HTMLElement */ (event.target);

  // We prevent synthetic AT clicks from putting
  // the dialog in a weird place. The AT events sometimes
  // lack coordinates, so they have clientX/Y = 0
  const rect = target.getBoundingClientRect();
  if (
    event instanceof MouseEvent &&
    event.clientX >= rect.left &&
    event.clientY >= rect.top
  ) {
    // The event probably happened inside the bounding rect...
    return { x: event.clientX, y: event.clientY };
  }

  // Offset to the middle of the element
  const x = rect.x + rect.width / 2;
  // Placed at the bottom of the element
  const y = rect.y + rect.height;
  return { x, y };
  }

  /**
  * @param {Event} event
  */
  function deriveAction(event) {
  const target = /** @type {HTMLElement} */ (event.target);
  const hitALink = !!target.closest("a");
  if (target.closest("dfn:not([data-cite]), .index-term")) {
    return hitALink ? "none" : "show";
  }
  if (target.closest(".dfn-panel")) {
    if (hitALink) {
      return target.classList.contains("self-link") ? "hide" : "dock";
    }
    const panel = target.closest(".dfn-panel");
    return panel.classList.contains("docked") ? "hide" : "none";
  }
  if (document.querySelector(".dfn-panel:not([hidden])")) {
    return "hide";
  }
  return "none";
  }

  /**
  * @param {HTMLElement} dfn
  * @param {HTMLElement} panel
  * @param {{ x: number, y: number }} clickPosition
  */
  function displayPanel(dfn, panel, { x, y }) {
  panel.hidden = false;
  // distance (px) between edge of panel and the pointing triangle (caret)
  const MARGIN = 20;

  const dfnRects = dfn.getClientRects();
  // Find the `top` offset when the `dfn` can be spread across multiple lines
  let closestTop = 0;
  let minDiff = Infinity;
  for (const rect of dfnRects) {
    const { top, bottom } = rect;
    const diffFromClickY = Math.abs((top + bottom) / 2 - y);
    if (diffFromClickY < minDiff) {
      minDiff = diffFromClickY;
      closestTop = top;
    }
  }

  const top = window.scrollY + closestTop + dfnRects[0].height;
  const left = x - MARGIN;
  panel.style.left = `${left}px`;
  panel.style.top = `${top}px`;

  // Find if the panel is flowing out of the window
  const panelRect = panel.getBoundingClientRect();
  const SCREEN_WIDTH = Math.min(window.innerWidth, window.screen.width);
  if (panelRect.right > SCREEN_WIDTH) {
    const newLeft = Math.max(MARGIN, x + MARGIN - panelRect.width);
    const newCaretOffset = left - newLeft;
    panel.style.left = `${newLeft}px`;
    /** @type {HTMLElement} */
    const caret = panel.querySelector(".caret");
    caret.style.left = `${newCaretOffset}px`;
  }

  // As it's a dialog, we trap focus.
  // TODO: when <dialog> becomes a implemented, we should really
  // use that.
  trapFocus(panel, dfn);
  }

  /**
  * @param {HTMLElement} panel
  * @param {HTMLElement} dfn
  * @returns
  */
  function trapFocus(panel, dfn) {
  /** @type NodeListOf<HTMLAnchorElement> elements */
  const anchors = panel.querySelectorAll("a[href]");
  // No need to trap focus
  if (!anchors.length) return;

  // Move focus to first anchor element
  const first = anchors.item(0);
  first.focus();

  const trapListener = createTrapListener(anchors, panel, dfn);
  panel.addEventListener("keydown", trapListener);

  // Hiding the panel releases the trap
  const mo = new MutationObserver(records => {
    const [record] = records;
    const target = /** @type HTMLElement */ (record.target);
    if (target.hidden) {
      panel.removeEventListener("keydown", trapListener);
      mo.disconnect();
    }
  });
  mo.observe(panel, { attributes: true, attributeFilter: ["hidden"] });
  }

  /**
  *
  * @param {NodeListOf<HTMLAnchorElement>} anchors
  * @param {HTMLElement} panel
  * @param {HTMLElement} dfn
  * @returns
  */
  function createTrapListener(anchors, panel, dfn) {
  const lastIndex = anchors.length - 1;
  let currentIndex = 0;
  return event => {
    switch (event.key) {
      // Hitting "Tab" traps us in a nice loop around elements.
      case "Tab": {
        event.preventDefault();
        currentIndex += event.shiftKey ? -1 : +1;
        if (currentIndex < 0) {
          currentIndex = lastIndex;
        } else if (currentIndex > lastIndex) {
          currentIndex = 0;
        }
        anchors.item(currentIndex).focus();
        break;
      }

      // Hitting "Enter" on an anchor releases the trap.
      case "Enter":
        hidePanel(panel);
        break;

      // Hitting "Escape" returns focus to dfn.
      case "Escape":
        hidePanel(panel);
        dfn.focus();
        return;
    }
  };
  }

  /** @param {HTMLElement} panel */
  function hidePanel(panel) {
  if (!panel) return;
  panel.hidden = true;
  panel.classList.remove("docked");
  }
  })()
  </script>
  <script src=
  "https://www.w3.org/scripts/TR/2016/fixup.js"></script>
</body>
</html>
